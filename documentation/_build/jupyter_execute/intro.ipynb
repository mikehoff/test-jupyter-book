{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e9dfac",
   "metadata": {},
   "source": [
    "# Data Engineer Workshop 9: Advanced Stream Processing Pipelines\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### üß± Building on Previous Workshops: From On-Demand to Batch and Streaming\n",
    "\n",
    "Our workshop journey so far through different data processing patterns has prepared us for today's exploration of streaming architectures.\n",
    "\n",
    "- In Workshop 5, we learned how to make data rapidly available for analysts using AWS cloud services. We transformed CSV files into optimised Parquet format, used Glue to automatically discover and catalogue data structures, and enabled quick analysis through Athena's serverless querying. This taught us how we might handle data at rest efficiently and make it immediately queryable.\n",
    "\n",
    "![workshop5-final-arch_01](./images/workshop5-final-arch_01.png)<br>\n",
    "\n",
    "- Workshop 7 then introduced the challenges of keeping analytical data current as source systems change. Using the Sakila DVD rental database, we built automated pipelines in Azure Synapse to detect changes in our OLTP data source and update our dimensional model (the customer dimension). This showed us how to maintain data warehouse consistency through scheduled batch updates.\n",
    "\n",
    "![workshop_07_architecture](./images/workshop_07_architecture.png)<br>\n",
    "\n",
    "Today, we can combine these practical insights as we move from periodic batch processing to continuous streaming. So instead of waiting to gather changes and process them on a schedule (e.g. daily, weekly, monthly), we'll build a pipeline that can processes weather data from an API in real-time as it arrives. This will combine the efficient data storage patterns we learned in Workshop 5 with the change handling techniques from Workshop 7, but applies them to data in motion! üèÉ‚Äç‚ôÇÔ∏è‚Äç‚û°Ô∏è\n",
    "\n",
    "Our new streaming architecture will:\n",
    "- Ingest data continuously through Kinesis streams (versus periodic batch loads in Workshop 5)\n",
    "- Process records immediately as they arrive (versus scheduled updates)\n",
    "- Enable multiple downstream consumers to analyse the same data stream\n",
    "- Support both real-time analytics and historical querying\n",
    "\n",
    "![Workshop_09_architecture.drawio](./images/Workshop_09_architecture.drawio.png)<br>\n",
    "\n",
    "This approach particularly suits use cases where data freshness is a key businesss requirement, like monitoring weather conditions, tracking financial transactions, or analysing Internet of Things (IoT) sensor data (e.g... ). As you work through today's exercises, consider how streaming might complement the batch processing patterns you've already learned, and when each approach might be most appropriate for your own orgnasation or project.\n",
    "\n",
    "### ‚ÅâÔ∏è Should we really be streaming data?\n",
    "\n",
    "While streaming architectures can help answer *\"what's happening right now?\"* versus historical *\"what happened?\"* analysis, it's important to carefully evaluate whether real-time data serves a genuine business need. Consider:\n",
    "\n",
    "1. **Business Value**: Does real-time insight enable better decision-making or customer experience that justifies the additional complexity?\n",
    "\n",
    "2. **True Requirements**: When stakeholders request *\"real-time\"* data, would near real-time or daily batch updates suffice?\n",
    "\n",
    "3. **Cost-Benefit Analysis**:  Streaming architectures can introduce complexity we will encounter directly in the workshop exercises. While these examples may not be immediately clear now, they'll become concrete as you build and test each component of the weather data pipeline:\n",
    "    - Error handling and recovery (e.g. handling failed API calls to the weather service, managing retry logic for Kinesis stream failures, dealing with Lambda function timeouts)\n",
    "    - Data consistency and ordering (e.g. ensuring weather readings from different cities arrive in the correct chronological order, handling duplicate temperature measurements from multiple collections of the same reading)\n",
    "    - Infrastructure management (e.g. monitoring Kinesis stream throughput capacity, managing Lambda concurrency limits, scaling Firehose delivery performance)\n",
    "    - Monitoring and alerting (e.g. tracking end-to-end latency from weather measurement time to Redshift availability, detecting gaps in city data collection, alerting on abnormal temperature variations)\n",
    "\n",
    "4. **Alternative Approaches**: Could simpler solutions work?\n",
    "  - On-demand updates (Workshops 5)\n",
    "  - Scheduled batch processing (Workshop 7)\n",
    "  - More frequent but still batch-based loads\n",
    "\n",
    "As Zach Wilson provocatively notes in this X post: *\"Stop building streaming pipelines when your stakeholders request 'real time' data!\"*\n",
    "\n",
    "![Zach Wilson Tweet](https://pbs.twimg.com/media/GgVZDkhbwAErpUs?format=jpg&name=900x900)\n",
    "\n",
    "This afternoon, as well as reflecting on the practical exercises here we'll explore how to evaluate these tradeoffs for your specific project needs and stakeholder requirements.\n",
    "\n",
    "### ü§ù Alignment with Data Engineer Pass Descriptors\n",
    "\n",
    "```{note}\n",
    "This workshop aligns with several IFATE Data Engineer Pass Descriptors: https://www.instituteforapprenticeships.org/apprenticeship-standards/data-engineer-v1-0\n",
    "```\n",
    "````{dropdown} Click here to review more detail\n",
    "\n",
    "- **Describes how they use data ingestion frameworks such as streaming, batching and on demand services to move data from one location to another in order to optimise data ingestion processes. (K18, S15)**  \n",
    "  We focus on streaming and this workshop demonstrate different kinds of streaming patterns and consider their optimisation and business uses.\n",
    "\n",
    "- **Describes the types and uses of data engineering tools in their own organisation and how they apply them. (K20)**  \n",
    " The workshop covers multiple AWS data engineering tools that represent common architectural patterns you'll find across different cloud platforms and on-premises solutions. By understanding these core patterns, you'll be better equipped to work with equivalent tools in your own organisation, whether they use AWS, Azure, Google Cloud, or open-source alternatives like Kafka:\n",
    " \n",
    "    - **AWS Lambda: Functions as a Service** - A serverless compute service that lets you run code without having to provision a server. It also automatically scales your applications in response to incoming requests. If your organisation uses Azure Functions, or Google Cloud Functions you'll find the concepts very similar.\n",
    "\n",
    "    - **Amazon EventBridge: Event Orchestration Service** - A serverless \"event bus\" that makes it easy to connect applications together using data from your own applications. While your organisation might use Azure Event Grid, Google Cloud Pub/Sub, or Apache Kafka for event orchestration, the fundamental patterns of event-driven architecture remain the same.\n",
    "    \n",
    "    - **Amazon Kinesis Data Stream: Streaming Broker** - A massively scalable and durable real-time data streaming service. Think of it as a continuously flowing river of data that can handle hundreds of terabytes of data per hour from multiple sources! The patterns you learn here apply equally to Azure Event Hubs, Google Cloud Pub/Sub streams, or Apache Kafka topics.\n",
    "    \n",
    "    - **Amazon Kinesis Firehose: Streaming Consumer** - A fully managed service that reliably loads streaming data into data lakes, data stores, and analytics services. These concepts map directly to services like Azure Stream Analytics, Google Cloud Dataflow, or Kafka Connect in your organisation.\n",
    "    \n",
    "    - **AWS Glue: Serverless ETL Service** - A fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load your data for analytics. Whether your organisation uses Azure Data Factory, Google Cloud Dataflow, or Apache NiFi, the ETL principles remain consistent.\n",
    "    \n",
    "    - **Amazon Athena: Serverless Query Service** - An interactive query service that makes it easy to analyse data directly in Amazon S3 using standard SQL. Your organization might use Azure Synapse Analytics serverless SQL pools (we used these in workshop 7) or Google BigQuery, but the concept of serverless SQL querying is universal.\n",
    "    \n",
    "    - **Amazon Redshift: Cloud Data Warehouse** - A fully managed, OLAP data warehouse service. While your organisation might use Azure Synapse Analytics dedicated SQL pools (we used these in workshop 7), Google BigQuery, or Snowflake, the fundamental concepts of cloud data warehousing remain the same.\n",
    "    \n",
    "    These services work together to create a comprehensive data engineering pipeline: EventBridge can trigger Lambda functions, which can process data and send it to Kinesis Streams. Firehose can then load this data into S3, where it can be cataloged by Glue, queried by Athena, and eventually loaded into Redshift for complex analytics! We will do all this together!\n",
    "\n",
    "- **Explains the deployment approaches processes for new data pipelines and automated processes. (K8)**  \n",
    "   - We walk through the complete deployment of a real-time weather data pipeline in a learning environment, using an infrastructure-as-code approach with CloudFormation. The pipeline flows through these key layers:<br><br>\n",
    "\n",
    "    **‚ú® 1. PRODUCER:** Lambda function collects weather data via API<br>\n",
    "    üîª<br>\n",
    "    **‚û°Ô∏è‚û°Ô∏è‚û°Ô∏è 2. BROKER:** Kinesis Data Stream buffers real-time data<br>\n",
    "    üîª<br>\n",
    "    **üì© 3. CONSUMER:** Kinesis Firehose manages delivery<br>\n",
    "    üîª<br>\n",
    "    **ü™£ 4. STORAGE:** S3 data lake with smart partitioning<br>\n",
    "    üîª<br>\n",
    "    **üìà 5. ANALYTICS/SERVING:** Athena for querying, Redshift for warehousing<br>\n",
    "    \n",
    "    - In Going Further, there is an exercies in automating and orchestrating this entire flow through Glue Workflows, demonstrating how to coordinate data ingestion, transformation, and loading processes in a systematic way. Note we will consider orchestration practically and in more detail in Workhsop 10.\n",
    "\n",
    "- **Demonstrates how they used security, scalability and governance when automating data pipelines using programming languages and data integration platforms with graphical user interfaces. (K13, S4)\"** \n",
    "    - While this workshop uses purposefully permissive security settings (public subnets, broad IAM roles, simplified authentication) to facilitate learning, we will emphasise how these would need to be hardened for production use, such as implementing private subnets with NAT Gateways, restrictive security groups, and AWS Secrets Manager for credentials.\n",
    "\n",
    "````\n",
    "\n",
    "### üå¨Ô∏è Today's Workshop Scenario and Objectives\n",
    "\n",
    "Real-time weather monitoring is essential for many industries. Consider these business use cases for streaming weather data:\n",
    "\n",
    "- **Wind Energy**: Wind farm operators need immediate temperature alerts to prevent turbine damage and optimise power generation. When temperatures exceed 35¬∞C, they must quickly adjust operations to prevent overheating of mechanical components. Historical temperature analysis helps schedule maintenance during optimal weather windows, potentially saving millions in repair costs.\n",
    "\n",
    "- **Transportation**: Railway operators monitor temperature changes across thousands of miles of track, as sudden temperature variations can compromise rail integrity. Their monitoring systems must detect concerning temperature gradients in real-time while building historical analysis to identify vulnerable sections requiring extra monitoring.\n",
    "\n",
    "- **Retail**: Major supermarket chains adjust inventory and staffing based on temperature forecasts and real-time conditions. A sudden temperature spike can drive demand for certain products up by 300%, requiring rapid supply chain adjustments based on both current readings and historical sales patterns.\n",
    "\n",
    "- **Smart Buildings**: Modern commercial buildings use temperature data streams to automatically optimize HVAC operations (Heating, ventilation, and air conditioning), typically achieving 30-50% energy savings through real-time adjustments and predictive optimisation based on historical patterns. This requires both immediate response to current conditions and analysis of long-term temperature trends.\n",
    "\n",
    "For our workshop, we'll focus on wind energy, where weather data directly impacts operations and revenue. As noted in \"The Impact of Weather on Wind Farms\" (2024): https://www.infoplaza.com/en/blog/the-impact-of-weather-on-wind-farms\n",
    "\n",
    "\n",
    "> *\"The power output of a turbine is related to density, which is a function of altitude, pressure, and also temperature. Dense air (which comes with lower temperatures) exerts more force on the rotors, resulting in a higher power output, even at relatively lower wind speeds compared to warmer and less dense air.\"*\n",
    "\n",
    "![Turbines](./images/turbine.png)\n",
    "\n",
    "A wind farm operator has approached you to prototype a real-time temperature monitoring system. While their production system will need to monitor hundreds of turbine locations and track multiple weather parameters (temperature, wind speed, humidity), they want to start with a proof-of-concept using just temperature data from three cities to validate the approach. This temperature monitoring is needed as *\"extreme heat can cause overheating of the turbine's electrical and mechanical components as well as lubrication systems, potentially leading to shutdowns and increased maintenance costs.\"*\n",
    "\n",
    "This represents a typical project progression that you learned about in module `9.1\tFrom Prototype to Production: Implementing Data Solutions`.\n",
    "\n",
    "1. **Scoping (Current)**: We've identified the core requirements - tracking temperature variations that could impact turbine performance and maintenance scheduling\n",
    "2. **Prototype (This Workshop)**: Building a working demo with 3 locations using AWS services to get rapid stakeholder feedback\n",
    "3. **Development**: Will expand to all turbine locations, add wind speed and humidity monitoring, implement production-grade security, and integrate with turbine control systems\n",
    "4. **Production**: Full deployment with comprehensive monitoring, automated scaling, and disaster recovery\n",
    "5. **Continuous Improvement**: Regular evaluation of performance, costs, and new requirements\n",
    "\n",
    "Using AWS's serverless and streaming services, we'll build this initial prototype. By the end of this workshop, you will have:\n",
    "\n",
    "- Set up a complete data streaming infrastructure using CloudFormation  \n",
    "- Created an automated data collection system that pulls weather data every minute using Lambda\n",
    "- Built a resilient streaming pipeline using Kinesis services\n",
    "- Developed a streaming pipeline that:\n",
    " - Processes weather readings from three UK cities (expandable to more locations)\n",
    " - Automatically partitions data by location and time for efficient querying\n",
    "- Tested the pipeline by:\n",
    " - Monitoring real-time data flows through CloudWatch\n",
    " - Querying historical weather patterns using Athena\n",
    " - Creating analytical views for temperature trend analysis\n",
    "\n",
    "This workshop will provide practical experience with:\n",
    "- Infrastructure-as-code deployment using CloudFormation\n",
    "- Real-time data collection using Lambda and EventBridge\n",
    "- Stream processing with Kinesis Data Streams and Firehose\n",
    "- Data lake organisation and partitioning strategies \n",
    "- SQL analytics using Athena and Redshift\n",
    "- ETL workflow automation / orchestration using AWS Glue\n",
    "\n",
    "In this prototype, your pipeline will perform two main operations:\n",
    "1. **Real-time Processing**: Continuously collect and stream temperature data from multiple locations, helping operators monitor conditions that could impact turbine performance\n",
    "2. **Historical Analytics**: Automatically organise historical weather data in the data lake for analysing temperature patterns and optimising maintenance schedules\n",
    "\n",
    "By the end of the workshop, you will have built this architecture:\n",
    "\n",
    "![Workshop_09_architecture.drawio](./images/Workshop_09_architecture.drawio.png)\n",
    "\n",
    "## ‚öôÔ∏è Task 1: Configuring the Development Environment\n",
    "\n",
    "> In Task 1, you will set up a development environment where weather data streaming and analytics can take place. This involves deploying a comprehensive AWS infrastructure using CloudFormation, which will create multiple interconnected services including a data ingestion pipeline (Lambda and Kinesis), storage layer (S3 data lake), and foundational analytics services (Redshift cluster and Glue database).\n",
    "\n",
    "1. **Start an ACG AWS sandbox:** \n",
    "  - Use this link and start `AWS Sandbox - Default` and log in: https://learn.acloud.guru/cloud-playground/cloud-sandboxes <br>\n",
    "  ![AWS_ACG_start](./images/AWS_ACG_start.png)\n",
    "\n",
    "2. **Deploy AWS CloudFormation template:** \n",
    "  - Once logged in search for `CloudFormation`. \n",
    "  - At the top, click on the far right drop down `Create stack` and select `With new resources (standard)`\n",
    "  - At `Step 1: Create Stack` copy and paste this URL `https://da5corndel.s3.eu-west-2.amazonaws.com/CloudFormation_streaming.yaml` into the `Amazon S3 URL` box and click `Next`.\n",
    "  ![CloudFormation_S3_URL](./images/CloudFormation_S3_URL.png)<br><br>\n",
    "  - At `Step 2: Specify stack details` notice how may of the parameters are pre-completed for you and don't need to be changed. The only task here is to complete `Provide a stack name`. As this name is used to dynamically create resource names using the name you give the stack. For this reason, a simple name is recommended of `stream` all in lower case. Then click `Next`.\n",
    "  ![CloudFormation_step2](./images/CloudFormation_step2.png)<br><br>\n",
    "  - At `Step 3: Configure stack options` scroll down to the bottom of the page then simply tick the check box next to the statement: `I acknowledge that AWS CloudFormation might create IAM resources with customised names.` then click `Next`.\n",
    "  - At `Step 4: Configure stack options`, scroll to the bottom of the page and click the `Submit` button. Your stack (we called `stream`) will now deploy and will show `‚Ñπ CREATE_IN_PROGRESS` while it deploys. After about 3 minutes it should show the message `‚Ñπ CREATE_COMPLETE`.\n",
    "\n",
    "````{dropdown} While the template deploys, click here for useful information and troubleshooting tips we reccomend you read.\n",
    "```{note}\n",
    "‚åö Using an AWS CloudFormation template in this workshop automates the deployment of our entire streaming analytics architecture through infrastructure-as-code (IaC). The template provisions and configures network infrastructure (VPC, subnets, gateways), security components (IAM roles, security groups), and all required services (Lambda, Kinesis, S3, Redshift, Glue) with appropriate permissions and connections between them.\n",
    "\n",
    "For learning purposes, the template uses simplified security settings: public subnets instead of private ones, basic authentication rather than AWS Secrets Manager, and permissive security group rules that allow broad access. A production environment would need significant hardening, including private subnets with NAT gateways, strict security controls, and high-availability configurations.\n",
    "\n",
    "Learn more about CloudFormation templates here: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html\n",
    "```\n",
    "\n",
    "```{Note}\n",
    "‚åõ Your ACG Azure sandbox will automatically shut down and its data will be deleted after four hours. You will receive a notification one hour before the sandbox expires, allowing you to extend it for an additional four hours. Please plan your work accordingly to avoid disruptions.\n",
    "```\n",
    "\n",
    "```{Note}\n",
    "üîÉ If you encounter deployment issues or want to start fresh, we recommend deleting the entire sandbox from the ACG playground rather than individual resources or the CloudFormation stack. This is faster and ensures a clean slate for redeployment.\n",
    "\n",
    "The CloudFormation stack includes retention settings for resources like S3 buckets to avoid accidental data loss, and deletion can take 15‚Äì20 minutes due to dependencies between components (e.g., Lambda, Kinesis, Redshift). If you must delete the stack, empty S3 buckets manually first, as their deletion protection will block the process.\n",
    "```\n",
    "\n",
    "```{Note}\n",
    "‚õî When using AWS Glue, your ACG sandbox may shut down due to exceeding Glue DPU (Data Processing Unit) limits. You‚Äôll receive an email titled ‚ÄúYour Hands-On Lab or Cloud Playground has been shut down,‚Äù explaining the suspension due to excessive DPU usage.\n",
    "\n",
    "AWS Glue jobs, being Spark-based, provision distributed computing environments even for small tasks, which can quickly hit ACG's limits designed to prevent runaway costs. This restriction is a helpful reminder of resource management in data processing.\n",
    "\n",
    "If your sandbox is suspended, don‚Äôt worry, this is part of learning to use powerful tools like AWS Glue. Simply start a new sandbox and redeploy the CloudFormation template, which will be ready in 3‚Äì4 minutes. Learn more about Glue DPUs and optimisation here: https://docs.aws.amazon.com/glue/latest/dg/monitor-debug-capacity.html\n",
    "```\n",
    "````\n",
    "\n",
    "3. **How to explore your stack `Outputs`**\n",
    "  - Once your stack shows `‚Ñπ CREATE_COMPLETE`, click on the `Outputs` tab. This is a set of URLs created by the deployment for the key resources you will use in this workshop. Ideally keep this page open during the workhsop so you can come back here and easily navigate to different parts of the application.\n",
    "  - Let's start by simply opening the S3 bucket that will be our data lake. Right click on the URL  to the right of the key `DataLakeBucketURL` and select `Open link in new tab`. This will open the S3 bucket used to store streamed weather data in this application.\n",
    "  ![CloudFormation_datalake](./images/CloudFormation_datalake.png)<br>\n",
    "  - Great, now that you know how to quickly navigate key resources in this data streaming application, in the next section we will open and explore key resources in a logical order across this architecture, beginning with the Producer!!\n",
    "\n",
    "   \n",
    "## üó∫Ô∏è Task 2: Explore and understand your streaming application so far\n",
    "\n",
    "> In this task, we will systematically explore the core data processing pipeline that has been provisioned for you. You'll examine how weather data flows through the ingestion layer (Lambda and Kinesis), into the storage layer (S3 data lake), creating the foundation for analytics workloads.\n",
    "\n",
    "    **‚ú® 1. PRODUCER:** Lambda function collects weather data via API<br>\n",
    "    üîª<br>\n",
    "    **‚û°Ô∏è‚û°Ô∏è‚û°Ô∏è 2. BROKER:** Kinesis Data Stream buffers real-time data<br>\n",
    "    üîª<br>\n",
    "    **üì© 3. CONSUMER:** Kinesis Firehose manages delivery<br>\n",
    "    üîª<br>\n",
    "    **ü™£ 4. STORAGE:** S3 data lake with smart partitioning<br>\n",
    "    üîª<br>\n",
    "    **üìà 5. ANALYTICS/SERVING:** Athena for querying, Redshift for warehousing<br>\n",
    "\n",
    "### ‚ú® 1 PRODUCER: Explore the Data Producer (Lambda Function):\n",
    "\n",
    "- This represents the start where real-time weather data enters our system. The combination of Lambda and EventBridge creates a reliable, serverless data collection mechanism that will continuously feed data into our streaming pipeline. Let's explore them.\n",
    "- First, from your CloudFormation `Outputs` tab, locate the `LambdaFunctionURL` and open it in a new tab.\n",
    "- This Lambda function serves as our data producer - think of it as an automated weather station that collects and reports data every minute.\n",
    "![Lambda](./images/Lambda.png)<br>\n",
    "\n",
    "````{dropdown} üå¶Ô∏è Before we explore the code, let's understand our data source by clicking here to expand.\n",
    "```{Note}\n",
    "We're using the Open-Meteo Weather API (https://open-meteo.com/), which provides free weather data for educational and development purposes. We chose this API deliberately for our workshop because its both free and doesn't require any authentication or API keys, eliminating the complexity of credential management. This means the application starts calling the API immediately without you needing to register and manage access tokens. \n",
    "\n",
    "In a production environment, you'd typically need to handle API authentication, rate limiting, and usage tracking, but by removing these complexities here, we can focus purely on building our data pipeline and analytics capabilities. We will explore these important production considerations in Workshop 13: 'Integrating  API Data Sources'.\n",
    "      \n",
    "You can explore the API documentation at https://open-meteo.com/en/docs to understand all the available weather parameters. For our workshop, we're using just a subset of the available data (current temperature) to keep things focused, but in a real-world application, you might want to collect additional parameters like humidity, wind speed, or precipitation.\n",
    "```\n",
    "````\n",
    "\n",
    "   - Let's examine how the Lambda function collects and streams weather data bey reviewing its Python code shown below.\n",
    "   - The Lambda handler function manages the flow of data flow by pulling from the weather API to a Kinesis data stream. It uses `boto3` to connect to Kinesis, with the stream name configured through environment variables from our CloudFormation template.\n",
    "   - The process follows a logical flow of:\n",
    "        - The code stores coordinates for three UK cities (London, Manchester, Edinburgh) in a list called `locations`.\n",
    "        - A helper function called ` def fetch_weather_data()` handles the API interaction, transforming raw weather data into a structured format with city name, temperature, and timestamps\n",
    "        - The main processing loop (that begins with `for location in locations:`) collects data for each city from the API and streams it to a Kinesis data stream, using the city name as the partition key for organised downstream processing we will look at shortly.\n",
    "        - Logging tracks successes and failures and can be seen below as the `response` object finally retunred by the overall Lambda handler function. \n",
    "\n",
    "````{dropdown} üßë‚Äçüíª Clicking here to view the Python Code in the Lambda function\n",
    "```{code-block} python\n",
    "import json\n",
    "import boto3\n",
    "import urllib.request\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Lambda handler that fetches weather data from Open-Meteo API and streams it to Kinesis.\n",
    "    The function processes multiple UK cities and structures the data for downstream analytics.\n",
    "    \n",
    "    Args:\n",
    "        event: AWS Lambda event trigger data\n",
    "        context: AWS Lambda runtime information\n",
    "    \n",
    "    Returns:\n",
    "        dict: Response containing execution status and processing summary\n",
    "    \"\"\"\n",
    "    # Initialize Kinesis client - Lambda automatically uses the correct region\n",
    "    kinesis_client = boto3.client('kinesis')\n",
    "    \n",
    "    # Reference the Kinesis stream created by CloudFormation\n",
    "    stream_name = f\"{os.environ['PROJECT_NAME']}-stream-{os.environ['ENVIRONMENT']}\"\n",
    "\n",
    "    # Define UK cities to monitor - structured for potential expansion\n",
    "    locations = [\n",
    "        {\"city\": \"London\", \"latitude\": 51.5072, \"longitude\": -0.1276},\n",
    "        {\"city\": \"Manchester\", \"latitude\": 53.4808, \"longitude\": -2.2426},\n",
    "        {\"city\": \"Edinburgh\", \"latitude\": 55.9533, \"longitude\": -3.1883}\n",
    "    ]\n",
    "\n",
    "    def fetch_weather_data(location):\n",
    "        \"\"\"\n",
    "        Fetches current weather data for a given location using Open-Meteo API.\n",
    "        \n",
    "        Args:\n",
    "            location (dict): Dictionary containing city name, latitude, and longitude\n",
    "        \n",
    "        Returns:\n",
    "            dict: Weather data including city, temperature, measurement time, and collection time\n",
    "                  Returns None if data fetch fails\n",
    "        \"\"\"\n",
    "        base_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "        params = f\"?latitude={location['latitude']}&longitude={location['longitude']}&current_weather=true\"\n",
    "        \n",
    "        try:\n",
    "            response = urllib.request.urlopen(base_url + params)\n",
    "            data = json.load(response)\n",
    "            \n",
    "            return {\n",
    "                \"city\": location[\"city\"],\n",
    "                \"temperature\": data[\"current_weather\"][\"temperature\"],\n",
    "                \"measurement_time\": data[\"current_weather\"][\"time\"],\n",
    "                \"collection_time\": datetime.utcnow().isoformat(),\n",
    "                \"source\": \"open-meteo\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {location['city']}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    # Process each city and send data to Kinesis\n",
    "    successfully_processed = 0\n",
    "    failed_cities = []\n",
    "    \n",
    "    for location in locations:\n",
    "        try:\n",
    "            weather_data = fetch_weather_data(location)\n",
    "            if weather_data:\n",
    "                # Log the data being sent for monitoring and debugging\n",
    "                print(f\"Processing data for {location['city']}: {json.dumps(weather_data)}\")\n",
    "                \n",
    "                # Send to Kinesis stream with city as partition key\n",
    "                kinesis_client.put_record(\n",
    "                    StreamName=stream_name,\n",
    "                    Data=json.dumps(weather_data),\n",
    "                    PartitionKey=weather_data[\"city\"]\n",
    "                )\n",
    "                successfully_processed += 1\n",
    "            else:\n",
    "                failed_cities.append(location[\"city\"])\n",
    "        except Exception as e:\n",
    "            error_message = f\"Failed to process {location['city']}: {str(e)}\"\n",
    "            print(error_message)\n",
    "            failed_cities.append(location[\"city\"])\n",
    "\n",
    "    # Prepare detailed execution summary\n",
    "    response = {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": {\n",
    "            \"message\": f\"Processed {successfully_processed} of {len(locations)} cities\",\n",
    "            \"successful_count\": successfully_processed,\n",
    "            \"total_cities\": len(locations),\n",
    "            \"execution_time\": datetime.utcnow().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if failed_cities:\n",
    "        response[\"body\"][\"failed_cities\"] = failed_cities\n",
    "        \n",
    "    return response\n",
    "```\n",
    "````\n",
    "\n",
    "   - Now, let's understand how this lambda function gets triggered automatically by an Amazon EventBridge rule:\n",
    "        - In the `Function overview` section, in the diagram click on the `EventBridge (CloudWatch Events)` then right click on the URL that now appears in the `Configuration` section as shown below. (You could also simply search for `EventBridge` at the top of the page).\n",
    "        ![EventBridge](./images/EventBridge.png)<br>\n",
    "        - Notice in the `Event schedule` the text `Fixed rate of 1 minute`. Click on the `Edit` button to the right and notice how in the `Schedule pattern` you could change the rate unit to be any number of `Minutes`, `Hours` or `Days`. Or, you could also change the schedule type to be a cron expression for specific minutes, hours, day of month, month, day of week and year.\n",
    "        ![EventBridgeRate](./images/EventBridgeRate.png)<br>    \n",
    "        - This rule acts like an automated timer, invoking our Lambda function every minute.\n",
    "        - You can verify this by watching the CloudWatch logs for this rule where will see new entries appearing every minute. To do this:\n",
    "            - In the search bar at the top type `CloudWatch`, \n",
    "            - Then in the left hand menu of CloudWatch select from within `Logs`, `Log groups` then click on the Log group called `/aws/lambda/weather-analytics-producer-dev`\n",
    "            ![CloudWatchLogGroups](./images/CloudWatchLogGroups.png)<br>\n",
    "            - From the list of `Log streams` click on the container with the most recent `Last event time`. Expand some log entries and see if you can find entries from the `START` and `END` of one run of the Lambda function that has been triggered by the EventBridge rule you explored earlier. These markers wrap each function execution and between them you'll see log messages showing weather data for each city. By checking the timestamps between different START entries, you can verify the function is running every minute as configured.\n",
    "            ![CloudWatchLogEntries](./images/CloudWatchLogEntries.png)<br>\n",
    "\n",
    "```{note}\n",
    "üîé Keep an eye on the CloudWatch logs as you proceed through the workshop. They provide valuable insights into the data being collected and can help you troubleshoot any issues that arise.\n",
    "```\n",
    "\n",
    "### üóÉÔ∏è 2 BROKER: View data in the Data Stream (Kinesis):\n",
    "\n",
    "  - We saw that the lambda function pulled weather data for three cities from the weather API and streamed those to a Kinesis data stream with the following code: `kinesis_client.put_record(StreamName=stream_name, Data=json.dumps(weather_data), PartitionKey=weather_data[\"city\"])` Let's now explore that resource to understand how this broker works.\n",
    "    \n",
    "  - From your CloudFormation `Outputs` tab, locate `KinesisStreamURL` and open in a new tab.\n",
    "    \n",
    "  - This Kinesis stream acts as our real-time data buffer, receiving weather data from the Lambda function and temporarily storing it for downstream consumers like Kinesis Firehose. Think of it as a moving window of data that maintains records for 24 hours.\n",
    "    \n",
    "  - The stream uses the city name as a partition key, helping organize data for downstream processing. This partitioning ensures data from the same city goes to the same shard, maintaining order within each city's data stream. This was achieved in the Python code shown above where we set `PartitionKey=weather_data[\"city\"]`.\n",
    "    \n",
    "  - Let's now use the Data Viewer to inspect data in our Kinesis Stream:\n",
    "      1. Select the `Data Viewer` tab\n",
    "      2. Choose the single shard available from the `Shard` drop-down\n",
    "      3. From `Starting Position` drop-down, select `Trim horizon`\n",
    "      4. Click `Get records` to view the most recent weather data from our cities\n",
    "    ![KinesisDataStreamDataViewer](./images/KinesisDataStreamDataViewer.png)<br>\n",
    "\n",
    "```{note}\n",
    "üìÅ Our stream uses one shard because our data volume (3 cities √ó 1 record/minute = 3 records/minute) is well within a single shard's capacity of 1,000 records/second. This is perfect for workshop purposes and cost-efficient. VALIDATE WITH DOCS IN GEMINI\n",
    "```\n",
    "```{note}\n",
    " ‚åö`Trim horizon` works because it shows all records in the stream's 24-hour retention window, letting you see historical data. `Latest` might not show records immediately because it only shows data that arrives after you start viewing. Since our Lambda writes once per minute, you might need to wait for the next data collection cycle to see new records.\n",
    "```\n",
    "\n",
    "### üì© 3 CONSUMER: Examine the Data Transformation (Kinesis Firehose):\n",
    "  - From your CloudFormation `Outputs` tab, locate `KinesisFirehoseURL` and open in a new tab.\n",
    "    - Kinesis Firehose acts as our delivery service, taking data from the Kinesis stream and preparing it for long-term storage in S3. Let's explore its key configurations:\n",
    "        - Click on the `weather-analytics-firehose-dev` delivery stream\n",
    "        - Select `Edit destination settings` to view the configuration details\n",
    "        ![KinesisFirehoseDestination](./images/KinesisFirehoseDestination.png)<br>\n",
    "\n",
    "    - Examine these important settings:\n",
    "        - **Dynamic Partitioning**: Notice how this feature is enabled in the `Dynamic partitioning` section. It automatically organises our data in S3:\n",
    "            - Enabled with a partition key based on `city` using the JQ expression `.city`\n",
    "            - Creates a logical hierarchy in S3 using the prefix pattern you can see of:\n",
    "              ```\n",
    "              weather-data/location=!{partitionKeyFromQuery:city}/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/\n",
    "              ```\n",
    "            - This pattern means data is automatically organised by city, year, month, and day\n",
    "        \n",
    "        - **Error Handling**: Notice the error output prefix:\n",
    "            ```\n",
    "            errors/!{firehose:error-output-type}/!{timestamp:yyyy}/!{timestamp:MM}/!{timestamp:dd}/\n",
    "            ```\n",
    "            This helps track and debug any processing issues by organising any error logs in a similar hierarchical structure  \n",
    "        - **Buffering**: Scroll to the bottom of the page and expand the section `Buffer hints, compression, file extension and encryption`. Notice how Firehose buffers data for either:\n",
    "            - 60 seconds (time-based buffer) OR\n",
    "            - 64MB (size-based buffer)\n",
    "            - Whichever threshold is met first triggers a write to S3\n",
    "        \n",
    "```{note}\n",
    "üì¶ The buffering configuration balances between data freshness and storage efficiency. Smaller buffers mean fresher data but more S3 write operations, while larger buffers are more cost-effective but introduce more latency. Our 60-second buffer is ideal for our workshop environment where we want to see results quickly.\n",
    "\n",
    "üóÇÔ∏è The dynamic partitioning structure provides efficient querying later. By organising data by city and time components, we can quickly locate specific data subsets without scanning the entire dataset. For example, finding all London temperatures for a specific month becomes a targeted operation.\n",
    "```\n",
    "\n",
    "### üìÇ 4 STORAGE: Inspect the Data Lake (S3):\n",
    "   - From your CloudFormation `Outputs` tab, locate `DataLakeBucketURL` and open in a new tab. \n",
    "\n",
    "   - This S3 bucket serves as our data lake, providing long-term storage of our weather data in an organised and cost-effective way. Let's explore its structure:\n",
    "       1. Click into the `weather-data` folder\n",
    "       2. Notice the hierarchical organisation created by Kinesis Firehose we explored previously:\n",
    "           - First level: `location=cityname` (e.g., `location=London`)\n",
    "           - Second level: `year=YYYY`\n",
    "           - Third level: `month=MM`\n",
    "           - Fourth level: `day=DD`\n",
    "       3. Navigate down through these levels to find the actual data files\n",
    "       4. Notice the `.gz` extension on the files - this indicates GZIP compression\n",
    "   \n",
    "   - Key features to observe:\n",
    "       - **Partitioning Structure**: The folder hierarchy directly matches the Firehose prefix pattern we examined earlier:\n",
    "         ```\n",
    "         weather-data/location=city/year=YYYY/month=MM/day=DD/\n",
    "         ```\n",
    "         This isn't just tidy organisation, it supports  efficient querying\n",
    "       \n",
    "       - **File Formats**: \n",
    "           - Files are automatically compressed using GZIP\n",
    "           - Each file contains multiple weather records collected during the 60-second buffer window\n",
    "           - File names include timestamp information for easy tracking\n",
    "       \n",
    "       - **Lifecycle Management**: The bucket has intelligent lifecycle rules that were defined in the CloudFormation template you deployed:\n",
    "           - Data moves to Infrequent Access (IA) storage after 90 days\n",
    "           - Archives to Glacier storage after 180 days\n",
    "           - This tiered approach optimises storage costs\n",
    "\n",
    "```{note}\n",
    "üíæ The combination of GZIP compression and intelligent lifecycle policies helps manage storage costs as your data grows. For examle, in a production environment, with years of historical weather data for many more locations the savings could become significant.\n",
    "\n",
    "üéØ The partitioned structure isn't just for organisation, it enables targeted data access. When you later query this data with Athena or process it with Glue, it can efficiently access specific time periods or locations without scanning the entire dataset.\n",
    "```\n",
    "\n",
    "### üìà 5 ANALYTICS / SERVING: Preview the Analytics Foundation:\n",
    "   - Let's explore the analytical and data serving layer that will help us derive insights from our weather data. We'll examine both components:\n",
    "       - AWS Glue for data cataloging and ETL\n",
    "       - Amazon Redshift for data warehousing\n",
    "   \n",
    "   - First, let's look at our AWS Glue setup:\n",
    "       - Open the `GlueDatabaseURL` from your CloudFormation `Outputs` tab in a new tab\n",
    "       - Click into the database named `weather-analytics-dev-db`\n",
    "       - Notice it's currently empty - we'll populate it in the next task\n",
    "       - Observe the database location points to our S3 data lake:\n",
    "          ```\n",
    "          s3://weather-analytics-data-lake-dev-[YOUR-ACCOUNT-ID]/processed/\n",
    "          ```\n",
    "      ![GlueDatabase](./images/GlueDatabase.png)<br>\n",
    "   - Next, examine the Redshift configuration:\n",
    "       - Open the `RedshiftClusterURL` from your CloudFormation `Outputs` tab in a new tab\n",
    "       - Notice the cluster configuration:\n",
    "           - Single-node cluster (`dc2.large`) suitable for workshop volumes\n",
    "           - Publicly accessible for workshop simplicity\n",
    "           - Located in the same VPC as other components\n",
    "   \n",
    "   - Key architectural features:\n",
    "       - **Data Catalog Integration**: \n",
    "           - Glue database is pre-configured to work with our S3 data lake\n",
    "           - Supports both batch and streaming analytics (we will do both later)\n",
    "           - Will enable schema discovery and ETL job creation (which we will do later)\n",
    "       \n",
    "       - **Warehouse Configuration**:\n",
    "           - Redshift cluster sized appropriately for workshop loads\n",
    "           - VPC security groups allow necessary access\n",
    "           - Set up for both direct queries and ETL operations\n",
    "\n",
    "```{note}\n",
    "üèóÔ∏è This foundation sets us up for both batch and real-time analytics. The combination of Glue (for ETL and cataloging) and Redshift (for warehousing) provides a robust platform for deriving insights from our weather data.\n",
    "\n",
    "üîê While this setup uses simplified security for workshop purposes (public access, basic credentials), production environments should implement:\n",
    "- Private subnets with NAT Gateways\n",
    "- AWS Secrets Manager for credentials\n",
    "- More restrictive security group rules\n",
    "- Enhanced VPC endpoint policies\n",
    "```\n",
    "\n",
    "Now that you understand how data flows through the system - from collection through streaming and into storage - you're ready to build the analytics capabilities in the next task! You'll create crawlers to catalog this data, develop ETL jobs to transform it, and ultimately load it into Redshift for analysis.\n",
    "\n",
    "```{note}\n",
    "ü™ü Keep the CloudFormation Outputs tab open as you continue working - you'll frequently refer back to these resources throughout the workshop.\n",
    "```\n",
    "\n",
    "## üîé Task 3: Data Lake Exploration with Glue and Athena\n",
    "\n",
    "> We've been asked to create a dashboard of... updated.. real time up \n",
    "\n",
    "> Remember to make point can now stream into Redshift https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html\n",
    "\n",
    "> In this task, we'll first use AWS Glue to automatically discover and catalog our data structure, then use Athena to query this cataloged data. This represents best practice for data lake exploration, ensuring consistent schema management across your analytics services.\n",
    "\n",
    "![glue](https://docs.aws.amazon.com/images/glue/latest/dg/images/HowItWorks-overview.png)\n",
    "\n",
    "> Image from: https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\n",
    "\n",
    "\n",
    "### üìã Set Up Glue Crawler:\n",
    "\n",
    "- **Step 1: Set crawler properties**:\n",
    "   - In the AWS Console, search for `Glue` and open it\n",
    "   - In the left navigation menu, expand `Data Catalog` and select `Crawlers`.\n",
    "   - Click the `Create crawler` button on the right.\n",
    "   - Name: `weather-data-crawler`\n",
    "   - Click `Next`\n",
    "\n",
    "- **Step 2: Choose data sources and classifiers**:\n",
    "   - Click on `Add a data source`.\n",
    "   - Leave the default settings of `S3` as the data source and the location of the S3 data as `In this account`.\n",
    "   - For the S3 path click  `Browse S3` and click on\n",
    "     `s3://weather-analytics-data-lake-dev-[YOUR-ACCOUNT-ID]/weather-data/`\n",
    "   ![GlueS3path](./images/GlueS3path.png)<br>\n",
    "   - Even though you have added the S3 path to be crawled, the box may show a message in red of `This is a required field`. Just hit the tab key and it will be accepted.\n",
    "   - Leave the default setting of `Crawl all sub-folders`\n",
    "   - Click `Add an S3 data source`\n",
    "   ![GlueNext](./images/GlueNext.png)<br>\n",
    "   - Click `Next`\n",
    "\n",
    "- **Step 3: Configure security settings**:\n",
    "   - From the IAM role drop down menu select the role already created for you, `weather-analytics-glue-role-dev`\n",
    "   - Click `Next`\n",
    "   ![GlueRoleSelect](./images/GlueRoleSelect.png)<br>\n",
    "   \n",
    "```{note}\n",
    "üîê The CloudFormation template already created this role with appropriate permissions for the crawler to access S3 and create catalog entries.\n",
    "```\n",
    "- **Step 4 Set output and scheduling**:\n",
    "   - Target Database: Select `weather_analytics_dev_db`\n",
    "   - For table prefix, enter: `raw_`\n",
    "   - Crawler schedule Frequency:  `On demand`\n",
    "   - Click `Next`\n",
    "   ![GlueOutput](./images/GlueOutput.png)<br>\n",
    "   \n",
    "- **Step 5: Review and create**:\n",
    "   - Review your settings\n",
    "   - Click `Create crawler`\n",
    "\n",
    "### üèÉ‚Äç‚ôÇÔ∏è Run the Crawler:\n",
    "\n",
    "1. **Start Crawler**:\n",
    "   - Ensuring your new crawler is showing..\n",
    "   - Click `Run crawler`\n",
    "   - Wait for completion (usually around 2 minutes)\n",
    "\n",
    "2. **Verify Results and Edit Schema**:\n",
    "   - From the left hand navigation menu, expand `Data Catalog` and click on `Tables`, then select the table name created by the carwerly you implicity named with the `raw_` prefix eealire that should be called `raw_weather_data`\n",
    "   - Click on the table to examine its schema\n",
    "   ![GlueTableSchema](./images/GlueTableSchema.png)<br>\n",
    "   - At the top right of the Schema click `Edit schemas as JSON` and for the two fields of `measurement_time` and `collection_time` modify the type from `string` to `timestamp` then click `Save as new table version`.\n",
    "   ![GlueTableEditSchema](./images/GlueTableEditSchema.png)<br>\n",
    "\n",
    "```{note}\n",
    "üìö Notice how Glue automatically:\n",
    "- Detected the JSON structure\n",
    "- Identified some but not all data types (we changed two columns to timestamp)\n",
    "- Recognised the partitioning scheme (location/year/month/day)\n",
    "```\n",
    "\n",
    "###  ‚öôÔ∏è Set Up Athena Query Environment:\n",
    "\n",
    "1. **Configure Athena Settings**:\n",
    "   - In the AWS Console, search for `Athena` and open it in a new tab\n",
    "   - Then using the default option select `Launch query editor`<br>\n",
    "   ![AthenaQuery](./images/AthenaQuery.png)<br>\n",
    "   \n",
    "   - You should land on the `Editor` tab of Athena. Click on the `Settings` tab further to the right, then click the `Manage` button.\n",
    "   ![AthenaLanding](./images/AthenaLanding.png)<br>\n",
    "   - For the `Location of query result` box click `Browse S3` button to the right of it and select the bucket: `s3://weather-analytics-athena-results-dev-[YOUR-ACCOUNT-ID]/` then click `Choose`, then `Save`.\n",
    "   ![AthenaQueryS3](./images/AthenaQueryS3.png)<br>\n",
    "\n",
    "```{note}\n",
    "üìÅ We already created this results bucket in our CloudFormation template with appropriate lifecycle rules to clean up old query results automatically. This helps manage storage costs while maintaining useful query history.\n",
    "```\n",
    "\n",
    "\n",
    "2.  **Understanding the Athena default settings**:\n",
    "    - Click on the `Editor` tab of Athena.\n",
    "    - Look at the top right of the screen and note the default `primary` workgroup is selected. We will use this for our workshop.\n",
    "    - Looking on the left-hand side, for `Data source` and note that `AwsDataCatalog` is selected by default, and below that `catalogue` is none.\n",
    "    - Also, note that Athena has detected and selected the Glue database called `weather-analytics_dev_db` that was created by the CloudFormation template and is in our `AwsDataCatalog`, .\n",
    "    - Finally, look bottom left in the `Tables` section where the table `raw_weather_data` we created with the Glue Crawler can be seen ready to query!\n",
    "    - ![AthenaEditor](./images/AthenaEditor.png)\n",
    "\n",
    "```{note}\n",
    "üìö In production environments, creating separate workgroups is recommended. This allows for:\n",
    "\n",
    "-   Cost tracking and control\n",
    "-   Team-specific configurations\n",
    "-   Usage attribution\n",
    "\n",
    "The default settings you see are for the 'primary' workgroup.\n",
    "\n",
    "-   `AwsDataCatalog` is the system Athena uses to organize metadata. You can think of it as the root of your data organization.\n",
    "-   A catalog is a group of databases within the `AwsDataCatalog`. We are using the `AwsDataCatalog` with no sub-catalogs.\n",
    "-   Athena has selected the `weather-analytics_dev_db` database because it is the only database in the `AwsDataCatalog`.\n",
    "\n",
    "To learn more see: https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html\n",
    "```\n",
    "\n",
    "### üìä Inspect Your Weather Data:\n",
    "\n",
    "1. **Basic Data Exploration**:\n",
    "    - A quick way to quickly query the data in Athena is to click on the three dots to the right of a table then select `Preview Table`. This auto-genartes and runs working SQL to view the first 10 rows of the table. \n",
    "    ![AthenaPreviewTable](./images/AthenaPreviewTable.png)\n",
    "    - Or paste the code below into the query pane and click `Run`.\n",
    "\n",
    "```{code-block} sql\n",
    "SELECT *\n",
    "FROM raw_weather_data\n",
    "ORDER BY measurement_time DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "### üßΩ Create Clean View of Weather Data:\n",
    "\n",
    "1. **Understanding Raw Data**:\n",
    "   - First, let's examine our raw data to understand the duplication pattern:\n",
    "\n",
    "```{code-block} sql\n",
    "SELECT *\n",
    "FROM raw_weather_data\n",
    "ORDER BY city, measurement_time\n",
    "LIMIT 30;\n",
    "```\n",
    "   - Notice how we have:\n",
    "     - Multiple rows with the same temperature and measurement_time\n",
    "     - Different collection_times for the same reading\n",
    "     - Weather readings that update every 15 minutes\n",
    "\n",
    "2. **Create a Clean View**:\n",
    "   - Let's create a view that handles deduplication:\n",
    "\n",
    "```{code-block} sql\n",
    "CREATE OR REPLACE VIEW clean_weather_data AS\n",
    "SELECT \n",
    "    city,\n",
    "    temperature,\n",
    "    measurement_time,\n",
    "    MIN(collection_time) as first_collection_time,\n",
    "    COUNT(*) as collection_count,\n",
    "    location,\n",
    "    year,\n",
    "    month,\n",
    "    day\n",
    "FROM raw_weather_data\n",
    "GROUP BY \n",
    "    city,\n",
    "    temperature,\n",
    "    measurement_time,\n",
    "    location,\n",
    "    year,\n",
    "    month,\n",
    "    day;\n",
    "```\n",
    "\n",
    "```{note}\n",
    "üéØ This view:\n",
    "- Takes only the first collection of each unique reading\n",
    "- Maintains the partition columns for performance\n",
    "- Tracks how many times each reading was collected (useful for monitoring)\n",
    "```\n",
    "\n",
    "3. **Verify the View**:\n",
    "   - Let's check our view is working as expected:\n",
    "\n",
    "```{code-block} sql\n",
    "SELECT *\n",
    "FROM clean_weather_data\n",
    "ORDER BY city, measurement_time DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "### üìà Analyze Clean Data:\n",
    "\n",
    "1. **Temperature Trends**:\n",
    "   - Now we can write cleaner, more intuitive queries:\n",
    "\n",
    "```{code-block} sql\n",
    "SELECT \n",
    "    city,\n",
    "    DATE_TRUNC('hour', measurement_time) as hour,\n",
    "    AVG(temperature) as avg_temp,\n",
    "    COUNT(*) as readings_per_hour\n",
    "FROM clean_weather_data\n",
    "WHERE year = '2025' -- Change to current year\n",
    "    AND month = '01' -- Change to current month\n",
    "GROUP BY \n",
    "    city,\n",
    "    DATE_TRUNC('hour', measurement_time)\n",
    "ORDER BY \n",
    "    city, \n",
    "    hour DESC;\n",
    "```\n",
    "\n",
    "2. **City Comparisons**:\n",
    "\n",
    "```{code-block} sql\n",
    "SELECT \n",
    "    city,\n",
    "    COUNT(*) as total_readings,\n",
    "    ROUND(AVG(temperature), 1) as avg_temp,\n",
    "    ROUND(MIN(temperature), 1) as min_temp,\n",
    "    ROUND(MAX(temperature), 1) as max_temp,\n",
    "    ROUND(STDDEV(temperature), 2) as temp_variation\n",
    "FROM clean_weather_data\n",
    "WHERE year = '2025'\n",
    "    AND month = '01'\n",
    "GROUP BY city\n",
    "ORDER BY avg_temp DESC;\n",
    "```\n",
    "\n",
    "```{note}\n",
    "üí° Using the view:\n",
    "- Makes queries more readable\n",
    "- Ensures consistent deduplication\n",
    "- Improves query performance (less data processed)\n",
    "- Makes it easier to modify deduplication logic if needed\n",
    "```\n",
    "\n",
    "3. **Data Quality Monitoring**:\n",
    "   - We can also monitor our collection process:\n",
    "\n",
    "```{code-block} sql\n",
    "SELECT \n",
    "    city,\n",
    "    DATE(measurement_time) as date,\n",
    "    COUNT(*) as readings,\n",
    "    AVG(collection_count) as avg_collections_per_reading,\n",
    "    MAX(collection_count) as max_collections_per_reading\n",
    "FROM clean_weather_data\n",
    "WHERE year = '2025'\n",
    "    AND month = '01'\n",
    "GROUP BY \n",
    "    city,\n",
    "    DATE(measurement_time)\n",
    "ORDER BY \n",
    "    date DESC,\n",
    "    city;\n",
    "```\n",
    "\n",
    "4. **Leverage Partitioning**:\n",
    "\n",
    "```{code-block} sql\n",
    "SELECT \n",
    "    city,\n",
    "    AVG(temperature) as avg_temp,\n",
    "    MIN(temperature) as min_temp,\n",
    "    MAX(temperature) as max_temp,\n",
    "    DATE(measurement_time) as date\n",
    "FROM clean_weather_data\n",
    "WHERE location = 'London'\n",
    "    AND year = '2025' -- Change this to the current year\n",
    "    AND month = '01'  -- Change this to the current month\n",
    "GROUP BY city, DATE(measurement_time)\n",
    "ORDER BY date DESC;\n",
    "```\n",
    "\n",
    "```{note}\n",
    "üí° Notice how we use partition columns (location, year, month) in the WHERE clause. Athena uses these to read only relevant data files, making queries more efficient and cost-effective.\n",
    "```\n",
    "\n",
    "5. **City Comparison Analysis**:\n",
    "\n",
    "```{code-block} sql\n",
    "SELECT \n",
    "    city,\n",
    "    COUNT(*) as measurements,\n",
    "    ROUND(AVG(temperature), 2) as avg_temp,\n",
    "    ROUND(STDDEV(temperature), 2) as temp_stddev\n",
    "FROM clean_weather_data\n",
    "WHERE year = '2025'\n",
    "    AND month = '01'\n",
    "GROUP BY city\n",
    "ORDER BY avg_temp DESC;\n",
    "```\n",
    "\n",
    "### üìà Advanced Analytics:\n",
    "\n",
    "1. **Temperature Trends**:\n",
    "\n",
    "```{code-block} sql\n",
    "WITH hourly_temps AS (\n",
    "    SELECT \n",
    "        city,\n",
    "        DATE_TRUNC('hour', measurement_time) as hour,\n",
    "        AVG(temperature) as avg_temp\n",
    "    FROM clean_weather_data\n",
    "    WHERE year = '2025' AND month = '01'\n",
    "    GROUP BY city, DATE_TRUNC('hour', measurement_time)\n",
    ")\n",
    "SELECT \n",
    "    city,\n",
    "    hour,\n",
    "    avg_temp,\n",
    "    LAG(avg_temp) OVER (PARTITION BY city ORDER BY hour) as prev_hour_temp,\n",
    "    ROUND(avg_temp - LAG(avg_temp) OVER (PARTITION BY city ORDER BY hour), 2) as temp_change\n",
    "FROM hourly_temps\n",
    "ORDER BY city, hour DESC;\n",
    "```\n",
    "\n",
    "2. **Data Quality Checks**:\n",
    "\n",
    "```{code-block} sql\n",
    "SELECT \n",
    "    location,\n",
    "    year,\n",
    "    month,\n",
    "    day,\n",
    "    COUNT(*) as record_count,\n",
    "    COUNT(DISTINCT EXTRACT(hour FROM measurement_time)) as unique_hours,\n",
    "    MIN(measurement_time) as first_record,\n",
    "    MAX(measurement_time) as last_record\n",
    "FROM clean_weather_data\n",
    "WHERE year = '2025' AND month = '01'\n",
    "GROUP BY location, year, month, day\n",
    "ORDER BY location, year, month, day DESC;\n",
    "```\n",
    "\n",
    "```{note}\n",
    "‚úÖ These quality checks help identify any gaps in data collection. In a production environment, you might set up alerts based on these metrics to monitor data pipeline health.\n",
    "```\n",
    "\n",
    "### üéØ Challenge Exercises:\n",
    "\n",
    "Try writing queries to answer these questions:\n",
    "\n",
    "1. Which city has the most variable temperature (highest standard deviation)?\n",
    "2. What time of day typically records the highest temperatures?\n",
    "3. Calculate the rolling 3-hour average temperature for each city.\n",
    "\n",
    "```{note}\n",
    "üí° Hint: Look up Athena's window functions documentation for the rolling average calculation.\n",
    "```\n",
    "\n",
    "### Cost Management Best Practices:\n",
    "\n",
    "1. **Optimise Your Queries**:\n",
    "   - Always use partition filtering when possible\n",
    "   - Select only needed columns instead of SELECT *\n",
    "   - Use appropriate data types and compression\n",
    "\n",
    "2. **Monitor Query Metrics**:\n",
    "   - Click \"Recent queries\" to view:\n",
    "     - Data scanned per query\n",
    "     - Execution time\n",
    "     - Cost implications\n",
    "\n",
    "```{note}\n",
    "üí∞ Athena pricing is based on data scanned. Well-structured queries on partitioned data help minimise costs. The partitioning scheme we implemented (by city and date) helps optimise both query performance and cost.\n",
    "```\n",
    "\n",
    "## Wrapping Up and Reflecting on Your Project\n",
    "\n",
    "### Streaming, real time or near real time? What's the difference?\n",
    "\n",
    "\n",
    "```{note}\n",
    "While our focus today is on ...\n",
    "```\n",
    "````{dropdown} For more detail on ...., click here\n",
    "In this workshop, ..\n",
    "````\n",
    "\n",
    "```{admonition} Question\n",
    "Can you remember the difference between A and B?\n",
    "```\n",
    "`````{hint}\n",
    ":class: dropdown\n",
    "Think about ....\n",
    "````{dropdown} For the solution, click here\n",
    "The answer is ....\n",
    "````\n",
    "`````\n",
    "\n",
    "### Another topic\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### This afternoon - reflecting on your project\n",
    "\n",
    "With this morning's workshop exercise completed, you have now explored key processes for...\n",
    "\n",
    "This afternoon, we will consider how your project and its potential ...\n",
    "\n",
    "```{note}\n",
    "üéâ Congratulations - you've completed today's main exercise! \n",
    "\n",
    "Now that you've explored your weather data lake with Athena, if your up for the going further exercises, you're ready to move on to more advanced analytics using AWS Glue and Redshift. One task is to :\n",
    "- Create and run Glue crawlers\n",
    "- Build ETL jobs for data transformation\n",
    "- Load processed data into Redshift for high-performance querying\n",
    "\n",
    "```{note}\n",
    "üóíÔ∏è Keep your Athena queries handy - you'll use them to validate your ETL results and compare query performance between Athena and Redshift.\n",
    "```\n",
    "\n",
    "Below are three optional exercises that extend what you've learned. Choose any that:\n",
    "- Are relevant to your current role\n",
    "- Match your project's needs\n",
    "- Interest you technically\n",
    "\n",
    "Even if you don't complete them, consider reviewing what they cover in your own time, they demonstrate common patterns you might need later in your data engineering career.\n",
    "\n",
    "\n",
    "## üöÄ Going Further\n",
    "\n",
    "### **üéº Going Further 1:** Orchestrated pipeline to write to Redshift\n",
    "\n",
    "> So far we have crawled the data being streamed into S3 on-demand. A data analyst has asked for a table in Redshift they can explore and connect to PowerBI that is regularly updated with temperature readings. In this exercise we will create the target table in Redshift, then create a Glue visual ETL job that Extracts, Transforms and Loads the table from the Glue database to the Redshfit table. We query Redshift and visualise temperature over time. Finally, we orchestrate the full process by using AWS Glue `Workflows (orchestration)` to regularly run our S3 crawler (created in the main exercise), followed by the visual ETL.\n",
    "\n",
    "#### üéØ Create target table in Redshift\n",
    "\n",
    "1. **Connect to Redshift database**:\n",
    "   - In the search bar at the top, search for `Redshift` and open in a new tab.\n",
    "   - You will several `Access denied..` messages you can safely ignore (they are explained in the note below).\n",
    "   - Click on the only Redshfit cluster listed called `weather-stream`.\n",
    "   - Click on the `Query data` dropdown (that is on the top right) then select `Query in query editor`.\n",
    "   - Click `Connect to database` then in the `Database name` box enter `weather` and for the `Database user` enter `corndeladmin`, then click `Connect`.\n",
    "    ![RedshfitDatabaseConnect](./images/RedshfitDatabaseConnect.png)\n",
    "\n",
    "```{note}\n",
    "‚õî  The red `Access denied..` error messages can be safely ignored. They simply indicate that you don't have administrative permissions for Redshift Serverless backups, workgroups, and snapshots. You can still do everything needed for creating tables, querying data, and running ETL processes.\n",
    "```\n",
    "\n",
    "2. **Create Redshift target table**:\n",
    "   - In the Redshift Query editor copy and paste the SQL below then click `Run`.\n",
    "\n",
    "```{code-block} sql\n",
    "DROP TABLE IF EXISTS real_time_weather_stats;\n",
    "CREATE TABLE real_time_weather_stats (\n",
    "   window_start_time TIMESTAMP,\n",
    "   window_end_time TIMESTAMP,\n",
    "   city VARCHAR(255),\n",
    "   temperature DOUBLE PRECISION,\n",
    "   previous_temperature DOUBLE PRECISION,\n",
    "   temperature_change DOUBLE PRECISION,\n",
    "   location VARCHAR(255),\n",
    "   source VARCHAR(50),\n",
    "   collection_attempts INTEGER,\n",
    "   collection_window_seconds INTEGER,\n",
    "   initial_latency_seconds INTEGER,\n",
    "   last_update TIMESTAMP,\n",
    "   ingest_year INTEGER,\n",
    "   ingest_month INTEGER, \n",
    "   ingest_day INTEGER,\n",
    "   ingest_hour INTEGER\n",
    ")\n",
    "DISTKEY(city)\n",
    "SORTKEY(window_start_time, city);\n",
    "```\n",
    "\n",
    "#### ‚û°Ô∏è Create AWS Glue visual ETL\n",
    "\n",
    "1. **Create Glue Visual ETL and data source**:\n",
    "   - In the search bar at the top, search for `Glue` and open in a new tab.\n",
    "   - From the left hand menu, from under `ELT jobs` click on `Visual ETL` the click `Visual ETL`.\n",
    "   - From the `Sources` menu click on `AWS Glue Data Catalog` then click on the node itself on the canvas to reveal the properties of the node ot the right.\n",
    "   - From `Database` select `weather-analytics_dev_db`.\n",
    "   - From `Table` select `raw_weather_data` (this is the table you created in the main exercise of this workshop by crawling the S3 data lake).\n",
    "   ![VisualETL_Source](./images/VisualETL_Source.png)\n",
    "   - So that this ETL has permission to access our data source, click on the `Job details` tab and for the `IAM Role` drop-down select `weather-analytics-glue-role-dev`. Also change `Requested number of workers` from the default `10` to `2`. This means our job will use less DPUs and not reach the \"A Cloud Guru\" DPU limit that has been set as quickly.\n",
    "   ![VisualETL_Role](./images/VisualETL_Role.png)\n",
    "   - On the top left edit the name of the ETL to `ETL_job`, then on the top right click `Save`.\n",
    "\n",
    "3. **Create Glue Visual ETL with data source and SQL transform**:\n",
    "   - Click on the `Visual` tab then left click on the data source node on the canvas.\n",
    "   - Then click the blue circle with a plus to add another node to the ETL.\n",
    "   - In the `Transforms` tab click on `SQL Query`.\n",
    "   - Then paste the following SQL into the `SQL query` box. When you paste the SQL into the box it will generate a `Data preview` showing the result of your query.\n",
    "\n",
    "```{code-block} sql\n",
    "WITH deduplicated_data AS (\n",
    "    SELECT \n",
    "        city,\n",
    "        temperature,\n",
    "        measurement_time,\n",
    "        source,\n",
    "        location,\n",
    "        MIN(collection_time) as first_collection_time,\n",
    "        MAX(collection_time) as last_collection_time,\n",
    "        COUNT(*) as collection_attempts\n",
    "    FROM myDataSource\n",
    "    GROUP BY \n",
    "        city,\n",
    "        temperature,\n",
    "        measurement_time,\n",
    "        source,\n",
    "        location\n",
    ")\n",
    "SELECT \n",
    "    date_trunc('minute', measurement_time) as window_start_time,\n",
    "    date_trunc('minute', measurement_time) + INTERVAL '15 minute' as window_end_time,\n",
    "    city,\n",
    "    temperature,\n",
    "    LAG(temperature) OVER (PARTITION BY city ORDER BY measurement_time) as previous_temperature,\n",
    "    temperature - LAG(temperature) OVER (PARTITION BY city ORDER BY measurement_time) as temperature_change,\n",
    "    location,\n",
    "    source,\n",
    "    collection_attempts,\n",
    "    unix_timestamp(last_collection_time) - unix_timestamp(first_collection_time) as collection_window_seconds,\n",
    "    unix_timestamp(first_collection_time) - unix_timestamp(measurement_time) as initial_latency_seconds,\n",
    "    current_timestamp() as last_update,\n",
    "    year(measurement_time) as ingest_year,\n",
    "    month(measurement_time) as ingest_month,\n",
    "    day(measurement_time) as ingest_day,\n",
    "    hour(measurement_time) as ingest_hour\n",
    "FROM deduplicated_data\n",
    "```  \n",
    "\n",
    "![VisualETL_SQL](./images/VisualETL_SQL.png)<br>\n",
    "\n",
    "4. **Add Redshift target node**:\n",
    "   - With the SQL node selected, click on the the blue circle with a plus to add another node to the ETL.\n",
    "   - In the `Targets` tab click on `Amazon Redshift`.\n",
    "   - In the pane to the right, from the `Redshift connection` select `weather-analytics-redshift-connection-dev`.\n",
    "   - For the `Schema` select `public`.\n",
    "   - For the `Table` select `real_time_weather_stats` (this is the table you created in an earlier step in Redshfit).\n",
    "   - For `Handling of data and target table` select `TRUNCATE`.\n",
    "   - Finally click `Save` and then `Run` to start your ETL job.\n",
    "   - From the gren message at the top click on `Run details` to monitor your job run. It will take around four minutes to complete and for its `Run status` to change from `Running` to `Succeeded`.\n",
    "![VisualETL_Redshift](./images/VisualETL_Redshift.png)<br>\n",
    "\n",
    "####  üìà Inspect and visualise Redshift target table\n",
    "\n",
    "1. **Inspect data in target table**:\n",
    "   - Go to the browswer tab for Redshift you had open earlier and the Query Editor.\n",
    "   - Use the plus icon to open a new blank query window and paste the SQL code below then click `Run` to inspect the raw data in the target table.\n",
    "\n",
    "```{code-block} sql\n",
    "SELECT *\n",
    "FROM real_time_weather_stats;\n",
    "```\n",
    "\n",
    "1. **Visualise London temperature over time**:\n",
    "   - Use the plus icon to open a new blank query window and paste the SQL code below then click `Run`.\n",
    "   - In the `Query results` pane click `Visualise`.\n",
    "   - Select `Chart type` as `Bar`.\n",
    "   - Select `X axis` as `minutessincemidnight`.\n",
    "   - Select  `Y axis` at `temperature`. \n",
    "   - Take a screenshot of your plot. We will look at this plot again later and compare it to the plot when run on a new data refresh.\n",
    "\n",
    "```{code-block} sql\n",
    "SELECT \n",
    "   EXTRACT(HOUR FROM window_start_time) * 60 + EXTRACT(MINUTE FROM window_start_time) as \"Minutes Since Midnight\",\n",
    "   ROUND(temperature::numeric, 1) as \"Temperature\",\n",
    "   ROUND(temperature_change::numeric, 2) as \"Temperature Change\",\n",
    "   initial_latency_seconds as \"Latency (seconds)\"\n",
    "FROM real_time_weather_stats\n",
    "WHERE city = 'London'  \n",
    "ORDER BY window_start_time;\n",
    "```\n",
    "![Redshift_Plot](./images/Redshift_Plot.png)<br>\n",
    "\n",
    "#### üéº Orchestrate the entire process!\n",
    "\n",
    "1. **Create and run orchesterated workflow**:\n",
    "   - Go to the browswer tab for Glue you had open earlier.\n",
    "   - In the left hand menu click on `Workflows (orchestration)`.\n",
    "   - Click `Add workflow` and name it `weather_workflow` then click `Create workflow`.\n",
    "   - Click on `weather_workflow` then `Add trigger` then `Add new`.\n",
    "   - Name the trigger `start_crawl`.\n",
    "   - For the `Trigger type` select `Schedule`.\n",
    "   - For the `Frequency` select `Custom`.\n",
    "   - In the `Cron expression` enter `*/10 * * * ? *` which will trigger every 10 minutes.\n",
    "   ![Workflow_Trigger](./images/Workflow_Trigger.png)<br>\n",
    "   - In the visual workflow canvas that appears, to the right of the `crawl_s3_data_lake` trigger, click on `Add node`, select the `crawlers` tab then tick the crawler name you created in your main exercise (likely called `weather-data-crawler`) then click `Add`.\n",
    "   - Now click on the crawler (the icon with the spider) and click on `Add trigger` to the right of it.\n",
    "   - Select the `Add new` then name the trigger `run_etl`.\n",
    "   - For `Trigger type` leave `Event` selected.\n",
    "   - For `Trigger logic` select `Start after ALL watched event` then click `Add`.\n",
    "   - Now to the right of the `run_etl` trigger node, click `Add node`, select the `Jobs` tab and then tick the visual ETL job you created earlier (likely called `ETL_job`) then click `Add`.\n",
    "   - Finally, click `Run workflow`.\n",
    "    ![Workflow_Canvas](./images/Workflow_Canvas.png)<br>\n",
    "\n",
    "1. **Monitor workflow**:\n",
    "   - In the left hand menu of glue click on `workflows (orchestration).\n",
    "   - Click on the workflow `weather_workflow`.\n",
    "   - Click on the `History` tab.\n",
    "   - Select the `Workflow run ID` with the status `Running` then click `View run details`.\n",
    "   - When this triggers you should see the nodes incrementally chaning from `Not Started` to `üîÅ Running` then `‚úÖ Succeeded`.\n",
    "    ![Workflow_Monitor](./images/Workflow_Monitor.png)<br>\n",
    "    - when all noes are at `‚úÖ Succeeded` status, return to redshift and re-run your plot code from the previous task to confirm new data has arrived into the target table by comparing this plot to your previous plot.\n",
    "    ![Redshift_NewPlot](./images/Redshift_NewPlot.png)<br>\n",
    "\n",
    "```{Note}\n",
    "‚õî At this point or soon after, your ACG sandbox may shut down due to exceeding Glue DPU (Data Processing Unit) limits. You‚Äôll receive an email titled ‚ÄúYour Hands-On Lab or Cloud Playground has been shut down,‚Äù explaining the suspension due to excessive DPU usage.\n",
    "\n",
    "AWS Glue jobs, being Spark-based, provision distributed computing environments even for small tasks, which can quickly hit ACG's limits designed to prevent runaway costs. This restriction is a helpful reminder of resource management in data processing.\n",
    "\n",
    "If your sandbox is suspended, don‚Äôt worry, this is part of learning to use powerful tools like AWS Glue. Simply start a new sandbox and redeploy the CloudFormation template, which will be ready in 3‚Äì4 minutes. Learn more about Glue DPUs and optimisation here: https://docs.aws.amazon.com/glue/latest/dg/monitor-debug-capacity.html\n",
    "```\n",
    "\n",
    "### **üìù Going Further 2:** Data architecture diagram\n",
    "\n",
    "> So far, we‚Äôve used many AWS services in a deliberate sequence. If you had to explain this architecture to a colleague, they might lose track when you describe all the different AWS services and how they interact. This is where architecture diagrams become essential‚Äînot just a nice-to-have. They facilitate clear and transparent discussions with various colleagues, whether to address the security of what you‚Äôre building, collaborate, or consider changes to the architecture. These changes might involve using different services, transitioning to another cloud platform like Azure or GCP, or adopting a new streaming framework like Kafka.\n",
    "\n",
    "![Workshop_09_architecture.drawio](./images/Workshop_09_architecture.drawio.png)<br>\n",
    "\n",
    "```{note}\n",
    "üó∫Ô∏èThis is also an excellent opportunity to practice creating clear and visually appealing architecture diagrams. These diagrams are not only helpful for explaining your current work but could also form part of your project, including as an appendix in your project evaluation report for the End Point Assessment. Additionally, when answering questions about your project to provide verbal evidence for pass descriptors, a clear architecture diagram can be an invaluable tool. It gives you a strong visual aid to screen share, discuss, and use as a reference point to demonstrate your knowledge and skills gained during the apprenticeship.\n",
    "```\n",
    "\n",
    "#### AWS Architecture Icons and Guidance\n",
    "\n",
    "1. **Visit Architecture Icons page**:\n",
    "   - Visit this web page `https://aws.amazon.com/architecture/icons/` and consider clicking on `Get started` and downloading `Microsoft PPtx toolkits`. \n",
    "   - When unzipped this folder contains two PowerPoint files, one for Light and one for Dark themes. Consider these your reference for working out how to create a professional AWS compliant architecture diagram you would see from an experience data architect create. Slides 13 to 18 have simple rules to follow.\n",
    " \n",
    "#### Drawing and diagramming tools\n",
    "\n",
    "1. **Create workshop diagram**:\n",
    "   - Scroll down the web page `https://aws.amazon.com/architecture/icons/` to the section `Drawing and diagramming tools` and note the different tools AWS reccomends.\n",
    "   - We suggest starting with Draw.io as its free, simple to use and can be used to create professional diagrams in AWS, Azure, GCP, IBM Cloud, and on-premises architectures: `https://app.diagrams.net/`\n",
    "   - Select where to save your diagram (or decide later)\n",
    "   ![SaveDiagrams](./images/SaveDiagrams.png)<br>\n",
    "   - Then select `File / New...` and click the `Blank Diagram` icon followed by clicking `Cloud` in the sidebar, then `Create`.\n",
    "   ![drawio_blank](./images/drawio_blank.png)<br>\n",
    "   - To find the AWS icons that represent this architecture you can scroll down on the left hand side pallete and expand different AWS resource categories such as `AWS / Analytics` in which you will find Kinesis.\n",
    "   ![drawio_pallette](./images/drawio_pallette.png)<br>\n",
    "   - A faster way to find the icon you need is to use search at the top. For instance, searching for `Kinesis data stream` returns two icons shown below. You can mouse over over an icon to see how it is described. To determine which icon is the current official icon for that service, in the AWS PowerPoint file you downloaded earlier, search for the same words to see which icon is the current accepted one to use that matches to one of the choices in draw.io.\n",
    "   ![drawio_search](./images/drawio_search.png)<br>\n",
    "   - Now see if you can re-create the workshop architecture diagram shown above. Would you re-create it exactly or could you improve the diagram to look similar to an official AWS diagram like the example below? You can search for diagrams here: https://aws.amazon.com/architecture/reference-architecture-diagrams\n",
    "   ![AWS_reference_architecture](./images/AWS_reference_architecture.png)<br>\n",
    "   \n",
    "### **‚û°Ô∏è Going Further 3:** Direct Streaming\n",
    "\n",
    "> In this final going further let's explore alternative patterns where we are more direct and see that we can potentially, using AWS Glue, stream directly from our Amazon Kinesis Data Stream into a Redshift table, or stream directly from Redshift itself and not use Glue at all. In this fast exercise we simply preview the data directly streamed in Glue, but then spend most of our time reflecting on the different possible patterns and why you may prefer one over the other depending on business needs.\n",
    "\n",
    "![Workshop_09_architecture_going_further_direct_streaming.drawio](./images/Workshop_09_architecture_going_further_direct_streaming.drawio.png)<br>\n",
    "\n",
    "#### üö∞ Stream directly in glue\n",
    "\n",
    "1. **Create Glue Visual ETL and data source**:\n",
    "   - In the search bar at the top, search for `Glue` and open in a new tab.\n",
    "   - From the left hand menu, from under `ELT jobs` click on `Visual ETL` the click `Visual ETL`.\n",
    "    - On the top left edit the name of the ETL to `Streaming`, then on the top right click `Save`.\n",
    "   - From the `Sources` menu click on `Amazon Kinesis` then click on the node itself on the canvas to reveal the properties of the node ot the right.\n",
    "   - From `Stream name` select `weather-analytics-stream-dev` which is the Amazon Kinesisi data stream (Broker) that the lambda function (Producer) writes to every 60 seconds.\n",
    "   ![GlueStream](./images/GlueStream.png)<br>\n",
    "   - So that this ETL has permission to access our Kinesis data stream, click on the `Job details` tab and for the `IAM Role` drop-down select `weather-analytics-glue-role-dev`. Also change `Worker type` to the lower spec `G 1X`.\n",
    "   ![GlueStreamJobDetails](./images/GlueStreamJobDetails.png)<br>\n",
    "\n",
    "3. **Create Glue Visual ETL with data source and SQL transform**:\n",
    "   - Return to the `Visual` tab and you should soon see a `Data preview` of that source data\n",
    "   ![GlueStreamSourcePreview](./images/GlueStreamSourcePreview.png)<br>\n",
    "   - Behind the Data preview tab is an `Output Schema` tab. This is the schema of the data that will output from this node into the next node of the exercise. Note that all types are correct but not the timesampe fields.\n",
    "   ![GlueStreamSourceSchema](./images/GlueStreamSourceSchema.png)<br>\n",
    "   - To address this, let's add a node after the source node to map the ..click the blue circle with a plus to add another node to the ETL.\n",
    "   - In the `Transforms` tab click on `Changes Schema`.\n",
    "   - Select the `Change Schema` node so that its properites show to the right. Correct the two data timestamp data types and drop the final field. After these changes the Data preview should automatically update and show you the data in its new schema, and update the Output schema. \n",
    "   ![GlueStreamChangeSchema](./images/GlueStreamChangeSchema.png)<br>\n",
    "   - Just like in the first going further exercise where we wrote to Redshift we could create a Redshfit table to match our schema and select Redshift as the targer.\n",
    "   ![GlueStreamTarget](./images/GlueStreamTarget.png)<br>\n",
    "\n",
    "3. **Finalising this direct streaming pattern**:   \n",
    "    - In this direct streaming example we would still have duplications, think about how you could use both a SQL tranforms node and MERGE when you write to Redshift to handle potential duplicatoins. Click below to reveal a hint, then expand further for a fuller suggested answer.....\n",
    "\n",
    "4. **Direct streaming into Redshift**:   \n",
    "    - A recent addition to Redshift is streaming direct https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html\n",
    "    - In this example https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion-example-station-data.html"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}