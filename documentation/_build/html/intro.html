
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Data Engineer Workshop 9: Advanced Stream Processing Pipelines &#8212; DE Workshop 5</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intro';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="#">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/Corndel_Logos_RGB_Logo.png" class="logo__image only-light" alt="DE Workshop 5 - Home"/>
    <img src="_static/Corndel_Logos_RGB_Logo.png" class="logo__image only-dark pst-js-only" alt="DE Workshop 5 - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/intro.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Data Engineer Workshop 9: Advanced Stream Processing Pipelines</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-on-previous-workshops-from-on-demand-to-batch-and-streaming">üß± Building on Previous Workshops: From On-Demand to Batch and Streaming</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#should-we-really-be-streaming-data">‚ÅâÔ∏è Should we really be streaming data?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alignment-with-data-engineer-pass-descriptors">ü§ù Alignment with Data Engineer Pass Descriptors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#today-s-workshop-scenario-and-objectives">üå¨Ô∏è Today‚Äôs Workshop Scenario and Objectives</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#task-1-configuring-the-development-environment">‚öôÔ∏è Task 1: Configuring the Development Environment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#task-2-explore-and-understand-your-streaming-application-so-far">üó∫Ô∏è Task 2: Explore and understand your streaming application so far</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#producer-explore-the-data-producer-lambda-function">‚ú® 1 PRODUCER: Explore the Data Producer (Lambda Function):</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broker-view-data-in-the-data-stream-kinesis">‚û°Ô∏è‚û°Ô∏è‚û°Ô∏è 2 BROKER: View data in the Data Stream (Kinesis):</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#delivery-examine-how-data-is-delivered-to-the-sink-kinesis-firehose">üì© 3 DELIVERY: Examine how data is delivered to the sink (Kinesis Firehose):</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sink-storage-inspect-the-data-lake-s3">üìÇ 4 SINK/STORAGE: Inspect the Data Lake (S3):</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#processing-analytics-preview-the-analytics-foundation">üìà 5 PROCESSING / ANALYTICS: Preview the Analytics Foundation:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#task-3-data-lake-exploration-with-glue-and-athena">üîé Task 3: Data Lake Exploration with Glue and Athena</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up-glue-crawler">üìã Set Up Glue Crawler:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#run-the-crawler">üèÉ‚Äç‚ôÇÔ∏è Run the Crawler:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up-athena-query-environment">‚öôÔ∏è Set Up Athena Query Environment:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inspect-your-weather-data">üìä Inspect Your Weather Data:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-clean-view-of-weather-data">üßΩ Create Clean View of Weather Data:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analyse-clean-data">üìà Analyse Clean Data:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-analytics">üìà Advanced Analytics:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenge-exercises">üéØ Challenge Exercises:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-management-best-practices">Cost Management Best Practices:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wrapping-up-and-understanding-real-time-vs-near-real-time-streaming">Wrapping Up and Understanding Real-Time vs Near Real-Time Streaming</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#current-architecture-near-real-time-implementation">Current Architecture: Near Real-Time Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-time-processing">Real-Time Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#this-afternoon-reflecting-on-your-project">This afternoon - reflecting on your project</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#going-further">üöÄ Going Further</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#going-further-1-orchestrated-pipeline-to-write-to-redshift"><strong>üéº Going Further 1:</strong> Orchestrated pipeline to write to Redshift</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-target-table-in-redshift">üéØ Create target table in Redshift</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-aws-glue-visual-etl">‚û°Ô∏è Create AWS Glue visual ETL</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#inspect-and-visualise-redshift-target-table">üìà Inspect and visualise Redshift target table</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#orchestrate-the-entire-process">üéº Orchestrate the entire process!</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#going-further-2-data-architecture-diagram"><strong>üìù Going Further 2:</strong> Data architecture diagram</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#aws-architecture-icons-and-guidance">AWS Architecture Icons and Guidance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#drawing-and-diagramming-tools">Drawing and diagramming tools</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#going-further-3-direct-streaming"><strong>‚û°Ô∏è Going Further 3:</strong> Direct Streaming</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stream-directly-in-glue">üö∞ Stream directly in glue</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="data-engineer-workshop-9-advanced-stream-processing-pipelines">
<h1>Data Engineer Workshop 9: Advanced Stream Processing Pipelines<a class="headerlink" href="#data-engineer-workshop-9-advanced-stream-processing-pipelines" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<section id="building-on-previous-workshops-from-on-demand-to-batch-and-streaming">
<h3>üß± Building on Previous Workshops: From On-Demand to Batch and Streaming<a class="headerlink" href="#building-on-previous-workshops-from-on-demand-to-batch-and-streaming" title="Link to this heading">#</a></h3>
<p>Our workshop journey so far through different data processing patterns has prepared us for today‚Äôs exploration of streaming architectures.</p>
<ul class="simple">
<li><p>In Workshop 5, we learned how to make data rapidly available for analysts using AWS cloud services. We transformed CSV files into optimised Parquet format, used Glue to automatically discover and catalogue data structures, and enabled quick analysis through Athena‚Äôs serverless querying.</p></li>
</ul>
<p><img alt="workshop5-final-arch_01" src="_images/workshop5-final-arch_01.png" /><br></p>
<ul class="simple">
<li><p>Workshop 7 then introduced the challenges of keeping analytical data current as source systems change. Using the Sakila DVD rental database, we built automated pipelines in Azure Synapse to detect changes in our OLTP data source and update our dimensional model (the customer dimension).</p></li>
</ul>
<p><img alt="workshop_07_architecture" src="_images/workshop_07_architecture.png" /><br></p>
<p>Today, we can combine these practical insights as we move from periodic batch processing to continuous streaming. So instead of waiting to gather changes and process them on a schedule (e.g. daily, weekly, monthly), we‚Äôll build a pipeline that can processes weather data from an API in real-time as it arrives.</p>
<p>Our new streaming architecture will:</p>
<ul class="simple">
<li><p>Ingest data continuously through Kinesis streams (versus periodic batch loads in Workshop 5)</p></li>
<li><p>Process records immediately as they arrive (versus scheduled updates)</p></li>
<li><p>Enable multiple downstream consumers to analyse the same data stream</p></li>
<li><p>Support both real-time analytics and historical querying</p></li>
</ul>
<p><img alt="Workshop_09_architecture.drawio" src="_images/Workshop_09_architecture.drawio.png" /><br></p>
<p>This approach particularly suits use cases where data freshness is a key businesss requirement, like monitoring weather conditions, tracking financial transactions, or analysing Internet of Things (IoT) sensor data (e.g. monitoring vibration sensors on factory equipment that need to detect and report dangerous oscillations within seconds to prevent catastrophic machine failures).</p>
<p>As you work through today‚Äôs exercises, consider how streaming might complement the batch processing patterns you‚Äôve already learned, and when each approach might be most appropriate for your own organisation or even your project.</p>
</section>
<section id="should-we-really-be-streaming-data">
<h3>‚ÅâÔ∏è Should we really be streaming data?<a class="headerlink" href="#should-we-really-be-streaming-data" title="Link to this heading">#</a></h3>
<p>While streaming architectures can help answer <em>‚Äúwhat‚Äôs happening right now?‚Äù</em> versus historical <em>‚Äúwhat happened?‚Äù</em> analysis, it‚Äôs important to carefully evaluate whether real-time data serves a genuine business need. Consider:</p>
<ol class="arabic simple">
<li><p><strong>Business Value</strong>: Does real-time insight enable better decision-making or customer experience that justifies the additional complexity?</p></li>
<li><p><strong>True Requirements</strong>: If your stakeholders request <em>‚Äúreal-time‚Äù</em> data, would near real-time or daily batch updates suffice?</p></li>
<li><p><strong>Cost-Benefit Analysis</strong>:  Streaming architectures can introduce complexity we will encounter directly in the workshop exercises. While the examples of complexity below may not be immediately clear now, some will become concrete as you build and test each component of our weather streaming data pipeline:</p>
<ul class="simple">
<li><p>Error handling and recovery (e.g. handling failed API calls to the weather service, managing retry logic for Kinesis stream failures, dealing with Lambda function timeouts)</p></li>
<li><p>Data consistency and ordering (e.g. ensuring weather readings from different cities arrive in the correct chronological order, handling duplicate temperature measurements from multiple collections of the same reading)</p></li>
<li><p>Infrastructure management (e.g. monitoring Kinesis stream throughput capacity, managing Lambda concurrency limits, scaling Firehose delivery performance)</p></li>
<li><p>Monitoring and alerting (e.g. tracking end-to-end latency from weather measurement time to Redshift availability, detecting gaps in city data collection, alerting on abnormal temperature variations)</p></li>
</ul>
</li>
<li><p><strong>Alternative Approaches</strong>: Could simpler solutions work?</p></li>
</ol>
<ul class="simple">
<li><p>On-demand updates (Workshop 5)</p></li>
<li><p>Scheduled batch processing (Workshop 7)</p></li>
<li><p>More frequent but still batch-based loads</p></li>
</ul>
<p>As Zach Wilson provocatively notes in this X post: <em>‚ÄúStop building streaming pipelines when your stakeholders request ‚Äòreal time‚Äô data!‚Äù</em></p>
<p><img alt="Zach Wilson Tweet" src="https://pbs.twimg.com/media/GgVZDkhbwAErpUs?format=jpg&amp;name=900x900" /></p>
<p>This afternoon, as well as reflecting on the practical exercises here we‚Äôll explore how to evaluate these tradeoffs for your specific project needs and stakeholder requirements.</p>
</section>
<section id="alignment-with-data-engineer-pass-descriptors">
<h3>ü§ù Alignment with Data Engineer Pass Descriptors<a class="headerlink" href="#alignment-with-data-engineer-pass-descriptors" title="Link to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This workshop aligns with several IFATE Data Engineer Pass Descriptors: <a class="reference external" href="https://www.instituteforapprenticeships.org/apprenticeship-standards/data-engineer-v1-0">https://www.instituteforapprenticeships.org/apprenticeship-standards/data-engineer-v1-0</a></p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Click here to review more detail</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<ul>
<li><p class="sd-card-text"><strong>Describes how they use data ingestion frameworks such as streaming, batching and on demand services to move data from one location to another in order to optimise data ingestion processes. (K18, S15)</strong><br />
We focus on streaming and this workshop demonstrate different kinds of streaming patterns and consider their optimisation and business uses.</p></li>
<li><p class="sd-card-text"><strong>Describes the types and uses of data engineering tools in their own organisation and how they apply them. (K20)</strong><br />
This workshop covers multiple AWS data engineering tools that represent common architectural patterns you‚Äôll find across different cloud platforms and on-premises solutions. By understanding these core patterns, you can be better equipped to understand and describe equivalent tools in your own organisation, whether they use AWS, Azure, Google Cloud, or open-source alternatives like Kafka. Here are the AWS tools we will use and their equivalents from other providers.</p>
<ul class="simple">
<li><p class="sd-card-text"><strong>AWS Lambda: Functions as a Service:</strong> A serverless compute service that lets us run code without having to provision a server. If your organisation uses Azure Functions, or Google Cloud Functions you‚Äôll find the concepts very similar.</p></li>
<li><p class="sd-card-text"><strong>Amazon EventBridge: Event Orchestration Service:</strong> A serverless ‚Äúevent bus‚Äù that makes it easy to connect applications together using data from your own applications. Think of it like a smart postal sorting office for your data. Just as a postal service routes packages to their correct destinations based on addresses, EventBridge routes data ‚Äúpackages‚Äù (events) to the right applications based on rules you define. For example, in our workshop, it‚Äôs what makes sure our Lambda function gets ‚Äúwoken up‚Äù every minute to collect new weather data. While your organisation might use Azure Event Grid, Google Cloud Pub/Sub, or Apache Kafka for event orchestration, the fundamental patterns of event-driven architecture remain the same.</p></li>
<li><p class="sd-card-text"><strong>Amazon Kinesis Data Stream: Streaming Broker:</strong> A massively scalable and durable real-time data streaming service. Think of it as a continuously flowing river of data that can handle hundreds of terabytes of data per hour from multiple sources! The patterns you learn here apply equally to Azure Event Hubs, Google Cloud Pub/Sub streams, or Apache Kafka topics.</p></li>
<li><p class="sd-card-text"><strong>Amazon Kinesis Firehose: Streaming Consumer:</strong> A fully managed service that reliably loads streaming data into data lakes, data stores, and analytics services. These concepts map directly to services like Azure Stream Analytics, Google Cloud Dataflow, or Kafka Connect in your organisation.</p></li>
<li><p class="sd-card-text"><strong>AWS Glue: Serverless ETL Service</strong> A fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load your data for analytics. Whether your organisation uses Azure Data Factory, Google Cloud Dataflow, or Apache NiFi, the ETL principles remain consistent.</p></li>
<li><p class="sd-card-text"><strong>Amazon Athena: Serverless Query Service:</strong> An interactive query service that makes it easy to analyse data directly in Amazon S3 using standard SQL. Your organization might use Azure Synapse Analytics serverless SQL pools (we used these in workshop 7) or Google BigQuery, but the concept of serverless SQL querying is universal.</p></li>
<li><p class="sd-card-text"><strong>Amazon Redshift: Cloud Data Warehouse:</strong> A fully managed, OLAP data warehouse service. While your organisation might use Azure Synapse Analytics dedicated SQL pools (we used these in workshop 7), Google BigQuery, or Snowflake, the fundamental concepts of cloud data warehousing remain the same.</p></li>
</ul>
<p class="sd-card-text">These services work together to create a comprehensive data engineering pipeline: EventBridge triggerers the Lambda function, which can process data and send it to Kinesis Streams. Firehose can then load this data into S3, where it can be cataloged by Glue, queried by Athena, and eventually loaded into Redshift for complex analytics! We will do all this together!</p>
</li>
<li><p class="sd-card-text"><strong>Explains the deployment approaches processes for new data pipelines and automated processes. (K8)</strong></p>
<ul class="simple">
<li><p class="sd-card-text">In this workshop we will walk through the complete deployment of a real-time weather data pipeline in a learning environment, using an infrastructure-as-code approach with CloudFormation. The pipeline flows through these key layers:<br><br></p></li>
</ul>
<p class="sd-card-text"><strong>‚ú® 1. PRODUCER:</strong> Lambda function collects weather data via API<br>
üîª<br>
<strong>‚û°Ô∏è‚û°Ô∏è‚û°Ô∏è 2. BROKER:</strong> Kinesis Data Stream buffers real-time data<br>
üîª<br>
<strong>üì© 3. DELIVERY:</strong> Kinesis Firehose manages delivery<br>
üîª<br>
<strong>ü™£ 4. SINK/STORAGE:</strong> S3 data lake with smart partitioning<br>
üîª<br>
<strong>üìà 5. PROCESSING/ANALYTICS:</strong> Glue for processing, Athena for querying, Redshift for warehousing<br></p>
<ul class="simple">
<li><p class="sd-card-text">In Going Further, there is an exercise to automate and orchestrate this entire flow through Glue Workflows, demonstrating how to coordinate data ingestion, transformation, and loading processes in a systematic way. Note we will consider orchestration practically and in more detail in Workshop 10.</p></li>
</ul>
</li>
<li><p class="sd-card-text"><strong>Demonstrates how they used security, scalability and governance when automating data pipelines using programming languages and data integration platforms with graphical user interfaces. (K13, S4)</strong></p>
<ul class="simple">
<li><p class="sd-card-text">While this workshop uses purposefully permissive security settings (public subnets, broad IAM roles, simplified authentication) to facilitate learning, we will later emphasise how these would need to be hardened for production use, such as implementing private subnets with NAT Gateways, restrictive security groups, and AWS Secrets Manager for credentials.</p></li>
<li><p class="sd-card-text">Don‚Äôt worry if some of these terms sound unfamiliar or complex, they‚Äôre just specific tools that help with security and are typically handled by cloud security engineers and infrastructure teams working alongside data engineers to ensure both security and performance. The main point being that all these technical controls serve one clear purpose: to ensure your data only flows exactly where it should, is only accessed by exactly who should access it, and remains protected at every step of its journey through your pipeline.</p></li>
</ul>
</li>
</ul>
</div>
</details></section>
<section id="today-s-workshop-scenario-and-objectives">
<h3>üå¨Ô∏è Today‚Äôs Workshop Scenario and Objectives<a class="headerlink" href="#today-s-workshop-scenario-and-objectives" title="Link to this heading">#</a></h3>
<p>Real-time weather monitoring is essential for many industries. Consider these business use cases for streaming weather data:</p>
<ul class="simple">
<li><p><strong>Wind Energy</strong>: Wind farm operators need immediate temperature alerts to prevent turbine damage and optimise power generation. When temperatures exceed 35¬∞C, they must quickly adjust operations to prevent overheating of mechanical components. Historical temperature analysis helps schedule maintenance during optimal weather windows, potentially saving millions in repair costs.</p></li>
<li><p><strong>Transportation</strong>: Railway operators monitor temperature changes across thousands of miles of track, as sudden temperature variations can compromise rail integrity. Their monitoring systems must detect concerning temperature gradients in real-time while building historical analysis to identify vulnerable sections requiring extra monitoring.</p></li>
<li><p><strong>Retail</strong>: Major supermarket chains adjust inventory and staffing based on temperature forecasts and real-time conditions. A sudden temperature spike can drive demand for certain products up by 300%, requiring rapid supply chain adjustments based on both current readings and historical sales patterns.</p></li>
<li><p><strong>Smart Buildings</strong>: Modern commercial buildings use temperature data streams to automatically optimize HVAC operations (Heating, ventilation, and air conditioning), typically achieving 30-50% energy savings through real-time adjustments and predictive optimisation based on historical patterns. This requires both immediate response to current conditions and analysis of long-term temperature trends.</p></li>
</ul>
<p>For our workshop, we‚Äôll focus on wind energy, where weather data directly impacts operations and revenue. As noted in ‚ÄúThe Impact of Weather on Wind Farms‚Äù (2024): <a class="reference external" href="https://www.infoplaza.com/en/blog/the-impact-of-weather-on-wind-farms">https://www.infoplaza.com/en/blog/the-impact-of-weather-on-wind-farms</a></p>
<blockquote>
<div><p><em>‚ÄúThe power output of a turbine is related to density, which is a function of altitude, pressure, and also temperature. Dense air (which comes with lower temperatures) exerts more force on the rotors, resulting in a higher power output, even at relatively lower wind speeds compared to warmer and less dense air.‚Äù</em>
<img alt="Turbines" src="_images/turbine.png" /></p>
</div></blockquote>
<p>A wind farm operator has approached you to prototype a real-time temperature monitoring system. While their production system will need to monitor hundreds of turbine locations and track multiple weather parameters (temperature, wind speed, humidity), they want to start with a proof-of-concept using just temperature data from three cities to validate the approach. This temperature monitoring is needed as <em>‚Äúextreme heat can cause overheating of the turbine‚Äôs electrical and mechanical components as well as lubrication systems, potentially leading to shutdowns and increased maintenance costs.‚Äù</em></p>
<p>This represents a typical project progression that you learned about in module <code class="docutils literal notranslate"><span class="pre">9.1	From</span> <span class="pre">Prototype</span> <span class="pre">to</span> <span class="pre">Production:</span> <span class="pre">Implementing</span> <span class="pre">Data</span> <span class="pre">Solutions</span></code>.</p>
<ol class="arabic simple">
<li><p><strong>Scoping (Current)</strong>: We‚Äôve identified the core requirements: tracking temperature variations that could impact turbine performance and maintenance scheduling.</p></li>
<li><p><strong>Prototype (This Workshop)</strong>: Building a working demo with 3 locations using AWS services to get rapid stakeholder feedback.</p></li>
<li><p><strong>Development</strong>: Will expand to all turbine locations using exact Lat and Lon co-ordinates, add wind speed and humidity monitoring, implement production-grade security, and integrate with turbine control systems.</p></li>
<li><p><strong>Production</strong>: Full deployment with comprehensive monitoring, automated scaling, and disaster recovery</p></li>
<li><p><strong>Continuous Improvement</strong>: Regular evaluation of performance, costs, and new requirements</p></li>
</ol>
<p>Using AWS‚Äôs serverless and streaming services, we‚Äôll build this initial prototype. By the end of this workshop, you will have:</p>
<ul class="simple">
<li><p>Set up a complete data streaming infrastructure using CloudFormation</p></li>
<li><p>Created an automated data collection system that pulls weather data every minute using Lambda</p></li>
<li><p>Built a resilient streaming pipeline using Kinesis services</p></li>
<li><p>Developed a streaming pipeline that:</p>
<ul>
<li><p>Processes weather readings from three UK cities (expandable to more locations)</p></li>
<li><p>Automatically partitions data by location and time for efficient querying</p></li>
</ul>
</li>
<li><p>Tested the pipeline by:</p>
<ul>
<li><p>Monitoring real-time data flows through CloudWatch</p></li>
<li><p>Querying historical weather patterns using Athena</p></li>
<li><p>Creating analytical views for temperature trend analysis</p></li>
</ul>
</li>
</ul>
<p>This workshop will provide practical experience with:
- Infrastructure-as-code deployment using CloudFormation
- Real-time data collection using Lambda and EventBridge
- Stream processing with Kinesis Data Streams and Firehose
- Data lake organisation and partitioning strategies
- SQL analytics using Athena and Redshift
- ETL workflow automation / orchestration using AWS Glue</p>
<p>In this prototype, your pipeline will perform two main operations:</p>
<ol class="arabic simple">
<li><p><strong>Real-time Processing</strong>: Continuously collect and stream temperature data from multiple locations, helping operators monitor conditions that could impact turbine performance.</p></li>
<li><p><strong>Historical Analytics</strong>: Automatically organise historical weather data in the data lake for analysing temperature patterns and optimising maintenance schedules.</p></li>
</ol>
<p>By the end of the workshop, you will have built this architecture:</p>
<p><img alt="Workshop_09_architecture.drawio" src="_images/Workshop_09_architecture.drawio.png" /></p>
</section>
</section>
<section id="task-1-configuring-the-development-environment">
<h2>‚öôÔ∏è Task 1: Configuring the Development Environment<a class="headerlink" href="#task-1-configuring-the-development-environment" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>In Task 1 we set up the development environment for our prototype where weather data streaming and analytics can take place. This involves deploying a comprehensive AWS infrastructure using CloudFormation, which will create multiple interconnected services including a data ingestion pipeline (Lambda and Kinesis), storage layer (S3 data lake), and foundational analytics services (Redshift cluster and Glue database).</p>
</div></blockquote>
<ol class="arabic simple">
<li><p><strong>Start an ACG AWS sandbox:</strong></p></li>
</ol>
<ul class="simple">
<li><p>Use this link and start <code class="docutils literal notranslate"><span class="pre">AWS</span> <span class="pre">Sandbox</span> <span class="pre">-</span> <span class="pre">Default</span></code> and log in: <a class="reference external" href="https://learn.acloud.guru/cloud-playground/cloud-sandboxes">https://learn.acloud.guru/cloud-playground/cloud-sandboxes</a> <br>
<img alt="AWS_ACG_start" src="_images/AWS_ACG_start.png" /></p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Deploy AWS CloudFormation template:</strong></p></li>
</ol>
<ul class="simple">
<li><p>Once logged in search for <code class="docutils literal notranslate"><span class="pre">CloudFormation</span></code>.</p></li>
<li><p>At the top, click on the far right drop down <code class="docutils literal notranslate"><span class="pre">Create</span> <span class="pre">stack</span></code> and select <code class="docutils literal notranslate"><span class="pre">With</span> <span class="pre">new</span> <span class="pre">resources</span> <span class="pre">(standard)</span></code></p></li>
<li><p>At <code class="docutils literal notranslate"><span class="pre">Step</span> <span class="pre">1:</span> <span class="pre">Create</span> <span class="pre">Stack</span></code> copy and paste this URL <code class="docutils literal notranslate"><span class="pre">https://da5corndel.s3.eu-west-2.amazonaws.com/CloudFormation_streaming.yaml</span></code> into the <code class="docutils literal notranslate"><span class="pre">Amazon</span> <span class="pre">S3</span> <span class="pre">URL</span></code> box and click <code class="docutils literal notranslate"><span class="pre">Next</span></code>.
<img alt="CloudFormation_S3_URL" src="_images/CloudFormation_S3_URL.png" /><br><br></p></li>
<li><p>At <code class="docutils literal notranslate"><span class="pre">Step</span> <span class="pre">2:</span> <span class="pre">Specify</span> <span class="pre">stack</span> <span class="pre">details</span></code>, many parameters are already complete so do not need to be changed. Your only task is to complete the <code class="docutils literal notranslate"><span class="pre">Provide</span> <span class="pre">a</span> <span class="pre">stack</span> <span class="pre">name</span></code> field. This name will dynamically generate resource names based on the stack name you provide. To simplify, use <code class="docutils literal notranslate"><span class="pre">stream</span></code> (all lowercase) as the stack name. Then, click <code class="docutils literal notranslate"><span class="pre">Next</span></code>.
<img alt="CloudFormation_step2" src="_images/CloudFormation_step2.png" /><br><br></p></li>
<li><p>At <code class="docutils literal notranslate"><span class="pre">Step</span> <span class="pre">3:</span> <span class="pre">Configure</span> <span class="pre">stack</span> <span class="pre">options</span></code> scroll down to the bottom of the page then simply tick the check box next to the statement: <code class="docutils literal notranslate"><span class="pre">I</span> <span class="pre">acknowledge</span> <span class="pre">that</span> <span class="pre">AWS</span> <span class="pre">CloudFormation</span> <span class="pre">might</span> <span class="pre">create</span> <span class="pre">IAM</span> <span class="pre">resources</span> <span class="pre">with</span> <span class="pre">customised</span> <span class="pre">names.</span></code> then click <code class="docutils literal notranslate"><span class="pre">Next</span></code>.</p></li>
<li><p>At <code class="docutils literal notranslate"><span class="pre">Step</span> <span class="pre">4:</span> <span class="pre">Configure</span> <span class="pre">stack</span> <span class="pre">options</span></code>, scroll to the bottom of the page and click the <code class="docutils literal notranslate"><span class="pre">Submit</span></code> button. Your stack (we called <code class="docutils literal notranslate"><span class="pre">stream</span></code>) will now deploy and will show <code class="docutils literal notranslate"><span class="pre">‚Ñπ</span> <span class="pre">CREATE_IN_PROGRESS</span></code> while it deploys. After about 3 minutes it should show the message <code class="docutils literal notranslate"><span class="pre">‚Ñπ</span> <span class="pre">CREATE_COMPLETE</span></code>.</p></li>
</ul>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">While the template is deploying, click here to access helpful information and troubleshooting tips we recommend reading.</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p class="sd-card-text">‚åõ Your ACG Azure sandbox will automatically shut down and its data will be deleted after four hours. You will receive a notification one hour before the sandbox expires, allowing you to extend it for an additional four hours. Please plan your work accordingly to avoid disruptions.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p class="sd-card-text">üîÉ If you encounter deployment issues or want to start fresh, we recommend deleting the entire sandbox from the ACG playground rather than individual resources or the CloudFormation stack. This is faster and ensures a clean slate for redeployment.</p>
<p class="sd-card-text">The CloudFormation stack includes retention settings for resources like S3 buckets to avoid accidental data loss, and deletion can take 15‚Äì20 minutes due to dependencies between components (e.g. Lambda, Kinesis, Redshift). If you must delete the stack, empty S3 buckets manually first, as their deletion protection will block the process.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p class="sd-card-text">‚õî When using AWS Glue, your ACG sandbox may shut down due to exceeding Glue DPU (Data Processing Unit) limits. You‚Äôll receive an email titled ‚ÄúYour Hands-On Lab or Cloud Playground has been shut down,‚Äù explaining the suspension due to excessive DPU usage.</p>
<p class="sd-card-text">AWS Glue jobs, being Spark-based, provision distributed computing environments even for small tasks, which can quickly hit ACG‚Äôs limits designed to prevent runaway costs. This restriction is a helpful reminder of resource management in data processing.</p>
<p class="sd-card-text">If your sandbox is suspended, don‚Äôt worry, this is part of learning to use powerful tools like AWS Glue. Simply start a new sandbox and redeploy the CloudFormation template, which will be ready in 3‚Äì4 minutes. Learn more about Glue DPUs and optimisation here: <a class="reference external" href="https://docs.aws.amazon.com/glue/latest/dg/monitor-debug-capacity.html">https://docs.aws.amazon.com/glue/latest/dg/monitor-debug-capacity.html</a></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p class="sd-card-text">‚åö Using an AWS CloudFormation template in this workshop automates the deployment of our entire streaming analytics architecture through infrastructure-as-code (IaC). The template provisions and configures network infrastructure (VPC, subnets, gateways), security components (IAM roles, security groups), and all required services (Lambda, Kinesis, S3, Redshift, Glue) with appropriate permissions and connections between them.</p>
<p class="sd-card-text">For learning purposes, the template uses simplified security settings: public subnets instead of private ones, basic authentication rather than AWS Secrets Manager, and permissive security group rules that allow broad access. A production environment would need significant hardening, including private subnets with NAT gateways, strict security controls, and high-availability configurations.</p>
<p class="sd-card-text">Learn more about CloudFormation templates here: <a class="reference external" href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html</a></p>
</div>
</div>
</details><ol class="arabic simple" start="3">
<li><p><strong>How to explore your stack <code class="docutils literal notranslate"><span class="pre">Outputs</span></code></strong></p></li>
</ol>
<ul class="simple">
<li><p>Once your stack shows <code class="docutils literal notranslate"><span class="pre">‚Ñπ</span> <span class="pre">CREATE_COMPLETE</span></code>, click on the <code class="docutils literal notranslate"><span class="pre">Outputs</span></code> tab. This is a set of URLs created by the deployment for the key resources you will use in this workshop. Ideally keep this page open during the workhsop so you can come back here and easily navigate to different parts of the application.</p></li>
<li><p>Let‚Äôs start by simply opening the S3 bucket that will be our data lake. Right click on the URL  to the right of the key <code class="docutils literal notranslate"><span class="pre">DataLakeBucketURL</span></code> and select <code class="docutils literal notranslate"><span class="pre">Open</span> <span class="pre">link</span> <span class="pre">in</span> <span class="pre">new</span> <span class="pre">tab</span></code>. This will open the S3 bucket used to store streamed weather data in this application.
<img alt="CloudFormation_datalake" src="_images/CloudFormation_datalake.png" /><br></p></li>
<li><p>Great, now that you know how to quickly navigate key resources in this data streaming application, in the next section we will open and explore key resources in a logical order across this architecture, beginning with the Producer!!</p></li>
</ul>
</section>
<section id="task-2-explore-and-understand-your-streaming-application-so-far">
<h2>üó∫Ô∏è Task 2: Explore and understand your streaming application so far<a class="headerlink" href="#task-2-explore-and-understand-your-streaming-application-so-far" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>In this task, we will systematically explore the core data processing pipeline that has been provisioned for you. You‚Äôll examine how weather data flows through these different layers:</p>
</div></blockquote>
<p><strong>‚ú® 1. PRODUCER:</strong> Lambda function collects weather data via API<br>
üîª<br>
<strong>‚û°Ô∏è‚û°Ô∏è‚û°Ô∏è 2. BROKER:</strong> Kinesis Data Stream buffers real-time data<br>
üîª<br>
<strong>üì© 3. DELIVERY:</strong> Kinesis Firehose manages delivery<br>
üîª<br>
<strong>ü™£ 4. SINK/STORAGE:</strong> S3 data lake with smart partitioning<br>
üîª<br>
<strong>üìà 5. PROCESSING/ANALYTICS:</strong> Glue for processing, Athena for querying, Redshift for warehousing<br></p>
<section id="producer-explore-the-data-producer-lambda-function">
<h3>‚ú® 1 PRODUCER: Explore the Data Producer (Lambda Function):<a class="headerlink" href="#producer-explore-the-data-producer-lambda-function" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>This represents the start where real-time weather data enters our system. The combination of Lambda and EventBridge creates a reliable, serverless data collection mechanism that will continuously feed data into our streaming pipeline. Let‚Äôs explore them.</p></li>
<li><p>First, from your CloudFormation <code class="docutils literal notranslate"><span class="pre">Outputs</span></code> tab, locate the <code class="docutils literal notranslate"><span class="pre">LambdaFunctionURL</span></code> and open it in a new tab.</p></li>
<li><p>This Lambda function serves as our data producer - think of it as an automated weather station that collects and reports data every minute.
<img alt="Lambda" src="_images/Lambda.png" /><br></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We‚Äôre using the Open-Meteo Weather API (<a class="reference external" href="https://open-meteo.com/">https://open-meteo.com/</a>) to simulate collecting temperature data from our wind turbine locations. In a real wind farm, each turbine would have its own temperature sensors feeding data directly into our pipeline, but for learning purposes, we‚Äôre using this public weather API as a reliable source of real-time temperature data for our three ‚Äúturbine sites‚Äù (London, Manchester, and Edinburgh).</p>
<p>We chose this API deliberately for our workshop because it provides free weather data without requiring authentication or API keys. This means our application starts collecting temperature readings immediately, letting us focus on learning how to process and analyze streaming data. In a production wind farm, you‚Äôd need to handle sensor authentication, data validation, and careful rate limiting to ensure you don‚Äôt miss any critical temperature readings that could indicate potential turbine issues.</p>
<p>While the API offers many weather parameters (which you can explore at <a class="reference external" href="https://open-meteo.com/en/docs">https://open-meteo.com/en/docs</a>), we‚Äôre focusing solely on temperature data for this prototype. This mirrors good development practice: start simple with one essential metric (temperature, which directly affects turbine performance and maintenance needs), then expand to include other parameters like wind speed and humidity once the basic pipeline is working well. We‚Äôll explore these considerations around scaling up data collection, along with proper API security and management, in Workshop 13: ‚ÄòIntegrating API Data Sources‚Äô.</p>
</div>
<ul class="simple">
<li><p>Let‚Äôs examine how our Lambda function works to collect and stream weather data. The function code shown below in full serves as our ‚Äúweather station,‚Äù regularly checking temperatures at our wind farm locations.</p></li>
<li><p>The function uses Python and relies on two key tools:</p>
<ul>
<li><p>It uses the Open-Meteo API to collect weather data</p></li>
<li><p>It uses <code class="docutils literal notranslate"><span class="pre">boto3</span></code> (which is AWS‚Äôs Python software development kit - think of it as Python‚Äôs way of talking to AWS services, like a translator that lets Python and AWS understand each other) to send that data into our Kinesis stream</p></li>
</ul>
</li>
<li><p>Here‚Äôs how the function flows, step by step:</p></li>
</ul>
<ol class="arabic simple">
<li><p>First, it keeps a list of our ‚Äúwind farm locations‚Äù - the coordinates for London, Manchester, and Edinburgh stored in a list called <code class="docutils literal notranslate"><span class="pre">locations</span></code>. In a real wind farm, these would be your actual turbine coordinates.</p></li>
<li><p>When it runs, a helper function called <code class="docutils literal notranslate"><span class="pre">fetch_weather_data()</span></code> does the important job of:</p>
<ul class="simple">
<li><p>Calling the weather API for each location</p></li>
<li><p>Converting the raw weather data into a clean, structured format that includes:</p>
<ul>
<li><p>The city name</p></li>
<li><p>The current temperature</p></li>
<li><p>Timestamps to track when the data was measured and collected</p></li>
</ul>
</li>
</ul>
</li>
<li><p>The main processing loop (starting with <code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">location</span> <span class="pre">in</span> <span class="pre">locations:</span></code>) is like a continuous collection cycle:</p>
<ul class="simple">
<li><p>It visits each city in our locations list</p></li>
<li><p>Collects its current temperature data</p></li>
<li><p>Sends that data into our Kinesis stream, using the city name as a ‚Äúpartition key‚Äù (think of this like putting each city‚Äôs data in its own labeled channel)</p></li>
</ul>
</li>
<li><p>Throughout this process, the function keeps detailed logs of what it‚Äôs doing: whether it successfully collected and sent data, or if it ran into any problems. These logs help us monitor our data collection system‚Äôs health.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">üßë‚Äçüíª Clicking here to view the Python Code in the Lambda function</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">boto3</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">urllib.request</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datetime</span><span class="w"> </span><span class="kn">import</span> <span class="n">datetime</span>

<span class="k">def</span><span class="w"> </span><span class="nf">lambda_handler</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Lambda handler that fetches weather data from Open-Meteo API and streams it to Kinesis.</span>
<span class="sd">    The function processes multiple UK cities and structures the data for downstream analytics.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        event: AWS Lambda event trigger data</span>
<span class="sd">        context: AWS Lambda runtime information</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        dict: Response containing execution status and processing summary</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Initialize Kinesis client - Lambda automatically uses the correct region</span>
    <span class="n">kinesis_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s1">&#39;kinesis&#39;</span><span class="p">)</span>
    
    <span class="c1"># Reference the Kinesis stream created by CloudFormation</span>
    <span class="n">stream_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PROJECT_NAME&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">-stream-</span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;ENVIRONMENT&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="c1"># Define UK cities to monitor - structured for potential expansion</span>
    <span class="n">locations</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;city&quot;</span><span class="p">:</span> <span class="s2">&quot;London&quot;</span><span class="p">,</span> <span class="s2">&quot;latitude&quot;</span><span class="p">:</span> <span class="mf">51.5072</span><span class="p">,</span> <span class="s2">&quot;longitude&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mf">0.1276</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;city&quot;</span><span class="p">:</span> <span class="s2">&quot;Manchester&quot;</span><span class="p">,</span> <span class="s2">&quot;latitude&quot;</span><span class="p">:</span> <span class="mf">53.4808</span><span class="p">,</span> <span class="s2">&quot;longitude&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mf">2.2426</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;city&quot;</span><span class="p">:</span> <span class="s2">&quot;Edinburgh&quot;</span><span class="p">,</span> <span class="s2">&quot;latitude&quot;</span><span class="p">:</span> <span class="mf">55.9533</span><span class="p">,</span> <span class="s2">&quot;longitude&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.1883</span><span class="p">}</span>
    <span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fetch_weather_data</span><span class="p">(</span><span class="n">location</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fetches current weather data for a given location using Open-Meteo API.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            location (dict): Dictionary containing city name, latitude, and longitude</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            dict: Weather data including city, temperature, measurement time, and collection time</span>
<span class="sd">                  Returns None if data fetch fails</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">base_url</span> <span class="o">=</span> <span class="s2">&quot;https://api.open-meteo.com/v1/forecast&quot;</span>
        <span class="n">params</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;?latitude=</span><span class="si">{</span><span class="n">location</span><span class="p">[</span><span class="s1">&#39;latitude&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&amp;longitude=</span><span class="si">{</span><span class="n">location</span><span class="p">[</span><span class="s1">&#39;longitude&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&amp;current_weather=true&quot;</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">base_url</span> <span class="o">+</span> <span class="n">params</span><span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
            
            <span class="k">return</span> <span class="p">{</span>
                <span class="s2">&quot;city&quot;</span><span class="p">:</span> <span class="n">location</span><span class="p">[</span><span class="s2">&quot;city&quot;</span><span class="p">],</span>
                <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;current_weather&quot;</span><span class="p">][</span><span class="s2">&quot;temperature&quot;</span><span class="p">],</span>
                <span class="s2">&quot;measurement_time&quot;</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;current_weather&quot;</span><span class="p">][</span><span class="s2">&quot;time&quot;</span><span class="p">],</span>
                <span class="s2">&quot;collection_time&quot;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()</span><span class="o">.</span><span class="n">isoformat</span><span class="p">(),</span>
                <span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;open-meteo&quot;</span>
            <span class="p">}</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fetching data for </span><span class="si">{</span><span class="n">location</span><span class="p">[</span><span class="s1">&#39;city&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>

    <span class="c1"># Process each city and send data to Kinesis</span>
    <span class="n">successfully_processed</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">failed_cities</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">location</span> <span class="ow">in</span> <span class="n">locations</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">weather_data</span> <span class="o">=</span> <span class="n">fetch_weather_data</span><span class="p">(</span><span class="n">location</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">weather_data</span><span class="p">:</span>
                <span class="c1"># Log the data being sent for monitoring and debugging</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Processing data for </span><span class="si">{</span><span class="n">location</span><span class="p">[</span><span class="s1">&#39;city&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">weather_data</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
                <span class="c1"># Send to Kinesis stream with city as partition key</span>
                <span class="n">kinesis_client</span><span class="o">.</span><span class="n">put_record</span><span class="p">(</span>
                    <span class="n">StreamName</span><span class="o">=</span><span class="n">stream_name</span><span class="p">,</span>
                    <span class="n">Data</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">weather_data</span><span class="p">),</span>
                    <span class="n">PartitionKey</span><span class="o">=</span><span class="n">weather_data</span><span class="p">[</span><span class="s2">&quot;city&quot;</span><span class="p">]</span>
                <span class="p">)</span>
                <span class="n">successfully_processed</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">failed_cities</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">location</span><span class="p">[</span><span class="s2">&quot;city&quot;</span><span class="p">])</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Failed to process </span><span class="si">{</span><span class="n">location</span><span class="p">[</span><span class="s1">&#39;city&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>
            <span class="n">failed_cities</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">location</span><span class="p">[</span><span class="s2">&quot;city&quot;</span><span class="p">])</span>

    <span class="c1"># Prepare detailed execution summary</span>
    <span class="n">response</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;statusCode&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
        <span class="s2">&quot;body&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;message&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Processed </span><span class="si">{</span><span class="n">successfully_processed</span><span class="si">}</span><span class="s2"> of </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">locations</span><span class="p">)</span><span class="si">}</span><span class="s2"> cities&quot;</span><span class="p">,</span>
            <span class="s2">&quot;successful_count&quot;</span><span class="p">:</span> <span class="n">successfully_processed</span><span class="p">,</span>
            <span class="s2">&quot;total_cities&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">locations</span><span class="p">),</span>
            <span class="s2">&quot;execution_time&quot;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()</span><span class="o">.</span><span class="n">isoformat</span><span class="p">()</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="k">if</span> <span class="n">failed_cities</span><span class="p">:</span>
        <span class="n">response</span><span class="p">[</span><span class="s2">&quot;body&quot;</span><span class="p">][</span><span class="s2">&quot;failed_cities&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">failed_cities</span>
        
    <span class="k">return</span> <span class="n">response</span>
</pre></div>
</div>
</div>
</details><ul class="simple">
<li><p>Now, let‚Äôs understand how this lambda function gets triggered automatically by an Amazon EventBridge rule:</p>
<ul>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">Function</span> <span class="pre">overview</span></code> section, in the diagram click on the <code class="docutils literal notranslate"><span class="pre">EventBridge</span> <span class="pre">(CloudWatch</span> <span class="pre">Events)</span></code> then right click on the URL that now appears in the <code class="docutils literal notranslate"><span class="pre">Configuration</span></code> section as shown below. (You could also simply search for <code class="docutils literal notranslate"><span class="pre">EventBridge</span></code> at the top of the page).
<img alt="EventBridge" src="_images/EventBridge.png" /><br></p></li>
<li><p>Notice in the <code class="docutils literal notranslate"><span class="pre">Event</span> <span class="pre">schedule</span></code> the text <code class="docutils literal notranslate"><span class="pre">Fixed</span> <span class="pre">rate</span> <span class="pre">of</span> <span class="pre">1</span> <span class="pre">minute</span></code>. Click on the <code class="docutils literal notranslate"><span class="pre">Edit</span></code> button to the right and notice how in the <code class="docutils literal notranslate"><span class="pre">Schedule</span> <span class="pre">pattern</span></code> you could change the rate unit to be any number of <code class="docutils literal notranslate"><span class="pre">Minutes</span></code>, <code class="docutils literal notranslate"><span class="pre">Hours</span></code> or <code class="docutils literal notranslate"><span class="pre">Days</span></code>. Or, you could also change the schedule type to be a cron expression for specific minutes, hours, day of month, month, day of week and year.
<img alt="EventBridgeRate" src="_images/EventBridgeRate.png" /><br></p></li>
<li><p>This rule acts like an automated timer, invoking our Lambda function every minute.</p></li>
<li><p>You can verify this by watching the CloudWatch logs for this rule where will see new entries appearing every minute. To do this:</p>
<ul>
<li><p>In the search bar at the top type <code class="docutils literal notranslate"><span class="pre">CloudWatch</span></code>,</p></li>
<li><p>Then in the left hand menu of CloudWatch select from within <code class="docutils literal notranslate"><span class="pre">Logs</span></code>, <code class="docutils literal notranslate"><span class="pre">Log</span> <span class="pre">groups</span></code> then click on the Log group called <code class="docutils literal notranslate"><span class="pre">/aws/lambda/weather-analytics-producer-dev</span></code>
<img alt="CloudWatchLogGroups" src="_images/CloudWatchLogGroups.png" /><br></p></li>
<li><p>From the list of <code class="docutils literal notranslate"><span class="pre">Log</span> <span class="pre">streams</span></code> click on the container with the most recent <code class="docutils literal notranslate"><span class="pre">Last</span> <span class="pre">event</span> <span class="pre">time</span></code>. Expand some log entries and see if you can find entries from the <code class="docutils literal notranslate"><span class="pre">START</span></code> and <code class="docutils literal notranslate"><span class="pre">END</span></code> of one run of the Lambda function that has been triggered by the EventBridge rule you explored earlier. These markers wrap each function execution and between them you‚Äôll see log messages showing weather data for each city. By checking the timestamps between different START entries, you can verify the function is running every minute as configured.
<img alt="CloudWatchLogEntries" src="_images/CloudWatchLogEntries.png" /><br></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üîé Keep an eye on the CloudWatch logs as you proceed through the workshop. They provide valuable insights into the data being collected and can help you troubleshoot any issues that arise.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While this workshop uses a Lambda function to pull weather data, many real-world streaming applications instead have external systems pushing data through an API Gateway. Our Lambda approach is works well for learning as it provides reliable test data and lets us focus on stream processing concepts. However, production environments often need the additional security, access control, and data validation that API Gateway provides, especially when ingesting data from multiple external sources.</p>
<p>For example, in a real wind farm deployment, each turbine would likely push its sensor data to an API Gateway endpoint rather than having Lambda pull the readings. This provides better authentication, rate limiting, and the ability to validate data before it enters the stream. We‚Äôll consider these architectural patterns more in Workshop 13: ‚ÄòIntegrating API Data Sources‚Äô.</p>
</div>
<p><img alt="aws-streaming-data-using-api-gateway-architecture" src="_images/aws-streaming-data-using-api-gateway-architecture.png" /><br></p>
<blockquote>
<div><p><em>Image from AWS Solutions Library, Option 1: <a class="reference external" href="https://aws.amazon.com/solutions/implementations/streaming-data-solution-for-amazon-kinesis/">https://aws.amazon.com/solutions/implementations/streaming-data-solution-for-amazon-kinesis/</a></em></p>
</div></blockquote>
</section>
<section id="broker-view-data-in-the-data-stream-kinesis">
<h3>‚û°Ô∏è‚û°Ô∏è‚û°Ô∏è 2 BROKER: View data in the Data Stream (Kinesis):<a class="headerlink" href="#broker-view-data-in-the-data-stream-kinesis" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>We saw that the lambda function pulled weather data for three cities from the weather API and streamed those to a Kinesis data stream with the following code: <code class="docutils literal notranslate"><span class="pre">kinesis_client.put_record(StreamName=stream_name,</span> <span class="pre">Data=json.dumps(weather_data),</span> <span class="pre">PartitionKey=weather_data[&quot;city&quot;])</span></code> Let‚Äôs now explore that resource to understand how this broker works.</p></li>
<li><p>From your CloudFormation <code class="docutils literal notranslate"><span class="pre">Outputs</span></code> tab, locate <code class="docutils literal notranslate"><span class="pre">KinesisStreamURL</span></code> and open in a new tab.</p></li>
<li><p>This Kinesis stream acts as our real-time data broker/buffer, receiving weather data from the Lambda function and temporarily storing it for downstream consumers like Kinesis Firehose. Think of it as a moving window of data that maintains records for 24 hours.</p></li>
<li><p>The stream uses the city name as a partition key, which is like putting each city‚Äôs data into its own dedicated lane on a highway. Just as having separate lanes prevents cars from interfering with each other‚Äôs flow, having separate partitions ensures temperature readings from London don‚Äôt get mixed up with readings from Manchester. When we set <code class="docutils literal notranslate"><span class="pre">PartitionKey=weather_data[&quot;city&quot;]</span></code>, we‚Äôre telling Kinesis ‚Äúkeep all of London‚Äôs data together, all of Manchester‚Äôs data together, and all of Edinburgh‚Äôs data together.‚Äù This helps maintain the correct time order of temperature readings for each city.</p></li>
<li><p>Let‚Äôs now use the Data Viewer to inspect data in our Kinesis Stream:</p>
<ol class="arabic simple">
<li><p>Select the <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Viewer</span></code> tab</p></li>
<li><p>Choose the single shard available from the <code class="docutils literal notranslate"><span class="pre">Shard</span></code> drop-down</p></li>
<li><p>From <code class="docutils literal notranslate"><span class="pre">Starting</span> <span class="pre">Position</span></code> drop-down, select <code class="docutils literal notranslate"><span class="pre">Trim</span> <span class="pre">horizon</span></code></p></li>
<li><p>Click <code class="docutils literal notranslate"><span class="pre">Get</span> <span class="pre">records</span></code> to view the most recent weather data from our cities</p></li>
</ol>
</li>
</ul>
<p><img alt="KinesisDataStreamDataViewer" src="_images/KinesisDataStreamDataViewer.png" /><br></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üìÅ Our stream uses one shard because our data volume (3 cities √ó 1 record/minute = 3 records/minute) is well within a single shard‚Äôs capacity. A busier applications might need multiple shards to handle more data volume: <a class="reference external" href="https://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html">https://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html</a></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>‚åö<code class="docutils literal notranslate"><span class="pre">Trim</span> <span class="pre">horizon</span></code> works because it shows all records in the stream‚Äôs 24-hour retention window, letting you see historical data. <code class="docutils literal notranslate"><span class="pre">Latest</span></code> might not show records immediately because it only shows data that arrives after you start viewing. Since our Lambda writes once per minute, you might need to wait for the next data collection cycle to see new records.</p>
</div>
</section>
<section id="delivery-examine-how-data-is-delivered-to-the-sink-kinesis-firehose">
<h3>üì© 3 DELIVERY: Examine how data is delivered to the sink (Kinesis Firehose):<a class="headerlink" href="#delivery-examine-how-data-is-delivered-to-the-sink-kinesis-firehose" title="Link to this heading">#</a></h3>
<ul>
<li><p>From your CloudFormation <code class="docutils literal notranslate"><span class="pre">Outputs</span></code> tab, locate <code class="docutils literal notranslate"><span class="pre">KinesisFirehoseURL</span></code> and open in a new tab.</p>
<ul>
<li><p>Kinesis Firehose acts as our delivery service, taking data from the Kinesis stream and preparing it for long-term storage in S3. Let‚Äôs explore its key configurations:</p>
<ul class="simple">
<li><p>Click on the <code class="docutils literal notranslate"><span class="pre">weather-analytics-firehose-dev</span></code> delivery stream</p></li>
<li><p>Select <code class="docutils literal notranslate"><span class="pre">Edit</span> <span class="pre">destination</span> <span class="pre">settings</span></code> to view the configuration details
<img alt="KinesisFirehoseDestination" src="_images/KinesisFirehoseDestination.png" /><br></p></li>
</ul>
</li>
<li><p>Examine these important settings:</p>
<ul>
<li><p><strong>Dynamic Partitioning</strong>: Notice how this feature is enabled in the <code class="docutils literal notranslate"><span class="pre">Dynamic</span> <span class="pre">partitioning</span></code> section. It automatically organises our data in S3:</p>
<ul>
<li><p>Enabled with a partition key based on <code class="docutils literal notranslate"><span class="pre">city</span></code> using the JQ expression <code class="docutils literal notranslate"><span class="pre">.city</span></code></p></li>
<li><p>Creates a logical hierarchy in S3 using the prefix pattern you can see of:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>weather-data/location=!{partitionKeyFromQuery:city}/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/
</pre></div>
</div>
</li>
<li><p>This pattern means data is automatically organised by city, year, month, and day</p></li>
</ul>
</li>
<li><p><strong>Error Handling</strong>: Notice the error output prefix:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>errors/!{firehose:error-output-type}/!{timestamp:yyyy}/!{timestamp:MM}/!{timestamp:dd}/
</pre></div>
</div>
<p>This helps track and debug any processing issues by organising any error logs in a similar hierarchical structure</p>
</li>
<li><p><strong>Buffering</strong>: Scroll to the bottom of the page and expand the section <code class="docutils literal notranslate"><span class="pre">Buffer</span> <span class="pre">hints,</span> <span class="pre">compression,</span> <span class="pre">file</span> <span class="pre">extension</span> <span class="pre">and</span> <span class="pre">encryption</span></code>. Notice how Firehose buffers data for either:</p>
<ul class="simple">
<li><p>60 seconds (time-based buffer) OR</p></li>
<li><p>64MB (size-based buffer)</p></li>
<li><p>Whichever threshold is met first triggers a write to S3</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üì¶ The buffering configuration balances between data freshness and storage efficiency. Smaller buffers mean fresher data but more S3 write operations, while larger buffers are more cost-effective but introduce more latency. Our 60-second buffer is ideal for our workshop environment where we want to see results quickly.</p>
<p>üóÇÔ∏è The dynamic partitioning structure provides efficient querying later. By organising data by city and time components, we can quickly locate specific data subsets without scanning the entire dataset. For example, finding all London temperatures for a specific month becomes a targeted operation.</p>
</div>
</section>
<section id="sink-storage-inspect-the-data-lake-s3">
<h3>üìÇ 4 SINK/STORAGE: Inspect the Data Lake (S3):<a class="headerlink" href="#sink-storage-inspect-the-data-lake-s3" title="Link to this heading">#</a></h3>
<ul>
<li><p>From your CloudFormation <code class="docutils literal notranslate"><span class="pre">Outputs</span></code> tab, locate <code class="docutils literal notranslate"><span class="pre">DataLakeBucketURL</span></code> and open in a new tab.</p></li>
<li><p>This S3 bucket serves as our data lake, providing long-term storage of our weather data in an organised and cost-effective way. Let‚Äôs explore its structure:</p>
<ol class="arabic simple">
<li><p>Click into the <code class="docutils literal notranslate"><span class="pre">weather-data</span></code> folder</p></li>
<li><p>Notice the hierarchical organisation created by Kinesis Firehose we explored previously:</p>
<ul class="simple">
<li><p>First level: <code class="docutils literal notranslate"><span class="pre">location=cityname</span></code> (e.g., <code class="docutils literal notranslate"><span class="pre">location=London</span></code>)</p></li>
<li><p>Second level: <code class="docutils literal notranslate"><span class="pre">year=YYYY</span></code></p></li>
<li><p>Third level: <code class="docutils literal notranslate"><span class="pre">month=MM</span></code></p></li>
<li><p>Fourth level: <code class="docutils literal notranslate"><span class="pre">day=DD</span></code></p></li>
</ul>
</li>
<li><p>Navigate down through these levels to find the actual data files</p></li>
<li><p>Notice the <code class="docutils literal notranslate"><span class="pre">.gz</span></code> extension on the files - this indicates GZIP compression
<img alt="DataLakeBuckets" src="_images/DataLakeBuckets.png" /><br></p></li>
</ol>
</li>
<li><p>Key features to observe:</p>
<ul>
<li><p><strong>Partitioning Structure</strong>: The folder hierarchy directly matches the Firehose prefix pattern we examined earlier:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">weather</span><span class="o">-</span><span class="n">data</span><span class="o">/</span><span class="n">location</span><span class="o">=</span><span class="n">city</span><span class="o">/</span><span class="n">year</span><span class="o">=</span><span class="n">YYYY</span><span class="o">/</span><span class="n">month</span><span class="o">=</span><span class="n">MM</span><span class="o">/</span><span class="n">day</span><span class="o">=</span><span class="n">DD</span><span class="o">/</span>
</pre></div>
</div>
<p>This isn‚Äôt just tidy organisation, it supports  efficient querying</p>
</li>
<li><p><strong>File Formats</strong>:</p>
<ul class="simple">
<li><p>Files are automatically compressed using GZIP</p></li>
<li><p>Each file contains multiple weather records collected during the 60-second buffer window</p></li>
<li><p>File names include timestamp information for easy tracking</p></li>
</ul>
</li>
<li><p><strong>Lifecycle Management</strong>: The bucket has intelligent lifecycle rules that were defined in the CloudFormation template you deployed:</p>
<ul class="simple">
<li><p>Data moves to Infrequent Access (IA) storage after 90 days</p></li>
<li><p>Archives to Glacier storage after 180 days</p></li>
<li><p>This tiered approach optimises storage costs</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üíæ The combination of GZIP compression and intelligent lifecycle policies helps manage storage costs as your data grows. For examle, in a production environment, with years of historical weather data for many more locations the savings could become significant.</p>
<p>üéØ The partitioned structure isn‚Äôt just for organisation, it enables targeted data access. When you later query this data with Athena or process it with Glue, it can efficiently access specific time periods or locations without scanning the entire dataset.</p>
</div>
</section>
<section id="processing-analytics-preview-the-analytics-foundation">
<h3>üìà 5 PROCESSING / ANALYTICS: Preview the Analytics Foundation:<a class="headerlink" href="#processing-analytics-preview-the-analytics-foundation" title="Link to this heading">#</a></h3>
<ul>
<li><p>Let‚Äôs now explore the processing and analytical layer that will help us derive insights from our weather data. We‚Äôll examine both components:</p>
<ul class="simple">
<li><p>AWS Glue for data cataloguing and ETL</p></li>
<li><p>Amazon Redshift for data warehousing</p></li>
</ul>
</li>
<li><p>First, let‚Äôs look at our AWS Glue setup:</p>
<ul>
<li><p>Open the <code class="docutils literal notranslate"><span class="pre">GlueDatabaseURL</span></code> from your CloudFormation <code class="docutils literal notranslate"><span class="pre">Outputs</span></code> tab in a new tab</p></li>
<li><p>Click into the database named <code class="docutils literal notranslate"><span class="pre">weather-analytics-dev-db</span></code></p></li>
<li><p>Notice it‚Äôs currently empty - we‚Äôll populate it in the next task</p></li>
<li><p>Observe the database location points to our S3 data lake. However, even though this has been set we will still need to find it in our next exercise:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">s3</span><span class="p">:</span><span class="o">//</span><span class="n">weather</span><span class="o">-</span><span class="n">analytics</span><span class="o">-</span><span class="n">data</span><span class="o">-</span><span class="n">lake</span><span class="o">-</span><span class="n">dev</span><span class="o">-</span><span class="p">[</span><span class="n">YOUR</span><span class="o">-</span><span class="n">ACCOUNT</span><span class="o">-</span><span class="n">ID</span><span class="p">]</span><span class="o">/</span><span class="n">processed</span><span class="o">/</span>
</pre></div>
</div>
</li>
</ul>
<p><img alt="GlueDatabase" src="_images/GlueDatabase.png" /><br></p>
</li>
<li><p>Next, examine the Redshift configuration:</p>
<ul class="simple">
<li><p>Open the <code class="docutils literal notranslate"><span class="pre">RedshiftClusterURL</span></code> from your CloudFormation <code class="docutils literal notranslate"><span class="pre">Outputs</span></code> tab in a new tab</p></li>
<li><p>Notice the cluster configuration:</p>
<ul>
<li><p>Single-node cluster (<code class="docutils literal notranslate"><span class="pre">dc2.large</span></code>) suitable for workshop volumes</p></li>
<li><p>Publicly accessible for workshop simplicity</p></li>
<li><p>Located in the same VPC as other components</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üèóÔ∏è This foundation sets us up for both batch and real-time analytics. The combination of Glue (for ETL and cataloguing) and Redshift (for warehousing) provides a robust platform for deriving insights from our weather data.</p>
</div>
<p>Now that you understand how data flows through the system, from collection through streaming and into storage, you‚Äôre ready to build the analytics capabilities in the next task! You‚Äôll create crawlers to catalogue this data, develop ETL jobs to transform it, and ultimately load it into Redshift for analysis.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>ü™ü Keep the CloudFormation Outputs tab open as you continue working, you may frequently refer back to these resources throughout the workshop.</p>
</div>
</section>
</section>
<section id="task-3-data-lake-exploration-with-glue-and-athena">
<h2>üîé Task 3: Data Lake Exploration with Glue and Athena<a class="headerlink" href="#task-3-data-lake-exploration-with-glue-and-athena" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>As part of our wind farm prototype, the maintenance team needs to analyse both real-time and historical temperature patterns at each turbine location. They need to know when temperatures approach critical thresholds (like 35¬∞C) that could affect turbine performance, while also understanding longer-term temperature trends to optimize maintenance schedules. In this task, we‚Äôll use AWS Glue to automatically discover and catalogue our streaming temperature data, then use Athena to analyse it. Below is an excellent diagram from AWS about how crawled and catalgoued data can then be used in an ETL pipeline. We will do all of this!</p>
</div></blockquote>
<p><img alt="glue" src="https://docs.aws.amazon.com/images/glue/latest/dg/images/HowItWorks-overview.png" /></p>
<blockquote>
<div><p><em>Image from: <a class="reference external" href="https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a></em></p>
</div></blockquote>
<section id="set-up-glue-crawler">
<h3>üìã Set Up Glue Crawler:<a class="headerlink" href="#set-up-glue-crawler" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Step 1: Set crawler properties</strong>:</p>
<ul>
<li><p>In the AWS Console, search for <code class="docutils literal notranslate"><span class="pre">Glue</span></code> and open it</p></li>
<li><p>In the left navigation menu, expand <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Catalog</span></code> and select <code class="docutils literal notranslate"><span class="pre">Crawlers</span></code>.</p></li>
<li><p>Click the <code class="docutils literal notranslate"><span class="pre">Create</span> <span class="pre">crawler</span></code> button on the right.</p></li>
<li><p>Name: <code class="docutils literal notranslate"><span class="pre">weather-data-crawler</span></code></p></li>
<li><p>Click <code class="docutils literal notranslate"><span class="pre">Next</span></code></p></li>
</ul>
</li>
<li><p><strong>Step 2: Choose data sources and classifiers</strong>:</p>
<ul>
<li><p>Click on <code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">a</span> <span class="pre">data</span> <span class="pre">source</span></code>.</p></li>
<li><p>Leave the default settings of <code class="docutils literal notranslate"><span class="pre">S3</span></code> as the data source and the location of the S3 data as <code class="docutils literal notranslate"><span class="pre">In</span> <span class="pre">this</span> <span class="pre">account</span></code>.</p></li>
<li><p>For the S3 path click  <code class="docutils literal notranslate"><span class="pre">Browse</span> <span class="pre">S3</span></code> and click on
<code class="docutils literal notranslate"><span class="pre">s3://weather-analytics-data-lake-dev-[YOUR-ACCOUNT-ID]/weather-data/</span></code>
<img alt="GlueS3path" src="_images/GlueS3path.png" /><br></p></li>
<li><p>Even though you have added the S3 path to be crawled, the box may show a message in red of <code class="docutils literal notranslate"><span class="pre">This</span> <span class="pre">is</span> <span class="pre">a</span> <span class="pre">required</span> <span class="pre">field</span></code>. Just hit the tab key and it will be accepted.</p></li>
<li><p>Leave the default setting of <code class="docutils literal notranslate"><span class="pre">Crawl</span> <span class="pre">all</span> <span class="pre">sub-folders</span></code></p></li>
<li><p>Click <code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">an</span> <span class="pre">S3</span> <span class="pre">data</span> <span class="pre">source</span></code>
<img alt="GlueNext" src="_images/GlueNext.png" /><br></p></li>
<li><p>Click <code class="docutils literal notranslate"><span class="pre">Next</span></code></p></li>
</ul>
</li>
<li><p><strong>Step 3: Configure security settings</strong>:</p>
<ul>
<li><p>From the IAM role drop down menu select the role already created for you, <code class="docutils literal notranslate"><span class="pre">weather-analytics-glue-role-dev</span></code></p></li>
<li><p>Click <code class="docutils literal notranslate"><span class="pre">Next</span></code>
<img alt="GlueRoleSelect" src="_images/GlueRoleSelect.png" /><br></p></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üîê The CloudFormation template already created this role with appropriate permissions for the crawler to access S3 and create Data Catalog entries.</p>
</div>
<ul class="simple">
<li><p><strong>Step 4 Set output and scheduling</strong>:</p>
<ul>
<li><p>Target Database: Select <code class="docutils literal notranslate"><span class="pre">weather_analytics_dev_db</span></code></p></li>
<li><p>For table prefix, enter: <code class="docutils literal notranslate"><span class="pre">raw_</span></code></p></li>
<li><p>Crawler schedule Frequency:  <code class="docutils literal notranslate"><span class="pre">On</span> <span class="pre">demand</span></code></p></li>
<li><p>Click <code class="docutils literal notranslate"><span class="pre">Next</span></code>
<img alt="GlueOutput" src="_images/GlueOutput.png" /><br></p></li>
</ul>
</li>
<li><p><strong>Step 5: Review and create</strong>:</p>
<ul>
<li><p>Review your settings</p></li>
<li><p>Click <code class="docutils literal notranslate"><span class="pre">Create</span> <span class="pre">crawler</span></code></p></li>
</ul>
</li>
</ul>
</section>
<section id="run-the-crawler">
<h3>üèÉ‚Äç‚ôÇÔ∏è Run the Crawler:<a class="headerlink" href="#run-the-crawler" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Start Crawler</strong>:</p>
<ul class="simple">
<li><p>Click <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">crawler</span></code></p></li>
<li><p>Wait for completion (usually around 2 minutes)</p></li>
</ul>
</li>
<li><p><strong>Verify Results and Edit Schema</strong>:</p>
<ul class="simple">
<li><p>From the left hand navigation menu, expand <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Catalog</span></code> and click on <code class="docutils literal notranslate"><span class="pre">Tables</span></code>, then select the table name created by the Crawler you prefixed earlier with <code class="docutils literal notranslate"><span class="pre">raw_</span></code> and should be called <code class="docutils literal notranslate"><span class="pre">raw_weather_data</span></code></p></li>
<li><p>Click on the table to examine its schema
<img alt="GlueTableSchema" src="_images/GlueTableSchema.png" /><br></p></li>
<li><p>At the top right of the Schema click <code class="docutils literal notranslate"><span class="pre">Edit</span> <span class="pre">schemas</span> <span class="pre">as</span> <span class="pre">JSON</span></code> and for the two fields of <code class="docutils literal notranslate"><span class="pre">measurement_time</span></code> and <code class="docutils literal notranslate"><span class="pre">collection_time</span></code> modify the type from <code class="docutils literal notranslate"><span class="pre">string</span></code> to <code class="docutils literal notranslate"><span class="pre">timestamp</span></code> then click <code class="docutils literal notranslate"><span class="pre">Save</span> <span class="pre">as</span> <span class="pre">new</span> <span class="pre">table</span> <span class="pre">version</span></code>.
<img alt="GlueTableEditSchema" src="_images/GlueTableEditSchema.png" /><br></p></li>
</ul>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üìö Notice how Glue automatically:</p>
<ul class="simple">
<li><p>Detected the JSON structure</p></li>
<li><p>Identified some but not all data types (we changed two columns to timestamp)</p></li>
<li><p>Recognised the partitioning scheme (location/year/month/day)</p></li>
</ul>
</div>
</section>
<section id="set-up-athena-query-environment">
<h3>‚öôÔ∏è Set Up Athena Query Environment:<a class="headerlink" href="#set-up-athena-query-environment" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Configure Athena Settings</strong>:</p>
<ul class="simple">
<li><p>In the AWS Console, search for <code class="docutils literal notranslate"><span class="pre">Athena</span></code> and open it in a new tab</p></li>
<li><p>Then using the default option select <code class="docutils literal notranslate"><span class="pre">Launch</span> <span class="pre">query</span> <span class="pre">editor</span></code><br>
<img alt="AthenaQuery" src="_images/AthenaQuery.png" /><br></p></li>
<li><p>You should land on the <code class="docutils literal notranslate"><span class="pre">Editor</span></code> tab of Athena. Click on the <code class="docutils literal notranslate"><span class="pre">Settings</span></code> tab further to the right, then click the <code class="docutils literal notranslate"><span class="pre">Manage</span></code> button.
<img alt="AthenaLanding" src="_images/AthenaLanding.png" /><br></p></li>
<li><p>For the <code class="docutils literal notranslate"><span class="pre">Location</span> <span class="pre">of</span> <span class="pre">query</span> <span class="pre">result</span></code> box click <code class="docutils literal notranslate"><span class="pre">Browse</span> <span class="pre">S3</span></code> button to the right of it and select the bucket: <code class="docutils literal notranslate"><span class="pre">s3://weather-analytics-athena-results-dev-[YOUR-ACCOUNT-ID]/</span></code> then click <code class="docutils literal notranslate"><span class="pre">Choose</span></code>, then <code class="docutils literal notranslate"><span class="pre">Save</span></code>.
<img alt="AthenaQueryS3" src="_images/AthenaQueryS3.png" /><br></p></li>
</ul>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üìÅ We already created this results bucket in our CloudFormation template with appropriate lifecycle rules to clean up old query results automatically. This helps manage storage costs while maintaining useful query history.</p>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>Understanding the Athena default settings</strong>:</p>
<ul class="simple">
<li><p>Click on the <code class="docutils literal notranslate"><span class="pre">Editor</span></code> tab of Athena.</p></li>
<li><p>Look at the top right of the screen and note the default <code class="docutils literal notranslate"><span class="pre">primary</span></code> workgroup is selected. We will use this for our workshop.</p></li>
<li><p>Looking on the left-hand side, for <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">source</span></code> and note that <code class="docutils literal notranslate"><span class="pre">AwsDataCatalog</span></code> is selected by default, and below that <code class="docutils literal notranslate"><span class="pre">catalogue</span></code> is none.</p></li>
<li><p>Also, note that Athena has detected and selected the Glue database called <code class="docutils literal notranslate"><span class="pre">weather-analytics_dev_db</span></code> that was created by the CloudFormation template and is in our <code class="docutils literal notranslate"><span class="pre">AwsDataCatalog</span></code>.</p></li>
<li><p>Finally, look bottom left in the <code class="docutils literal notranslate"><span class="pre">Tables</span></code> section where the table <code class="docutils literal notranslate"><span class="pre">raw_weather_data</span></code> we created with the Glue Crawler can be seen ready to query!</p></li>
<li><p><img alt="AthenaEditor" src="_images/AthenaEditor.png" /></p></li>
</ul>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üìö In production environments, creating separate workgroups is recommended by AWS. This allows for:</p>
<ul class="simple">
<li><p>Cost tracking and control</p></li>
<li><p>Team-specific configurations</p></li>
<li><p>Usage attribution</p></li>
</ul>
<p>The default settings you see are for the <code class="docutils literal notranslate"><span class="pre">primary</span></code> workgroup.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">AwsDataCatalog</span></code> is the system Athena uses to organise metadata. You can think of it as the root of your data organisation.</p></li>
<li><p>A catalog is a group of databases within the <code class="docutils literal notranslate"><span class="pre">AwsDataCatalog</span></code>. We are using the <code class="docutils literal notranslate"><span class="pre">AwsDataCatalog</span></code> with no sub-catalogs.</p></li>
<li><p>Athena has selected the <code class="docutils literal notranslate"><span class="pre">weather-analytics_dev_db</span></code> database because it is the only database in the <code class="docutils literal notranslate"><span class="pre">AwsDataCatalog</span></code>.</p></li>
</ul>
<p>To learn more see: <a class="reference external" href="https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html">https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html</a></p>
</div>
</section>
<section id="inspect-your-weather-data">
<h3>üìä Inspect Your Weather Data:<a class="headerlink" href="#inspect-your-weather-data" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Basic Data Exploration</strong>:</p>
<ul class="simple">
<li><p>To quickly query the data in Athena, click on the three dots to the right of a table then select <code class="docutils literal notranslate"><span class="pre">Preview</span> <span class="pre">Table</span></code>. This auto-generates and runs working SQL to view the first 10 rows of the table.
<img alt="AthenaPreviewTable" src="_images/AthenaPreviewTable.png" /></p></li>
<li><p>Or paste the code below into the query pane and click <code class="docutils literal notranslate"><span class="pre">Run</span></code>.</p></li>
</ul>
</li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">raw_weather_data</span>
<span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">measurement_time</span><span class="w"> </span><span class="k">DESC</span>
<span class="k">LIMIT</span><span class="w"> </span><span class="mi">10</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="create-clean-view-of-weather-data">
<h3>üßΩ Create Clean View of Weather Data:<a class="headerlink" href="#create-clean-view-of-weather-data" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Understanding Raw Data</strong>:</p>
<ul class="simple">
<li><p>First, let‚Äôs examine our raw data to understand the duplication pattern:</p></li>
</ul>
</li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">raw_weather_data</span>
<span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">city</span><span class="p">,</span><span class="w"> </span><span class="n">measurement_time</span>
<span class="k">LIMIT</span><span class="w"> </span><span class="mi">30</span><span class="p">;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Notice how we have:</p>
<ul>
<li><p>Multiple rows with the same temperature and measurement_time</p></li>
<li><p>Different <code class="docutils literal notranslate"><span class="pre">collection_times</span></code> for the same reading</p></li>
<li><p>Weather readings that update every 15 minutes</p></li>
</ul>
</li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Create a Clean View</strong>:</p>
<ul class="simple">
<li><p>Let‚Äôs create a view in Athena that removes this deduplication:</p></li>
</ul>
</li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="k">OR</span><span class="w"> </span><span class="k">REPLACE</span><span class="w"> </span><span class="k">VIEW</span><span class="w"> </span><span class="n">clean_weather_data</span><span class="w"> </span><span class="k">AS</span>
<span class="k">SELECT</span><span class="w"> </span>
<span class="w">    </span><span class="n">city</span><span class="p">,</span>
<span class="w">    </span><span class="n">temperature</span><span class="p">,</span>
<span class="w">    </span><span class="n">measurement_time</span><span class="p">,</span>
<span class="w">    </span><span class="k">MIN</span><span class="p">(</span><span class="n">collection_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">first_collection_time</span><span class="p">,</span>
<span class="w">    </span><span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">collection_count</span><span class="p">,</span>
<span class="w">    </span><span class="k">location</span><span class="p">,</span>
<span class="w">    </span><span class="k">year</span><span class="p">,</span>
<span class="w">    </span><span class="k">month</span><span class="p">,</span>
<span class="w">    </span><span class="k">day</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">raw_weather_data</span>
<span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span>
<span class="w">    </span><span class="n">city</span><span class="p">,</span>
<span class="w">    </span><span class="n">temperature</span><span class="p">,</span>
<span class="w">    </span><span class="n">measurement_time</span><span class="p">,</span>
<span class="w">    </span><span class="k">location</span><span class="p">,</span>
<span class="w">    </span><span class="k">year</span><span class="p">,</span>
<span class="w">    </span><span class="k">month</span><span class="p">,</span>
<span class="w">    </span><span class="k">day</span><span class="p">;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üéØ This view:</p>
<ul class="simple">
<li><p>Takes only the first collection of each unique reading</p></li>
<li><p>Maintains the partition columns for performance</p></li>
<li><p>Tracks how many times each reading was collected (useful for monitoring)</p></li>
</ul>
</div>
<ol class="arabic simple" start="3">
<li><p><strong>Verify the View</strong>:</p>
<ul class="simple">
<li><p>Let‚Äôs check our view is working as expected. Does this return de-duplicated data for you?</p></li>
</ul>
</li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">clean_weather_data</span>
<span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">city</span><span class="p">,</span><span class="w"> </span><span class="n">measurement_time</span><span class="w"> </span><span class="k">DESC</span>
<span class="k">LIMIT</span><span class="w"> </span><span class="mi">10</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="analyse-clean-data">
<h3>üìà Analyse Clean Data:<a class="headerlink" href="#analyse-clean-data" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Temperature Trends</strong>:</p>
<ul class="simple">
<li><p>Now we can write cleaner, more intuitive queries:</p></li>
</ul>
</li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span>
<span class="w">    </span><span class="n">city</span><span class="p">,</span>
<span class="w">    </span><span class="n">DATE_TRUNC</span><span class="p">(</span><span class="s1">&#39;hour&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">hour</span><span class="p">,</span>
<span class="w">    </span><span class="k">AVG</span><span class="p">(</span><span class="n">temperature</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">avg_temp</span><span class="p">,</span>
<span class="w">    </span><span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">readings_per_hour</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">clean_weather_data</span>
<span class="k">WHERE</span><span class="w"> </span><span class="k">year</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;2025&#39;</span><span class="w"> </span><span class="c1">-- Change to current year</span>
<span class="w">    </span><span class="k">AND</span><span class="w"> </span><span class="k">month</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;01&#39;</span><span class="w"> </span><span class="c1">-- Change to current month</span>
<span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span>
<span class="w">    </span><span class="n">city</span><span class="p">,</span>
<span class="w">    </span><span class="n">DATE_TRUNC</span><span class="p">(</span><span class="s1">&#39;hour&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">measurement_time</span><span class="p">)</span>
<span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span>
<span class="w">    </span><span class="n">city</span><span class="p">,</span><span class="w"> </span>
<span class="w">    </span><span class="n">hour</span><span class="w"> </span><span class="k">DESC</span><span class="p">;</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>City Comparisons</strong>:</p></li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span>
<span class="w">    </span><span class="n">city</span><span class="p">,</span>
<span class="w">    </span><span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">total_readings</span><span class="p">,</span>
<span class="w">    </span><span class="n">ROUND</span><span class="p">(</span><span class="k">AVG</span><span class="p">(</span><span class="n">temperature</span><span class="p">),</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">avg_temp</span><span class="p">,</span>
<span class="w">    </span><span class="n">ROUND</span><span class="p">(</span><span class="k">MIN</span><span class="p">(</span><span class="n">temperature</span><span class="p">),</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">min_temp</span><span class="p">,</span>
<span class="w">    </span><span class="n">ROUND</span><span class="p">(</span><span class="k">MAX</span><span class="p">(</span><span class="n">temperature</span><span class="p">),</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">max_temp</span><span class="p">,</span>
<span class="w">    </span><span class="n">ROUND</span><span class="p">(</span><span class="n">STDDEV</span><span class="p">(</span><span class="n">temperature</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">temp_variation</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">clean_weather_data</span>
<span class="k">WHERE</span><span class="w"> </span><span class="k">year</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;2025&#39;</span>
<span class="w">    </span><span class="k">AND</span><span class="w"> </span><span class="k">month</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;01&#39;</span>
<span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">city</span>
<span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">avg_temp</span><span class="w"> </span><span class="k">DESC</span><span class="p">;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üí° Using the view:</p>
<ul class="simple">
<li><p>Makes queries more readable</p></li>
<li><p>Ensures consistent deduplication</p></li>
<li><p>Improves query performance (less data processed)</p></li>
<li><p>Makes it easier to modify deduplication logic if needed</p></li>
</ul>
</div>
<ol class="arabic simple" start="3">
<li><p><strong>Data Quality Monitoring</strong>:</p>
<ul class="simple">
<li><p>We can also monitor our collection process:</p></li>
</ul>
</li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span>
<span class="w">    </span><span class="n">city</span><span class="p">,</span>
<span class="w">    </span><span class="nb">DATE</span><span class="p">(</span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nb">date</span><span class="p">,</span>
<span class="w">    </span><span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">readings</span><span class="p">,</span>
<span class="w">    </span><span class="k">AVG</span><span class="p">(</span><span class="n">collection_count</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">avg_collections_per_reading</span><span class="p">,</span>
<span class="w">    </span><span class="k">MAX</span><span class="p">(</span><span class="n">collection_count</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">max_collections_per_reading</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">clean_weather_data</span>
<span class="k">WHERE</span><span class="w"> </span><span class="k">year</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;2025&#39;</span>
<span class="w">    </span><span class="k">AND</span><span class="w"> </span><span class="k">month</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;01&#39;</span>
<span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span>
<span class="w">    </span><span class="n">city</span><span class="p">,</span>
<span class="w">    </span><span class="nb">DATE</span><span class="p">(</span><span class="n">measurement_time</span><span class="p">)</span>
<span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span>
<span class="w">    </span><span class="nb">date</span><span class="w"> </span><span class="k">DESC</span><span class="p">,</span>
<span class="w">    </span><span class="n">city</span><span class="p">;</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p><strong>Leverage Partitioning</strong>:</p></li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span>
<span class="w">    </span><span class="n">city</span><span class="p">,</span>
<span class="w">    </span><span class="k">AVG</span><span class="p">(</span><span class="n">temperature</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">avg_temp</span><span class="p">,</span>
<span class="w">    </span><span class="k">MIN</span><span class="p">(</span><span class="n">temperature</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">min_temp</span><span class="p">,</span>
<span class="w">    </span><span class="k">MAX</span><span class="p">(</span><span class="n">temperature</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">max_temp</span><span class="p">,</span>
<span class="w">    </span><span class="nb">DATE</span><span class="p">(</span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nb">date</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">clean_weather_data</span>
<span class="k">WHERE</span><span class="w"> </span><span class="k">location</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;London&#39;</span>
<span class="w">    </span><span class="k">AND</span><span class="w"> </span><span class="k">year</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;2025&#39;</span><span class="w"> </span><span class="c1">-- Change this to the current year</span>
<span class="w">    </span><span class="k">AND</span><span class="w"> </span><span class="k">month</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;01&#39;</span><span class="w">  </span><span class="c1">-- Change this to the current month</span>
<span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">city</span><span class="p">,</span><span class="w"> </span><span class="nb">DATE</span><span class="p">(</span><span class="n">measurement_time</span><span class="p">)</span>
<span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="nb">date</span><span class="w"> </span><span class="k">DESC</span><span class="p">;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üí° Notice how we use partition columns (location, year, month) in the WHERE clause. Athena uses these to read only relevant data files, making queries more efficient and cost-effective.</p>
</div>
<ol class="arabic simple" start="5">
<li><p><strong>City Comparison Analysis</strong>:</p></li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span>
<span class="w">    </span><span class="n">city</span><span class="p">,</span>
<span class="w">    </span><span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">measurements</span><span class="p">,</span>
<span class="w">    </span><span class="n">ROUND</span><span class="p">(</span><span class="k">AVG</span><span class="p">(</span><span class="n">temperature</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">avg_temp</span><span class="p">,</span>
<span class="w">    </span><span class="n">ROUND</span><span class="p">(</span><span class="n">STDDEV</span><span class="p">(</span><span class="n">temperature</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">temp_stddev</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">clean_weather_data</span>
<span class="k">WHERE</span><span class="w"> </span><span class="k">year</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;2025&#39;</span>
<span class="w">    </span><span class="k">AND</span><span class="w"> </span><span class="k">month</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;01&#39;</span>
<span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">city</span>
<span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">avg_temp</span><span class="w"> </span><span class="k">DESC</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="advanced-analytics">
<h3>üìà Advanced Analytics:<a class="headerlink" href="#advanced-analytics" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Temperature Trends</strong>:</p></li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">WITH</span><span class="w"> </span><span class="n">hourly_temps</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="p">(</span>
<span class="w">    </span><span class="k">SELECT</span><span class="w"> </span>
<span class="w">        </span><span class="n">city</span><span class="p">,</span>
<span class="w">        </span><span class="n">DATE_TRUNC</span><span class="p">(</span><span class="s1">&#39;hour&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">hour</span><span class="p">,</span>
<span class="w">        </span><span class="k">AVG</span><span class="p">(</span><span class="n">temperature</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">avg_temp</span>
<span class="w">    </span><span class="k">FROM</span><span class="w"> </span><span class="n">clean_weather_data</span>
<span class="w">    </span><span class="k">WHERE</span><span class="w"> </span><span class="k">year</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;2025&#39;</span><span class="w"> </span><span class="k">AND</span><span class="w"> </span><span class="k">month</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;01&#39;</span>
<span class="w">    </span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">city</span><span class="p">,</span><span class="w"> </span><span class="n">DATE_TRUNC</span><span class="p">(</span><span class="s1">&#39;hour&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">measurement_time</span><span class="p">)</span>
<span class="p">)</span>
<span class="k">SELECT</span><span class="w"> </span>
<span class="w">    </span><span class="n">city</span><span class="p">,</span>
<span class="w">    </span><span class="n">hour</span><span class="p">,</span>
<span class="w">    </span><span class="n">avg_temp</span><span class="p">,</span>
<span class="w">    </span><span class="n">LAG</span><span class="p">(</span><span class="n">avg_temp</span><span class="p">)</span><span class="w"> </span><span class="n">OVER</span><span class="w"> </span><span class="p">(</span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">hour</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">prev_hour_temp</span><span class="p">,</span>
<span class="w">    </span><span class="n">ROUND</span><span class="p">(</span><span class="n">avg_temp</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">LAG</span><span class="p">(</span><span class="n">avg_temp</span><span class="p">)</span><span class="w"> </span><span class="n">OVER</span><span class="w"> </span><span class="p">(</span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">hour</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">temp_change</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">hourly_temps</span>
<span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">city</span><span class="p">,</span><span class="w"> </span><span class="n">hour</span><span class="w"> </span><span class="k">DESC</span><span class="p">;</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>Data Quality Checks</strong>:</p></li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span>
<span class="w">    </span><span class="k">location</span><span class="p">,</span>
<span class="w">    </span><span class="k">year</span><span class="p">,</span>
<span class="w">    </span><span class="k">month</span><span class="p">,</span>
<span class="w">    </span><span class="k">day</span><span class="p">,</span>
<span class="w">    </span><span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">record_count</span><span class="p">,</span>
<span class="w">    </span><span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span><span class="w"> </span><span class="k">EXTRACT</span><span class="p">(</span><span class="n">hour</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">measurement_time</span><span class="p">))</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">unique_hours</span><span class="p">,</span>
<span class="w">    </span><span class="k">MIN</span><span class="p">(</span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">first_record</span><span class="p">,</span>
<span class="w">    </span><span class="k">MAX</span><span class="p">(</span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">last_record</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">clean_weather_data</span>
<span class="k">WHERE</span><span class="w"> </span><span class="k">year</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;2025&#39;</span><span class="w"> </span><span class="k">AND</span><span class="w"> </span><span class="k">month</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;01&#39;</span>
<span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="k">location</span><span class="p">,</span><span class="w"> </span><span class="k">year</span><span class="p">,</span><span class="w"> </span><span class="k">month</span><span class="p">,</span><span class="w"> </span><span class="k">day</span>
<span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="k">location</span><span class="p">,</span><span class="w"> </span><span class="k">year</span><span class="p">,</span><span class="w"> </span><span class="k">month</span><span class="p">,</span><span class="w"> </span><span class="k">day</span><span class="w"> </span><span class="k">DESC</span><span class="p">;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>‚úÖ These quality checks help identify any gaps in data collection. In a production environment, you might set up alerts based on these metrics to monitor data pipeline health.</p>
</div>
</section>
<section id="challenge-exercises">
<h3>üéØ Challenge Exercises:<a class="headerlink" href="#challenge-exercises" title="Link to this heading">#</a></h3>
<p>Try writing queries to answer these questions:</p>
<ol class="arabic simple">
<li><p>Which city has the most variable temperature (highest standard deviation)?</p></li>
<li><p>What time of day typically records the highest temperatures?</p></li>
<li><p>Calculate the rolling 3-hour average temperature for each city.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üí° Hint: Look up Athena‚Äôs window functions documentation for the rolling average calculation.</p>
</div>
</section>
<section id="cost-management-best-practices">
<h3>Cost Management Best Practices:<a class="headerlink" href="#cost-management-best-practices" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Optimise Your Queries</strong>:</p>
<ul class="simple">
<li><p>Always use partition filtering when possible</p></li>
<li><p>Select only needed columns instead of SELECT *</p></li>
<li><p>Use appropriate data types and compression</p></li>
</ul>
</li>
<li><p><strong>Monitor Query Metrics</strong>:</p>
<ul class="simple">
<li><p>Click <code class="docutils literal notranslate"><span class="pre">Recent</span> <span class="pre">queries</span></code> to view:</p>
<ul>
<li><p>Data scanned per query</p></li>
<li><p>Execution time</p></li>
<li><p>Cost implications</p></li>
</ul>
</li>
</ul>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üí∞ Athena pricing is based on data scanned. Well-structured queries on partitioned data help minimise costs. The partitioning scheme we implemented (by city and date) helps optimise both query performance and cost.</p>
</div>
</section>
</section>
<section id="wrapping-up-and-understanding-real-time-vs-near-real-time-streaming">
<h2>Wrapping Up and Understanding Real-Time vs Near Real-Time Streaming<a class="headerlink" href="#wrapping-up-and-understanding-real-time-vs-near-real-time-streaming" title="Link to this heading">#</a></h2>
<p>Our workshop has demonstrated near real-time streaming, but let‚Äôs understand how this differs from true real-time processing and how we could modify our architecture for real-time needs.</p>
<section id="current-architecture-near-real-time-implementation">
<h3>Current Architecture: Near Real-Time Implementation<a class="headerlink" href="#current-architecture-near-real-time-implementation" title="Link to this heading">#</a></h3>
<p>Our current pipeline introduces several intentional delays:</p>
<ol class="arabic simple">
<li><p>Lambda function collects data every 60 seconds from the weather API</p></li>
<li><p>Kinesis Data Streams acts as a buffer, holding records for up to 24 hours</p></li>
<li><p>Kinesis Firehose manages the delivery of data to S3:</p>
<ul class="simple">
<li><p>Writes occur when either condition is first met:</p>
<ul>
<li><p>60 seconds have elapsed since last write, OR</p></li>
<li><p>64 MB of data has accumulated</p></li>
</ul>
</li>
<li><p>In our case, with small temperature readings, the 60-second timer typically triggers first</p></li>
</ul>
</li>
<li><p>Glue crawler keeps our data catalog updated:</p>
<ul class="simple">
<li><p>Main exercise: Manual runs as needed</p></li>
<li><p>Going Further exercise: Automated 10-minute schedule as part of the orchestrated workflow</p></li>
</ul>
</li>
<li><p>End-to-end timing:</p>
<ul class="simple">
<li><p>~2-3 minutes from collection until data is available in S3 for Athena queries</p></li>
<li><p>If completing the Going Further exercise, both the data catalog and Redshift table refresh every 10 minutes through the orchestrated workflow
This creates a near real-time system where temperature data is typically available for analysis within a few minutes of collection, with regular refreshes of our analytical views (Redshift) every 10 minutes if using the orchestrated workflow.</p></li>
</ul>
</li>
</ol>
</section>
<section id="real-time-processing">
<h3>Real-Time Processing<a class="headerlink" href="#real-time-processing" title="Link to this heading">#</a></h3>
<p>To achieve real-time processing we could modify our architecture in several ways including:</p>
<ol class="arabic simple">
<li><p><strong>Use Kinesis Data Analytics with Apache Flink</strong>:</p></li>
</ol>
<ul class="simple">
<li><p>Process data directly in the stream using continuous SQL queries</p></li>
<li><p>React immediately to critical temperature thresholds</p></li>
<li><p>Filter, aggregate and analyse data in-motion</p></li>
<li><p>Here is an example tumbling window query analogous to Azure Stream Analytics tumbling windows covered in Module 9.3 Streaming tools and frameworks:</p></li>
</ul>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span><span class="n">TUMBLE_START</span><span class="p">(</span><span class="n">measurement_time</span><span class="p">,</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;15&#39;</span><span class="w"> </span><span class="n">MINUTES</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">window_start</span><span class="p">,</span>
<span class="w">       </span><span class="n">city</span><span class="p">,</span>
<span class="w">       </span><span class="k">AVG</span><span class="p">(</span><span class="k">DISTINCT</span><span class="w"> </span><span class="n">temperature</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">avg_temperature</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">weather_source</span>
<span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">TUMBLE</span><span class="p">(</span><span class="n">measurement_time</span><span class="p">,</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;15&#39;</span><span class="w"> </span><span class="n">MINUTES</span><span class="p">),</span><span class="w"> </span><span class="n">city</span><span class="p">;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Note: Using DISTINCT in the aggregation ensures we don‚Äôt skew averages by counting the same temperature multiple times, as our Lambda polls every minute but the weather API only updates every 15 minutes</p></li>
<li><p>You could start to explore this yourself by going to the Kinesis Data stream, and from the tab called <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">analytics</span> <span class="pre">-</span> <span class="pre">new</span></code> launch true real time data analytics with Apache Flink. Find out more here: <a class="reference external" href="https://docs.aws.amazon.com/managed-flink/latest/java/how-notebook.html">https://docs.aws.amazon.com/managed-flink/latest/java/how-notebook.html</a>
<img alt="ApacheFlink" src="_images/ApacheFlink.png" /><br></p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Firehose Lambda Transformation</strong>:</p>
<ul class="simple">
<li><p>Configure Firehose to invoke a Lambda function for data transformation</p></li>
<li><p>Lambda processes batches of records before they reach S3</p></li>
<li><p>Transform data in-flight with up to 5-minute processing window</p></li>
<li><p>Enable data cleaning, aggregation, or enrichment</p></li>
<li><p>Useful for deduplicating temperature readings that are collected every minute but only update every 15 minutes</p></li>
<li><p>Find out more here: <a class="reference external" href="https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html">https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html</a></p></li>
</ul>
</li>
<li><p><strong>Lambda Stream Processing</strong>:</p>
<ul class="simple">
<li><p>Configure Lambda to trigger on every record</p></li>
<li><p>Remove the one-minute collection interval</p></li>
<li><p>Process each temperature reading as it arrives</p></li>
<li><p>Enable immediate alerts and actions</p></li>
</ul>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The choice between real-time and near real-time depends on your specific needs. For instance with this workshop scenario:</p>
<p>Real-Time Processing (milliseconds):</p>
<ul class="simple">
<li><p>Critical temperature thresholds requiring immediate shutdown</p></li>
<li><p>Emergency cooling system activation</p></li>
<li><p>Live turbine control adjustments</p></li>
</ul>
<p>Near Real-Time Processing (seconds/minutes):</p>
<ul class="simple">
<li><p>General temperature monitoring</p></li>
<li><p>Maintenance scheduling</p></li>
<li><p>Performance optimisation</p></li>
<li><p>Historical analysis</p></li>
</ul>
</div>
</section>
<section id="this-afternoon-reflecting-on-your-project">
<h3>This afternoon - reflecting on your project<a class="headerlink" href="#this-afternoon-reflecting-on-your-project" title="Link to this heading">#</a></h3>
<p>This afternoon, we will consider how your organisation or project and its potential use cases that might benefit from streaming architectures. Consider:</p>
<ul class="simple">
<li><p>Could streaming solve any current data freshness challenges?</p></li>
<li><p>Would near real-time processing provide business value?</p></li>
<li><p>How might you adapt any of today‚Äôs patterns for your specific needs?</p></li>
</ul>
<hr class="docutils" />
<p>üéâ Congratulations - you‚Äôve completed today‚Äôs main exercise!</p>
<p>Now that you‚Äôve explored your weather data lake with Athena, if you‚Äôre up for the going further exercises, you‚Äôre ready to move on to more advanced analytics. Below are three optional exercises that extend what you‚Äôve learned. Choose any that:</p>
<ul class="simple">
<li><p>Are relevant to your current role</p></li>
<li><p>Match your project‚Äôs needs</p></li>
<li><p>Interest you technically</p></li>
</ul>
<p>Even if you don‚Äôt complete them, consider reviewing what they cover in your own time, they demonstrate common patterns you might need later in your data engineering career.</p>
</section>
</section>
<section id="going-further">
<h2>üöÄ Going Further<a class="headerlink" href="#going-further" title="Link to this heading">#</a></h2>
<section id="going-further-1-orchestrated-pipeline-to-write-to-redshift">
<h3><strong>üéº Going Further 1:</strong> Orchestrated pipeline to write to Redshift<a class="headerlink" href="#going-further-1-orchestrated-pipeline-to-write-to-redshift" title="Link to this heading">#</a></h3>
<blockquote>
<div><p>Our wind farm operators need their temperature data accessible for maintenance planning and turbine performance analysis. While our streaming pipeline continuously collects data in S3, the team wants an automatically refreshing Redshift table they can connect to PowerBI for ongoing temperature monitoring. In this exercise, we‚Äôll create this analytics pipeline by building a specialised Redshift table, developing a Glue visual ETL job to process the temperature readings, setting up temperature trend visualisation, and orchestrating regular data refresh using AWS Glue Workflows to coordinate crawling and ETL processes.</p>
</div></blockquote>
<section id="create-target-table-in-redshift">
<h4>üéØ Create target table in Redshift<a class="headerlink" href="#create-target-table-in-redshift" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Connect to Redshift database</strong>:</p>
<ul class="simple">
<li><p>In the search bar at the top, search for <code class="docutils literal notranslate"><span class="pre">Redshift</span></code> and open in a new tab.</p></li>
<li><p>You will see several <code class="docutils literal notranslate"><span class="pre">Access</span> <span class="pre">denied..</span></code> messages you can safely ignore (they are explained in the note below).</p></li>
<li><p>Click on the only Redshiit cluster listed called <code class="docutils literal notranslate"><span class="pre">weather-stream</span></code>.</p></li>
<li><p>Click on the <code class="docutils literal notranslate"><span class="pre">Query</span> <span class="pre">data</span></code> dropdown (that is on the top right) then select <code class="docutils literal notranslate"><span class="pre">Query</span> <span class="pre">in</span> <span class="pre">query</span> <span class="pre">editor</span></code>.</p></li>
<li><p>Click <code class="docutils literal notranslate"><span class="pre">Connect</span> <span class="pre">to</span> <span class="pre">database</span></code> then in the <code class="docutils literal notranslate"><span class="pre">Database</span> <span class="pre">name</span></code> box enter <code class="docutils literal notranslate"><span class="pre">weather</span></code> and for the <code class="docutils literal notranslate"><span class="pre">Database</span> <span class="pre">user</span></code> enter <code class="docutils literal notranslate"><span class="pre">corndeladmin</span></code>, then click <code class="docutils literal notranslate"><span class="pre">Connect</span></code>.
<img alt="RedshfitDatabaseConnect" src="_images/RedshfitDatabaseConnect.png" /></p></li>
</ul>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>‚õî  The red <code class="docutils literal notranslate"><span class="pre">Access</span> <span class="pre">denied..</span></code> error messages can be safely ignored. They simply indicate that you don‚Äôt have administrative permissions for Redshift Serverless backups, workgroups, and snapshots. You can still do everything needed for creating tables, querying data, and running ETL processes.</p>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>Create Redshift target table</strong>:</p>
<ul class="simple">
<li><p>In the Redshift Query editor copy and paste the SQL below then click <code class="docutils literal notranslate"><span class="pre">Run</span></code>.</p></li>
</ul>
</li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">DROP</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="w"> </span><span class="n">real_time_weather_stats</span><span class="p">;</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">real_time_weather_stats</span><span class="w"> </span><span class="p">(</span>
<span class="w">   </span><span class="n">window_start_time</span><span class="w"> </span><span class="k">TIMESTAMP</span><span class="p">,</span>
<span class="w">   </span><span class="n">window_end_time</span><span class="w"> </span><span class="k">TIMESTAMP</span><span class="p">,</span>
<span class="w">   </span><span class="n">city</span><span class="w"> </span><span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">255</span><span class="p">),</span>
<span class="w">   </span><span class="n">temperature</span><span class="w"> </span><span class="n">DOUBLE</span><span class="w"> </span><span class="k">PRECISION</span><span class="p">,</span>
<span class="w">   </span><span class="n">previous_temperature</span><span class="w"> </span><span class="n">DOUBLE</span><span class="w"> </span><span class="k">PRECISION</span><span class="p">,</span>
<span class="w">   </span><span class="n">temperature_change</span><span class="w"> </span><span class="n">DOUBLE</span><span class="w"> </span><span class="k">PRECISION</span><span class="p">,</span>
<span class="w">   </span><span class="k">location</span><span class="w"> </span><span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">255</span><span class="p">),</span>
<span class="w">   </span><span class="k">source</span><span class="w"> </span><span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
<span class="w">   </span><span class="n">collection_attempts</span><span class="w"> </span><span class="nb">INTEGER</span><span class="p">,</span>
<span class="w">   </span><span class="n">collection_window_seconds</span><span class="w"> </span><span class="nb">INTEGER</span><span class="p">,</span>
<span class="w">   </span><span class="n">initial_latency_seconds</span><span class="w"> </span><span class="nb">INTEGER</span><span class="p">,</span>
<span class="w">   </span><span class="n">last_update</span><span class="w"> </span><span class="k">TIMESTAMP</span><span class="p">,</span>
<span class="w">   </span><span class="n">ingest_year</span><span class="w"> </span><span class="nb">INTEGER</span><span class="p">,</span>
<span class="w">   </span><span class="n">ingest_month</span><span class="w"> </span><span class="nb">INTEGER</span><span class="p">,</span><span class="w"> </span>
<span class="w">   </span><span class="n">ingest_day</span><span class="w"> </span><span class="nb">INTEGER</span><span class="p">,</span>
<span class="w">   </span><span class="n">ingest_hour</span><span class="w"> </span><span class="nb">INTEGER</span>
<span class="p">)</span>
<span class="n">DISTKEY</span><span class="p">(</span><span class="n">city</span><span class="p">)</span>
<span class="n">SORTKEY</span><span class="p">(</span><span class="n">window_start_time</span><span class="p">,</span><span class="w"> </span><span class="n">city</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="create-aws-glue-visual-etl">
<h4>‚û°Ô∏è Create AWS Glue visual ETL<a class="headerlink" href="#create-aws-glue-visual-etl" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Create Glue Visual ETL and data source</strong>:</p>
<ul class="simple">
<li><p>In the search bar at the top, search for <code class="docutils literal notranslate"><span class="pre">Glue</span></code> and open in a new tab.</p></li>
<li><p>From the left hand menu, from under <code class="docutils literal notranslate"><span class="pre">ELT</span> <span class="pre">jobs</span></code> click on <code class="docutils literal notranslate"><span class="pre">Visual</span> <span class="pre">ETL</span></code> the click <code class="docutils literal notranslate"><span class="pre">Visual</span> <span class="pre">ETL</span></code>.</p></li>
<li><p>From the <code class="docutils literal notranslate"><span class="pre">Sources</span></code> menu click on <code class="docutils literal notranslate"><span class="pre">AWS</span> <span class="pre">Glue</span> <span class="pre">Data</span> <span class="pre">Catalog</span></code> then click on the node itself on the canvas to reveal the properties of the node ot the right.</p></li>
<li><p>From <code class="docutils literal notranslate"><span class="pre">Database</span></code> select <code class="docutils literal notranslate"><span class="pre">weather-analytics_dev_db</span></code>.</p></li>
<li><p>From <code class="docutils literal notranslate"><span class="pre">Table</span></code> select <code class="docutils literal notranslate"><span class="pre">raw_weather_data</span></code> (this is the table you created in the main exercise of this workshop by crawling the S3 data lake).
<img alt="VisualETL_Source" src="_images/VisualETL_Source.png" /></p></li>
<li><p>So that this ETL has permission to access our data source, click on the <code class="docutils literal notranslate"><span class="pre">Job</span> <span class="pre">details</span></code> tab and for the <code class="docutils literal notranslate"><span class="pre">IAM</span> <span class="pre">Role</span></code> drop-down select <code class="docutils literal notranslate"><span class="pre">weather-analytics-glue-role-dev</span></code>. Also change <code class="docutils literal notranslate"><span class="pre">Requested</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">workers</span></code> from the default <code class="docutils literal notranslate"><span class="pre">10</span></code> to <code class="docutils literal notranslate"><span class="pre">2</span></code>. This means our job will use less DPUs and take longer to reach the ‚ÄúA Cloud Guru‚Äù DPU limit.
<img alt="VisualETL_Role" src="_images/VisualETL_Role.png" /></p></li>
<li><p>On the top left edit the name of the ETL to <code class="docutils literal notranslate"><span class="pre">ETL_job</span></code>, then on the top right click <code class="docutils literal notranslate"><span class="pre">Save</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Create Glue Visual ETL with data source and SQL transform</strong>:</p>
<ul class="simple">
<li><p>Click on the <code class="docutils literal notranslate"><span class="pre">Visual</span></code> tab then left click on the data source node on the canvas.</p></li>
<li><p>Then click the blue circle with a plus to add another node to the ETL.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">Transforms</span></code> tab click on <code class="docutils literal notranslate"><span class="pre">SQL</span> <span class="pre">Query</span></code>.</p></li>
<li><p>Then paste the following SQL into the <code class="docutils literal notranslate"><span class="pre">SQL</span> <span class="pre">query</span></code> box. When you paste the SQL into the box it will generate a <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">preview</span></code> showing the result of your query.</p></li>
</ul>
</li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">WITH</span><span class="w"> </span><span class="n">deduplicated_data</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="p">(</span>
<span class="w">    </span><span class="k">SELECT</span><span class="w"> </span>
<span class="w">        </span><span class="n">city</span><span class="p">,</span>
<span class="w">        </span><span class="n">temperature</span><span class="p">,</span>
<span class="w">        </span><span class="n">measurement_time</span><span class="p">,</span>
<span class="w">        </span><span class="k">source</span><span class="p">,</span>
<span class="w">        </span><span class="k">location</span><span class="p">,</span>
<span class="w">        </span><span class="k">MIN</span><span class="p">(</span><span class="n">collection_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">first_collection_time</span><span class="p">,</span>
<span class="w">        </span><span class="k">MAX</span><span class="p">(</span><span class="n">collection_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">last_collection_time</span><span class="p">,</span>
<span class="w">        </span><span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">collection_attempts</span>
<span class="w">    </span><span class="k">FROM</span><span class="w"> </span><span class="n">myDataSource</span>
<span class="w">    </span><span class="k">GROUP</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span>
<span class="w">        </span><span class="n">city</span><span class="p">,</span>
<span class="w">        </span><span class="n">temperature</span><span class="p">,</span>
<span class="w">        </span><span class="n">measurement_time</span><span class="p">,</span>
<span class="w">        </span><span class="k">source</span><span class="p">,</span>
<span class="w">        </span><span class="k">location</span>
<span class="p">)</span>
<span class="k">SELECT</span><span class="w"> </span>
<span class="w">    </span><span class="n">date_trunc</span><span class="p">(</span><span class="s1">&#39;minute&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">window_start_time</span><span class="p">,</span>
<span class="w">    </span><span class="n">date_trunc</span><span class="p">(</span><span class="s1">&#39;minute&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="s1">&#39;15 minute&#39;</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">window_end_time</span><span class="p">,</span>
<span class="w">    </span><span class="n">city</span><span class="p">,</span>
<span class="w">    </span><span class="n">temperature</span><span class="p">,</span>
<span class="w">    </span><span class="n">LAG</span><span class="p">(</span><span class="n">temperature</span><span class="p">)</span><span class="w"> </span><span class="n">OVER</span><span class="w"> </span><span class="p">(</span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">previous_temperature</span><span class="p">,</span>
<span class="w">    </span><span class="n">temperature</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">LAG</span><span class="p">(</span><span class="n">temperature</span><span class="p">)</span><span class="w"> </span><span class="n">OVER</span><span class="w"> </span><span class="p">(</span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">temperature_change</span><span class="p">,</span>
<span class="w">    </span><span class="k">location</span><span class="p">,</span>
<span class="w">    </span><span class="k">source</span><span class="p">,</span>
<span class="w">    </span><span class="n">collection_attempts</span><span class="p">,</span>
<span class="w">    </span><span class="n">unix_timestamp</span><span class="p">(</span><span class="n">last_collection_time</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">unix_timestamp</span><span class="p">(</span><span class="n">first_collection_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">collection_window_seconds</span><span class="p">,</span>
<span class="w">    </span><span class="n">unix_timestamp</span><span class="p">(</span><span class="n">first_collection_time</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">unix_timestamp</span><span class="p">(</span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">initial_latency_seconds</span><span class="p">,</span>
<span class="w">    </span><span class="k">current_timestamp</span><span class="p">()</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">last_update</span><span class="p">,</span>
<span class="w">    </span><span class="k">year</span><span class="p">(</span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">ingest_year</span><span class="p">,</span>
<span class="w">    </span><span class="k">month</span><span class="p">(</span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">ingest_month</span><span class="p">,</span>
<span class="w">    </span><span class="k">day</span><span class="p">(</span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">ingest_day</span><span class="p">,</span>
<span class="w">    </span><span class="n">hour</span><span class="p">(</span><span class="n">measurement_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">ingest_hour</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">deduplicated_data</span>
</pre></div>
</div>
<p><img alt="VisualETL_SQL" src="_images/VisualETL_SQL.png" /><br></p>
<ol class="arabic simple" start="4">
<li><p><strong>Add Redshift target node</strong>:</p>
<ul class="simple">
<li><p>With the SQL node selected, click on the the blue circle with a plus to add another node to the ETL.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">Targets</span></code> tab click on <code class="docutils literal notranslate"><span class="pre">Amazon</span> <span class="pre">Redshift</span></code>.</p></li>
<li><p>In the pane to the right, from the <code class="docutils literal notranslate"><span class="pre">Redshift</span> <span class="pre">connection</span></code> select <code class="docutils literal notranslate"><span class="pre">weather-analytics-redshift-connection-dev</span></code>.</p></li>
<li><p>For the <code class="docutils literal notranslate"><span class="pre">Schema</span></code> select <code class="docutils literal notranslate"><span class="pre">public</span></code>.</p></li>
<li><p>For the <code class="docutils literal notranslate"><span class="pre">Table</span></code> select <code class="docutils literal notranslate"><span class="pre">real_time_weather_stats</span></code> (this is the table you created in an earlier step in Redshift).</p></li>
<li><p>For <code class="docutils literal notranslate"><span class="pre">Handling</span> <span class="pre">of</span> <span class="pre">data</span> <span class="pre">and</span> <span class="pre">target</span> <span class="pre">table</span></code> select <code class="docutils literal notranslate"><span class="pre">TRUNCATE</span></code>.</p></li>
<li><p>Finally click <code class="docutils literal notranslate"><span class="pre">Save</span></code> and then <code class="docutils literal notranslate"><span class="pre">Run</span></code> to start your ETL job.</p></li>
<li><p>From the green message at the top you can click on <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">details</span></code> to monitor your job run. It will take around four minutes to complete and for its <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">status</span></code> to change from <code class="docutils literal notranslate"><span class="pre">Running</span></code> to <code class="docutils literal notranslate"><span class="pre">Succeeded</span></code>.
<img alt="VisualETL_Redshift" src="_images/VisualETL_Redshift.png" /><br></p></li>
</ul>
</li>
</ol>
</section>
<section id="inspect-and-visualise-redshift-target-table">
<h4>üìà Inspect and visualise Redshift target table<a class="headerlink" href="#inspect-and-visualise-redshift-target-table" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Inspect data in target table</strong>:</p>
<ul class="simple">
<li><p>Go to the the browser tab for Redshift you had open earlier and use the Query Editor.</p></li>
<li><p>Use the plus icon to open a new blank query window and paste the SQL code from below, then click <code class="docutils literal notranslate"><span class="pre">Run</span></code> to inspect the raw data in the target table.</p></li>
</ul>
</li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">real_time_weather_stats</span><span class="p">;</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>Visualise London temperature over time</strong>:</p>
<ul class="simple">
<li><p>Use the plus icon to open another new blank query window and paste the SQL code below then click <code class="docutils literal notranslate"><span class="pre">Run</span></code>.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">Query</span> <span class="pre">results</span></code> pane click <code class="docutils literal notranslate"><span class="pre">Visualise</span></code>.</p></li>
<li><p>Select <code class="docutils literal notranslate"><span class="pre">Chart</span> <span class="pre">type</span></code> as <code class="docutils literal notranslate"><span class="pre">Bar</span></code>.</p></li>
<li><p>Select <code class="docutils literal notranslate"><span class="pre">X</span> <span class="pre">axis</span></code> as <code class="docutils literal notranslate"><span class="pre">minutessincemidnight</span></code>.</p></li>
<li><p>Select  <code class="docutils literal notranslate"><span class="pre">Y</span> <span class="pre">axis</span></code> at <code class="docutils literal notranslate"><span class="pre">temperature</span></code>.</p></li>
<li><p>Take a screenshot of your plot. We will look at this plot again later and compare it to the plot when run on a new data refresh.</p></li>
</ul>
</li>
</ol>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span>
<span class="w">   </span><span class="k">EXTRACT</span><span class="p">(</span><span class="n">HOUR</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">window_start_time</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">60</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="k">EXTRACT</span><span class="p">(</span><span class="k">MINUTE</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">window_start_time</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="ss">&quot;Minutes Since Midnight&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">ROUND</span><span class="p">(</span><span class="n">temperature</span><span class="p">::</span><span class="nb">numeric</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="ss">&quot;Temperature&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">ROUND</span><span class="p">(</span><span class="n">temperature_change</span><span class="p">::</span><span class="nb">numeric</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="ss">&quot;Temperature Change&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">initial_latency_seconds</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="ss">&quot;Latency (seconds)&quot;</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">real_time_weather_stats</span>
<span class="k">WHERE</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;London&#39;</span><span class="w">  </span>
<span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">window_start_time</span><span class="p">;</span>
</pre></div>
</div>
<p><img alt="Redshift_Plot" src="_images/Redshift_Plot.png" /><br></p>
</section>
<section id="orchestrate-the-entire-process">
<h4>üéº Orchestrate the entire process!<a class="headerlink" href="#orchestrate-the-entire-process" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Create and run orchesterated workflow</strong>:</p>
<ul class="simple">
<li><p>Go to the browser tab for Glue you had open earlier.</p></li>
<li><p>In the left hand menu click on <code class="docutils literal notranslate"><span class="pre">Workflows</span> <span class="pre">(orchestration)</span></code>.</p></li>
<li><p>Click <code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">workflow</span></code> and name it <code class="docutils literal notranslate"><span class="pre">weather_workflow</span></code> then click <code class="docutils literal notranslate"><span class="pre">Create</span> <span class="pre">workflow</span></code>.</p></li>
<li><p>Click on <code class="docutils literal notranslate"><span class="pre">weather_workflow</span></code> then <code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">trigger</span></code> then <code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">new</span></code>.</p></li>
<li><p>Name the trigger <code class="docutils literal notranslate"><span class="pre">start_crawl</span></code>.</p></li>
<li><p>For the <code class="docutils literal notranslate"><span class="pre">Trigger</span> <span class="pre">type</span></code> select <code class="docutils literal notranslate"><span class="pre">Schedule</span></code>.</p></li>
<li><p>For the <code class="docutils literal notranslate"><span class="pre">Frequency</span></code> select <code class="docutils literal notranslate"><span class="pre">Custom</span></code>.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">Cron</span> <span class="pre">expression</span></code> enter <code class="docutils literal notranslate"><span class="pre">*/10</span> <span class="pre">*</span> <span class="pre">*</span> <span class="pre">*</span> <span class="pre">?</span> <span class="pre">*</span></code> which will trigger every 10 minutes.
<img alt="Workflow_Trigger" src="_images/Workflow_Trigger.png" /><br></p></li>
<li><p>In the visual workflow canvas that appears, to the right of the <code class="docutils literal notranslate"><span class="pre">crawl_s3_data_lake</span></code> trigger, click on <code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">node</span></code>, select the <code class="docutils literal notranslate"><span class="pre">crawlers</span></code> tab then tick the crawler name you created in your main exercise (likely called <code class="docutils literal notranslate"><span class="pre">weather-data-crawler</span></code>) then click <code class="docutils literal notranslate"><span class="pre">Add</span></code>.</p></li>
<li><p>Now click on the crawler (the icon with the spider) and click on <code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">trigger</span></code> to the right of it.</p></li>
<li><p>Select the <code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">new</span></code> then name the trigger <code class="docutils literal notranslate"><span class="pre">run_etl</span></code>.</p></li>
<li><p>For <code class="docutils literal notranslate"><span class="pre">Trigger</span> <span class="pre">type</span></code> leave <code class="docutils literal notranslate"><span class="pre">Event</span></code> selected.</p></li>
<li><p>For <code class="docutils literal notranslate"><span class="pre">Trigger</span> <span class="pre">logic</span></code> select <code class="docutils literal notranslate"><span class="pre">Start</span> <span class="pre">after</span> <span class="pre">ALL</span> <span class="pre">watched</span> <span class="pre">event</span></code> then click <code class="docutils literal notranslate"><span class="pre">Add</span></code>.</p></li>
<li><p>Now to the right of the <code class="docutils literal notranslate"><span class="pre">run_etl</span></code> trigger node, click <code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">node</span></code>, select the <code class="docutils literal notranslate"><span class="pre">Jobs</span></code> tab and then tick the visual ETL job you created earlier (likely called <code class="docutils literal notranslate"><span class="pre">ETL_job</span></code>) then click <code class="docutils literal notranslate"><span class="pre">Add</span></code>.</p></li>
<li><p>Finally, click <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">workflow</span></code>.
<img alt="Workflow_Canvas" src="_images/Workflow_Canvas.png" /><br></p></li>
</ul>
</li>
<li><p><strong>Monitor workflow</strong>:</p>
<ul class="simple">
<li><p>In the left hand menu of glue click on <code class="docutils literal notranslate"><span class="pre">Workflows</span> <span class="pre">(orchestration)</span></code>.</p></li>
<li><p>Click on the workflow <code class="docutils literal notranslate"><span class="pre">weather_workflow</span></code>.</p></li>
<li><p>Click on the <code class="docutils literal notranslate"><span class="pre">History</span></code> tab.</p></li>
<li><p>Select the <code class="docutils literal notranslate"><span class="pre">Workflow</span> <span class="pre">run</span> <span class="pre">ID</span></code> with the status <code class="docutils literal notranslate"><span class="pre">Running</span></code> then click <code class="docutils literal notranslate"><span class="pre">View</span> <span class="pre">run</span> <span class="pre">details</span></code>.</p></li>
<li><p>When this triggers you should see the nodes incrementally changing from <code class="docutils literal notranslate"><span class="pre">Not</span> <span class="pre">Started</span></code> to <code class="docutils literal notranslate"><span class="pre">üîÅ</span> <span class="pre">Running</span></code> then <code class="docutils literal notranslate"><span class="pre">‚úÖ</span> <span class="pre">Succeeded</span></code>.
<img alt="Workflow_Monitor" src="_images/Workflow_Monitor.png" /><br></p></li>
<li><p>When all nodes are at <code class="docutils literal notranslate"><span class="pre">‚úÖ</span> <span class="pre">Succeeded</span></code> status, return to Redshift and re-run your plot code from the previous task to confirm new data has arrived into the target table by comparing this plot to your previous plot!
<img alt="Redshift_NewPlot" src="_images/Redshift_NewPlot.png" /><br></p></li>
</ul>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>‚õî At this point or soon after, your ACG sandbox may shut down due to exceeding Glue DPU (Data Processing Unit) limits. You‚Äôll receive an email titled ‚ÄúYour Hands-On Lab or Cloud Playground has been shut down‚Äù explaining the suspension due to excessive DPU usage.</p>
<p>AWS Glue jobs, being Spark-based, provision distributed computing environments even for small tasks, which can quickly hit ACG‚Äôs limits designed to prevent runaway costs. This restriction is a helpful reminder of resource management in data processing.</p>
<p>If your sandbox is suspended, don‚Äôt worry, this is part of learning to use powerful tools like AWS Glue. Simply start a new sandbox and redeploy the CloudFormation template, which will be ready in 3‚Äì4 minutes. Learn more about Glue DPUs and optimisation here: <a class="reference external" href="https://docs.aws.amazon.com/glue/latest/dg/monitor-debug-capacity.html">https://docs.aws.amazon.com/glue/latest/dg/monitor-debug-capacity.html</a></p>
</div>
</section>
</section>
<section id="going-further-2-data-architecture-diagram">
<h3><strong>üìù Going Further 2:</strong> Data architecture diagram<a class="headerlink" href="#going-further-2-data-architecture-diagram" title="Link to this heading">#</a></h3>
<blockquote>
<div><p>So far, we‚Äôve used many AWS services in a deliberate sequence. If you had to explain this architecture to a colleague, they might lose track when you describe all the different AWS services and how they interact. This is where architecture diagrams become essential, not just a nice-to-have. They help you  have clear and transparent discussions with various colleagues, whether to address the security of what you‚Äôre building, collaborate, or consider changes to the architecture. These changes might involve using different services, transitioning to another cloud platform like Azure or GCP, or adopting a new streaming framework like Kafka.</p>
</div></blockquote>
<p><img alt="Workshop_09_architecture.drawio" src="_images/Workshop_09_architecture.drawio.png" /><br></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>üó∫Ô∏èThis is also an excellent opportunity to practice creating clear and visually appealing architecture diagrams. These diagrams are not only helpful for explaining your current work but could also form part of your project, including as an appendix in your project evaluation report for your End Point Assessment. Additionally, when answering questions about your project to provide verbal evidence for Pass Descriptors, a clear architecture diagram gives you a strong visual aid to screen share, discuss, and use as a reference point to demonstrate your knowledge and skills gained during the apprenticeship!</p>
</div>
<section id="aws-architecture-icons-and-guidance">
<h4>AWS Architecture Icons and Guidance<a class="headerlink" href="#aws-architecture-icons-and-guidance" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Visit Architecture Icons page</strong>:</p>
<ul class="simple">
<li><p>Visit this web page <code class="docutils literal notranslate"><span class="pre">https://aws.amazon.com/architecture/icons/</span></code> and consider clicking on <code class="docutils literal notranslate"><span class="pre">Get</span> <span class="pre">started</span></code> and downloading <code class="docutils literal notranslate"><span class="pre">Microsoft</span> <span class="pre">PPtx</span> <span class="pre">toolkits</span></code>.</p></li>
<li><p>When unzipped this folder contains two PowerPoint files, one for Light and one for Dark themes. Consider these your reference for working out how to create a professional AWS compliant architecture diagram you would see from an experience data architect create. Slides 13 to 18 have simple rules to follow.</p></li>
</ul>
</li>
</ol>
</section>
<section id="drawing-and-diagramming-tools">
<h4>Drawing and diagramming tools<a class="headerlink" href="#drawing-and-diagramming-tools" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Create workshop diagram</strong>:</p>
<ul class="simple">
<li><p>Scroll down the web page <code class="docutils literal notranslate"><span class="pre">https://aws.amazon.com/architecture/icons/</span></code> to the section <code class="docutils literal notranslate"><span class="pre">Drawing</span> <span class="pre">and</span> <span class="pre">diagramming</span> <span class="pre">tools</span></code> and note the different tools AWS recommends.</p></li>
<li><p>We suggest starting with <code class="docutils literal notranslate"><span class="pre">Draw.io</span></code> as its free, simple to use and can be used to create professional diagrams in AWS, Azure, GCP, IBM Cloud, and on-premises architectures: <code class="docutils literal notranslate"><span class="pre">https://app.diagrams.net/</span></code></p></li>
<li><p>Select where to save your diagram (or decide later)
<img alt="SaveDiagrams" src="_images/SaveDiagrams.png" /><br></p></li>
<li><p>Then select <code class="docutils literal notranslate"><span class="pre">File</span> <span class="pre">/</span> <span class="pre">New...</span></code> and click the <code class="docutils literal notranslate"><span class="pre">Blank</span> <span class="pre">Diagram</span></code> icon followed by clicking <code class="docutils literal notranslate"><span class="pre">Cloud</span></code> in the sidebar, then <code class="docutils literal notranslate"><span class="pre">Create</span></code>.
<img alt="drawio_blank" src="_images/drawio_blank.png" /><br></p></li>
<li><p>To find the AWS icons that represent this architecture you can scroll down on the left hand side palette and expand different AWS resource categories such as <code class="docutils literal notranslate"><span class="pre">AWS</span> <span class="pre">/</span> <span class="pre">Analytics</span></code> in which you will find Kinesis.
<img alt="drawio_pallette" src="_images/drawio_pallette.png" /><br></p></li>
<li><p>A faster way to find the icon you need is to use search at the top. For instance, searching for <code class="docutils literal notranslate"><span class="pre">Kinesis</span> <span class="pre">data</span> <span class="pre">stream</span></code> returns two icons shown below. You can ‚Äúmouse over‚Äù over an icon to see how it is described. To determine which icon is the current official icon for that service, in the AWS PowerPoint file you downloaded earlier, search for the same words to see which icon is the current accepted one to use that matches to one of the choices in <a class="reference external" href="http://draw.io">draw.io</a>.
<img alt="drawio_search" src="_images/drawio_search.png" /><br></p></li>
<li><p>Now see if you can re-create the workshop architecture diagram shown above. Would you re-create it exactly? Or could you improve the diagram to look similar to an official AWS diagram like the example below? You can search for diagrams here: <a class="reference external" href="https://aws.amazon.com/architecture/reference-architecture-diagrams">https://aws.amazon.com/architecture/reference-architecture-diagrams</a>
<img alt="AWS_reference_architecture" src="_images/AWS_reference_architecture.png" /><br></p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="going-further-3-direct-streaming">
<h3><strong>‚û°Ô∏è Going Further 3:</strong> Direct Streaming<a class="headerlink" href="#going-further-3-direct-streaming" title="Link to this heading">#</a></h3>
<blockquote>
<div><p>Our current pipeline efficiently collects and processes turbine temperature data, but we can explore direct streaming approaches to reduce latency and improve data freshness. These approaches include consuming data directly from the broker (AWS Kinesis Data Streams) using AWS Glue or streaming it straight into Redshift, bypassing the intermediate S3 storage layer. We‚Äôll also examine Redshift‚Äôs native materialized view streaming capabilities. This section will guide you through these options and help evaluate the best pattern for rapid and reliable temperature monitoring, balancing factors like data consistency, operational complexity, and maintenance needs.</p>
</div></blockquote>
<p><img alt="Workshop_09_architecture_going_further_direct_streaming.drawio" src="_images/Workshop_09_architecture_going_further_direct_streaming.drawio.png" /><br></p>
<section id="stream-directly-in-glue">
<h4>üö∞ Stream directly in glue<a class="headerlink" href="#stream-directly-in-glue" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Create Glue Visual ETL and data source</strong>:</p>
<ul class="simple">
<li><p>In the search bar at the top, search for <code class="docutils literal notranslate"><span class="pre">Glue</span></code> and open in a new tab.</p></li>
<li><p>From the left hand menu, from under <code class="docutils literal notranslate"><span class="pre">ELT</span> <span class="pre">jobs</span></code> click on <code class="docutils literal notranslate"><span class="pre">Visual</span> <span class="pre">ETL</span></code> the click <code class="docutils literal notranslate"><span class="pre">Visual</span> <span class="pre">ETL</span></code>.</p></li>
<li><p>On the top left edit the name of the ETL to <code class="docutils literal notranslate"><span class="pre">Streaming</span></code>, then on the top right click <code class="docutils literal notranslate"><span class="pre">Save</span></code>.</p></li>
<li><p>From the <code class="docutils literal notranslate"><span class="pre">Sources</span></code> menu click on <code class="docutils literal notranslate"><span class="pre">Amazon</span> <span class="pre">Kinesis</span></code> then click on the node itself on the canvas to reveal the properties of the node ot the right.</p></li>
<li><p>From <code class="docutils literal notranslate"><span class="pre">Stream</span> <span class="pre">name</span></code> select <code class="docutils literal notranslate"><span class="pre">weather-analytics-stream-dev</span></code> which is the Amazon Kinesis data stream (Broker) that the lambda function (Producer) writes to every 60 seconds.
<img alt="GlueStream" src="_images/GlueStream.png" /><br></p></li>
<li><p>So that this ETL has permission to access our Kinesis data stream, click on the <code class="docutils literal notranslate"><span class="pre">Job</span> <span class="pre">details</span></code> tab and for the <code class="docutils literal notranslate"><span class="pre">IAM</span> <span class="pre">Role</span></code> drop-down select <code class="docutils literal notranslate"><span class="pre">weather-analytics-glue-role-dev</span></code>. Also change <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">type</span></code> to the lower spec <code class="docutils literal notranslate"><span class="pre">G</span> <span class="pre">1X</span></code>.
<img alt="GlueStreamJobDetails" src="_images/GlueStreamJobDetails.png" /><br></p></li>
</ul>
</li>
<li><p><strong>Create Glue Visual ETL with data source and SQL transform</strong>:</p>
<ul class="simple">
<li><p>Return to the <code class="docutils literal notranslate"><span class="pre">Visual</span></code> tab and you should soon see a <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">preview</span></code> of that source data
<img alt="GlueStreamSourcePreview" src="_images/GlueStreamSourcePreview.png" /><br></p></li>
<li><p>Behind the Data preview tab is an <code class="docutils literal notranslate"><span class="pre">Output</span> <span class="pre">Schema</span></code> tab. This is the schema of the data that will output from this node into the next node of the exercise. Note that all types are correct but not the two timestamp fields.
<img alt="GlueStreamSourceSchema" src="_images/GlueStreamSourceSchema.png" /><br></p></li>
<li><p>To address this, let‚Äôs add a node after the source node to correct the schema. Click the blue circle with a plus to add another node to the ETL.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">Transforms</span></code> tab click on <code class="docutils literal notranslate"><span class="pre">Change</span> <span class="pre">Schema</span></code>.</p></li>
<li><p>Select the <code class="docutils literal notranslate"><span class="pre">Change</span> <span class="pre">Schema</span></code> node so that its properties show to the right. Correct the two data timestamp data types and drop the final field. After these changes the Data preview should automatically update and show you the data in its new schema, and update the Output schema.
<img alt="GlueStreamChangeSchema" src="_images/GlueStreamChangeSchema.png" /><br></p></li>
<li><p>Just like in the first going further exercise where we wrote to Redshift we could create a Redshfit table to match our schema and select Redshift as the targer.
<img alt="GlueStreamTarget" src="_images/GlueStreamTarget.png" /><br></p></li>
</ul>
</li>
<li><p><strong>Finalising this direct streaming pattern</strong>:</p>
<ul class="simple">
<li><p>In this direct streaming example we would still have duplications, think about how you could use both a SQL tranforms node and in the Target node to write to Redshift, use MERGE to handle potential duplications? For example, you could use a SQL transform node to deduplicate readings by grouping them by city and measurement time, then use Redshift‚Äôs MERGE statement in the target node to intelligently update existing records or insert new ones, ensuring wind farm operators see accurate temperature data without duplicates while maintaining the low latency benefits of direct streaming.</p></li>
</ul>
</li>
<li><p><strong>Direct streaming into Redshift</strong>:</p>
<ul class="simple">
<li><p>A newly released feature allows streaming data directly from Kinesis into Redshift using ‚Äúmaterialized views‚Äù, potentially eliminating the need for Glue ETL entirely. This offers even lower latency for temperature monitoring, though it requires careful consideration of data consistency and error handling. You can learn more about this approach in the Redshift documentation at: <a class="reference external" href="https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html">https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html</a></p></li>
</ul>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>‚ö†Ô∏è While direct streaming offers lower latency, it provides fewer safeguards against data inconsistencies compared to our original architecture. The intermediate S3 storage and Glue-managed ETL process gives us a reliable backup of raw data, easier debugging capabilities, and more robust error handling. These could well be important considerations when monitoring expenisve equipment like wind turbines where accurate temperature data directly impacts operational safety and maintenance decisions.</p>
</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-on-previous-workshops-from-on-demand-to-batch-and-streaming">üß± Building on Previous Workshops: From On-Demand to Batch and Streaming</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#should-we-really-be-streaming-data">‚ÅâÔ∏è Should we really be streaming data?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alignment-with-data-engineer-pass-descriptors">ü§ù Alignment with Data Engineer Pass Descriptors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#today-s-workshop-scenario-and-objectives">üå¨Ô∏è Today‚Äôs Workshop Scenario and Objectives</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#task-1-configuring-the-development-environment">‚öôÔ∏è Task 1: Configuring the Development Environment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#task-2-explore-and-understand-your-streaming-application-so-far">üó∫Ô∏è Task 2: Explore and understand your streaming application so far</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#producer-explore-the-data-producer-lambda-function">‚ú® 1 PRODUCER: Explore the Data Producer (Lambda Function):</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broker-view-data-in-the-data-stream-kinesis">‚û°Ô∏è‚û°Ô∏è‚û°Ô∏è 2 BROKER: View data in the Data Stream (Kinesis):</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#delivery-examine-how-data-is-delivered-to-the-sink-kinesis-firehose">üì© 3 DELIVERY: Examine how data is delivered to the sink (Kinesis Firehose):</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sink-storage-inspect-the-data-lake-s3">üìÇ 4 SINK/STORAGE: Inspect the Data Lake (S3):</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#processing-analytics-preview-the-analytics-foundation">üìà 5 PROCESSING / ANALYTICS: Preview the Analytics Foundation:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#task-3-data-lake-exploration-with-glue-and-athena">üîé Task 3: Data Lake Exploration with Glue and Athena</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up-glue-crawler">üìã Set Up Glue Crawler:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#run-the-crawler">üèÉ‚Äç‚ôÇÔ∏è Run the Crawler:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up-athena-query-environment">‚öôÔ∏è Set Up Athena Query Environment:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inspect-your-weather-data">üìä Inspect Your Weather Data:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-clean-view-of-weather-data">üßΩ Create Clean View of Weather Data:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analyse-clean-data">üìà Analyse Clean Data:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-analytics">üìà Advanced Analytics:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenge-exercises">üéØ Challenge Exercises:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-management-best-practices">Cost Management Best Practices:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wrapping-up-and-understanding-real-time-vs-near-real-time-streaming">Wrapping Up and Understanding Real-Time vs Near Real-Time Streaming</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#current-architecture-near-real-time-implementation">Current Architecture: Near Real-Time Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-time-processing">Real-Time Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#this-afternoon-reflecting-on-your-project">This afternoon - reflecting on your project</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#going-further">üöÄ Going Further</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#going-further-1-orchestrated-pipeline-to-write-to-redshift"><strong>üéº Going Further 1:</strong> Orchestrated pipeline to write to Redshift</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-target-table-in-redshift">üéØ Create target table in Redshift</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-aws-glue-visual-etl">‚û°Ô∏è Create AWS Glue visual ETL</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#inspect-and-visualise-redshift-target-table">üìà Inspect and visualise Redshift target table</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#orchestrate-the-entire-process">üéº Orchestrate the entire process!</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#going-further-2-data-architecture-diagram"><strong>üìù Going Further 2:</strong> Data architecture diagram</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#aws-architecture-icons-and-guidance">AWS Architecture Icons and Guidance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#drawing-and-diagramming-tools">Drawing and diagramming tools</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#going-further-3-direct-streaming"><strong>‚û°Ô∏è Going Further 3:</strong> Direct Streaming</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stream-directly-in-glue">üö∞ Stream directly in glue</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Corndel
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>